{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "NUM_COT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"[INST] Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "A: [/INST] In April, Natalia sold clips to 48 friends. In May, she sold half as many as in April, so she sold 48 / 2 = 24 clips. In total, she sold 48 + 24 = 72 clips. The answer is 72.\n",
    "\n",
    "Or, we could use the following equation:\n",
    "Total Clips Sold = Clips Sold in April + Clips Sold in May\n",
    "Total Clips Sold = 48 + 24\n",
    "Total Clips Sold = 72.\n",
    "\n",
    "Therefore, the answer is 72 clips sold altogether in April and May.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messing Around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gpt35_df = pd.read_csv('../conditional/data/112_gsm8k_gpt35_cot_onesent_responses.csv')\n",
    "gpt35_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = gpt35_df['Question'].to_list()[:2]\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = questions[1]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokens = tokenizer(question, add_special_tokens=False)\n",
    "question_input_ids = question_tokens.input_ids\n",
    "print(len(question_tokens.input_ids), type(question_tokens), type(question_tokens.input_ids))\n",
    "question_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokens = {f\"question_{k}\": v for k, v in question_tokens.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = gpt35_df['Answer'].to_list()[:2]\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = answers[1]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokenized = tokenizer(question + answer, add_special_tokens=False)\n",
    "print(len(full_tokenized.input_ids), len(question_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_input_ids = full_tokenized[\"input_ids\"][len(question_input_ids) :]\n",
    "answer_attention_mask = full_tokenized[\"attention_mask\"][len(question_input_ids) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenized_answer(tokenizer, prompt, answer):\n",
    "        \"\"\"\n",
    "        Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n",
    "        It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n",
    "        Reference:\n",
    "            https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257\n",
    "        \"\"\"\n",
    "\n",
    "        full_tokenized = tokenizer(prompt + answer, add_special_tokens=False)\n",
    "        prompt_input_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids) :]\n",
    "        answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids) :]\n",
    "\n",
    "        # Concat tokens to form `enc(a) + enc(a + b)[len(enc(a)):]`\n",
    "        full_concat_input_ids = np.concatenate([prompt_input_ids, answer_input_ids])\n",
    "\n",
    "        # Prepare input tokens for token by token comparison\n",
    "        full_input_ids = np.array(full_tokenized[\"input_ids\"])\n",
    "\n",
    "        if len(full_input_ids) != len(full_concat_input_ids):\n",
    "            raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n",
    "\n",
    "        # On some tokenizers, like Llama-2 tokenizer, there are occasions where tokens\n",
    "        # can be merged together when tokenizing prompt+answer. This could result\n",
    "        # on the last token from the prompt being different when tokenized on its own\n",
    "        # vs when done as prompt+answer.\n",
    "        response_token_ids_start_idx = len(prompt_input_ids)\n",
    "\n",
    "        # If tokenized prompt is different than both prompt+answer, then it means the\n",
    "        # last token has changed due to merging.\n",
    "        if prompt_input_ids != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n",
    "            response_token_ids_start_idx -= 1\n",
    "\n",
    "        prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n",
    "        prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n",
    "\n",
    "        if len(prompt_input_ids) != len(prompt_attention_mask):\n",
    "            raise ValueError(\"Prompt input ids and attention mask should have the same length.\")\n",
    "\n",
    "        answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n",
    "        answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n",
    "\n",
    "        return dict(\n",
    "            prompt_input_ids=prompt_input_ids,\n",
    "            prompt_attention_mask=prompt_attention_mask,\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answer = build_tokenized_answer(tokenizer, question, answer)\n",
    "len(tokenized_answer['prompt_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_len_input_ids = len(question_tokens[\"question_input_ids\"])\n",
    "question_len_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_answer), type(tokenized_answer['prompt_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from util import pad_to_length\n",
    "\n",
    "def concatenated_inputs(\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        is_encoder_decoder: bool = False,\n",
    "        label_pad_token_id: int = -100,\n",
    "        padding_value: int = 0,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> Dict[str, torch.LongTensor]:\n",
    "    \"\"\"Concatenate the chosen and rejected inputs into a single tensor.\n",
    "\n",
    "    Args:\n",
    "        batch: A batch of data. Must contain the keys 'chosen_input_ids' and 'rejected_input_ids', which are tensors of shape (batch_size, sequence_length).\n",
    "        is_encoder_decoder: Whether the model is an encoder-decoder model.\n",
    "        label_pad_token_id: The label pad token id.\n",
    "        padding_value: The padding value to use for the concatenated inputs_ids.\n",
    "        device: The device for the concatenated inputs.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the concatenated inputs under the key 'concatenated_input_ids'.\n",
    "    \"\"\"\n",
    "    concatenated_batch = {}\n",
    "\n",
    "    if is_encoder_decoder:\n",
    "        max_length = max(batch[\"chosen_labels\"].shape[1], batch[\"rejected_labels\"].shape[1])\n",
    "    else:\n",
    "        max_length = max(batch[\"chosen_input_ids\"].shape[1], batch[\"rejected_input_ids\"].shape[1])\n",
    "\n",
    "    for k in batch:\n",
    "        if k.startswith(\"chosen\") and isinstance(batch[k], torch.Tensor):\n",
    "            if \"labels\" in k or is_encoder_decoder:\n",
    "                pad_value = label_pad_token_id\n",
    "            elif k.endswith(\"_input_ids\"):\n",
    "                pad_value = padding_value\n",
    "            elif k.endswith(\"_attention_mask\"):\n",
    "                pad_value = 0\n",
    "            concatenated_key = k.replace(\"chosen\", \"concatenated\")\n",
    "            concatenated_batch[concatenated_key] = pad_to_length(batch[k], max_length, pad_value=pad_value)\n",
    "    for k in batch:\n",
    "        if k.startswith(\"rejected\") and isinstance(batch[k], torch.Tensor):\n",
    "            if \"labels\" in k or is_encoder_decoder:\n",
    "                pad_value = label_pad_token_id\n",
    "            elif k.endswith(\"_input_ids\"):\n",
    "                pad_value = padding_value\n",
    "            elif k.endswith(\"_attention_mask\"):\n",
    "                pad_value = 0\n",
    "            concatenated_key = k.replace(\"rejected\", \"concatenated\")\n",
    "            concatenated_batch[concatenated_key] = torch.cat(\n",
    "                (\n",
    "                    concatenated_batch[concatenated_key],\n",
    "                    pad_to_length(batch[k], max_length, pad_value=pad_value),\n",
    "                ),\n",
    "                dim=0,\n",
    "            ).to(device=device)\n",
    "\n",
    "    if is_encoder_decoder:\n",
    "        concatenated_batch[\"concatenated_input_ids\"] = batch[\"prompt_input_ids\"].repeat(2, 1).to(device=device)\n",
    "        concatenated_batch[\"concatenated_attention_mask\"] = (\n",
    "            batch[\"prompt_attention_mask\"].repeat(2, 1).to(device=device)\n",
    "        )\n",
    "\n",
    "    return concatenated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1 = \" The answer is $12.\"\n",
    "answer2 = \" The answer is 10.\"\n",
    "answer3 = \" The answer is 3.\"\n",
    "sample_answers = [answer1, answer2, answer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def tokenize_sample_answers(tokenizer, prefix_text: str, suffixes: List[str], padding_value: int = 0):\n",
    "    \"\"\"Create tensor of tokenized suffixes\n",
    "\n",
    "    Args:\n",
    "        tokenizer: tokenizer.\n",
    "        prefix_text: text kept constant that precedes the suffixes\n",
    "        suffixes: List of answers to consider for given prefix_text\n",
    "        padding_value: padding token to make suffix input ids equal length\n",
    "    \"\"\"\n",
    "    combined_texts = [prefix_text + suffix for suffix in suffixes]\n",
    "    full_tokenized = tokenizer(combined_texts, add_special_tokens=True, padding=True, return_tensors=\"pt\")\n",
    "    print(\"full shape: \", full_tokenized[\"input_ids\"].shape)\n",
    "    \n",
    "    prefix_input_ids = tokenizer(prefix_text, add_special_tokens=True, return_tensors='pt')['input_ids']\n",
    "    print(\"prefix shape: \", prefix_input_ids.shape)\n",
    "\n",
    "    suffix_start_idx = prefix_input_ids.shape[1]\n",
    "    suffix_input_ids = full_tokenized[\"input_ids\"][:, suffix_start_idx:]\n",
    "\n",
    "    print(\"suffix shape: \", suffix_input_ids.shape)\n",
    "    print(\"suffix input ids: \", suffix_input_ids)\n",
    "\n",
    "    repeated_prefix_input_ids = prefix_input_ids.repeat(full_tokenized[\"input_ids\"].shape[0], 1)\n",
    "    print(\"repeated prefix shape: \", repeated_prefix_input_ids.shape)\n",
    "\n",
    "    # Check if the first 'prefix_length' tokens of each entry in full_tokenized are the same as prefix_input_ids\n",
    "    is_prefix_equal = torch.all(full_tokenized[\"input_ids\"][:, :suffix_start_idx] == repeated_prefix_input_ids, dim=1)\n",
    "    print(\"Is prefix equal for all entries: \", is_prefix_equal)\n",
    "    \n",
    "    return full_tokenized, suffix_input_ids, suffix_start_idx \n",
    "\n",
    "sample_answers = [' The answer is $12.', ' The answer is 10.', ' The answer is 3.']\n",
    "full_tokenized, suffix_input_ids, suffix_start_idx = tokenize_sample_answers(tokenizer, question, sample_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_labels_mask(full_tokenized, suffix_start_idx):\n",
    "    \"\"\"\n",
    "    Create a binary mask for the suffix tokens in the tokenized batch.\n",
    "\n",
    "    Args:\n",
    "        full_tokenized: The tokenized data containing 'input_ids' and 'attention_mask'.\n",
    "        suffix_start_idx: The start index of the suffix in the tokenized sequences.\n",
    "\n",
    "    Returns:\n",
    "        A binary mask tensor of the same shape as `full_tokenized['input_ids']` where suffix tokens are marked with 1 and others with 0.\n",
    "    \"\"\"\n",
    "    batch_size, seq_length = full_tokenized['input_ids'].shape\n",
    "\n",
    "    labels_mask = torch.zeros((batch_size, seq_length), dtype=torch.long)\n",
    "\n",
    "    labels_mask[:, suffix_start_idx:] = 1\n",
    "\n",
    "    labels_mask *= full_tokenized['attention_mask']\n",
    "\n",
    "    return labels_mask\n",
    "\n",
    "wrong_suffix_mask = create_labels_mask(full_tokenized, suffix_start_idx)\n",
    "print(\"Labels mask shape:\", wrong_suffix_mask.shape)\n",
    "print(\"Labels mask:\", wrong_suffix_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = model(full_tokenized['input_ids'].to(model.device)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_forward(\n",
    "        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\n",
    "\n",
    "    We do this to avoid doing two forward passes, because it's faster for FSDP.\n",
    "    \"\"\"\n",
    "    concatenated_batch = self.concatenated_inputs(\n",
    "        batch,\n",
    "        is_encoder_decoder=self.is_encoder_decoder,\n",
    "        label_pad_token_id=self.label_pad_token_id,\n",
    "        padding_value=self.padding_value,\n",
    "        device=self.accelerator.device,\n",
    "    )\n",
    "    len_chosen = batch[\"chosen_labels\"].shape[0]\n",
    "\n",
    "    model_kwargs = (\n",
    "        {\n",
    "            \"labels\": concatenated_batch[\"concatenated_labels\"],\n",
    "            \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n",
    "        }\n",
    "        if self.is_encoder_decoder\n",
    "        else {}\n",
    "    )\n",
    "    all_logits = model(\n",
    "        concatenated_batch[\"concatenated_input_ids\"],\n",
    "        attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "        use_cache=False,\n",
    "        **model_kwargs,\n",
    "    ).logits\n",
    "\n",
    "    all_logps = self.get_batch_logps(\n",
    "        all_logits,\n",
    "        concatenated_batch[\"concatenated_labels\"],\n",
    "        average_log_prob=self.loss_type == \"ipo\",\n",
    "        is_encoder_decoder=self.is_encoder_decoder,\n",
    "        label_pad_token_id=self.label_pad_token_id,\n",
    "    )\n",
    "\n",
    "    chosen_logps = all_logps[:len_chosen]\n",
    "    rejected_logps = all_logps[len_chosen:]\n",
    "\n",
    "    chosen_logits = all_logits[:len_chosen]\n",
    "    rejected_logits = all_logits[len_chosen:]\n",
    "\n",
    "    return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_logps(\n",
    "        logits: torch.FloatTensor,\n",
    "        token_mask: torch.LongTensor,\n",
    "        average_log_prob: bool = False,\n",
    "        label_pad_token_id: int = -100,\n",
    "        is_encoder_decoder: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "    \"\"\"Compute the log probabilities of the given labels under the given logits.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n",
    "        labels: Labels for which to compute the log probabilities. Label tokens with a value of label_pad_token_id are ignored. Shape: (batch_size, sequence_length)\n",
    "        average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.\n",
    "        label_pad_token_id: The label pad token id.\n",
    "        is_encoder_decoder: Whether the model is an encoder-decoder model.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.\n",
    "    \"\"\"\n",
    "    if logits.shape[:-1] != token_mask.shape:\n",
    "        raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n",
    "\n",
    "    # dummy token; we'll ignore the losses on these tokens later\n",
    "    token_mask[token_mask == label_pad_token_id] = 0\n",
    "\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=token_mask.unsqueeze(2)).squeeze(2)\n",
    "    print(per_token_logps.shape)\n",
    "    print(token_mask)\n",
    "    print(per_token_logps * token_mask)\n",
    "\n",
    "    tokens = [tokenizer.decode(ids) for ids in token_mask]\n",
    "\n",
    "    for i, token_sequence in enumerate(tokens):\n",
    "        print(f\"Tokens for sequence {i}: {token_sequence}\")\n",
    "        for j, token in enumerate(token_sequence.split()):\n",
    "            token_log_prob = per_token_logps[i, j]#.max().item()  # Get the max log prob for this token\n",
    "            print(f\"Token: {token}, Log Prob: {token_log_prob}\")\n",
    "\n",
    "    if average_log_prob:\n",
    "        return (per_token_logps * token_mask).sum(-1) / token_mask.sum(-1)\n",
    "    else:\n",
    "        return (per_token_logps * token_mask).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_suffix_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokenized.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch_logps(all_logits.to(\"cpu\"), full_tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobs_per_token(tokenizer, question, answers):\n",
    "    full_tokenized, suffix_input_ids, suffix_start_idx = tokenize_sample_answers(tokenizer, question, answers)\n",
    "    all_logits = model(full_tokenized['input_ids'].to(model.device)).logits\n",
    "    get_batch_logps(all_logits.to(\"cpu\"), full_tokenized.input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
