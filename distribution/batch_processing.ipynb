{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekvajipey/miniconda3/envs/nightly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from util import get_prompt_message, extract_last_integer, extract_last_number\n",
    "from util import remove_last_sentence\n",
    "\n",
    "gpt35_df = pd.read_csv('../conditional/data/112_gsm8k_gpt35_cot_onesent_responses.csv')\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mark has a garden with flowers. He planted plants of three different colors '\n",
      " 'in it. Ten of them are yellow, and there are 80% more of those in purple. '\n",
      " 'There are only 25% as many green flowers as there are yellow and purple '\n",
      " 'flowers. How many flowers does Mark have in his garden?')\n"
     ]
    }
   ],
   "source": [
    "row = gpt35_df.iloc[5]\n",
    "question = row['Question']\n",
    "from pprint import pprint\n",
    "pprint(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 3\n",
    "num_fewshot = 0\n",
    "temp = 0.7\n",
    "direct_prompt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:  Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\n",
      "A: Let's think step by step.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_outputs = []\n",
    "unique_answers = {}\n",
    "messages = get_prompt_message(question, num_fewshot, direct_prompt)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 86]), torch.Size([3, 86]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "input_tensor = input_1.repeat(3, 1)\n",
    "input_1.shape, input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAlCAYAAADfsepVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACiElEQVR4nO3YPWhVdxgG8HPujUVRFKuCBj+IgxVSg6BoBysWCcYsDqVUwam4KDg6d1WcVFREwU1wsJOmERcrCtoqtKAgVfwEBx38IC4mucdZ/m/wg0DA9/cbHx7OPTn33v99SN00TVMBAGm1pvoGAICpZQwAQHLGAAAkZwwAQHLGAAAkZwwAQHLGAAAkZwwAQHLGAAAk1/Wpxf7WL2Hemj49zO8e+77IHg6cCruDKzeG+dDdK3G//9ci69x/FHab0bEwf7VzXZjPPfNPkT3dF3fv7D0W5gM968N8+38Pw/zwoZ+LbMGJv8Nue963YV41nTB+vm1Fkc07HV+7btVhPvzkZpj3HtlTZIsP3Ai7rW+mhXkzHt/3i9/WFNmt34+H3S2Ly+6XvOaFR9eLrF1Pzl7euvyHIuu8Gw277Tmzw3z89ZswHxlaVmTX+v4Iu4O9P4V5Z+RtmNfT4iPi/L2rRTZZz2pg6doia82aGXZHVy0P82cbZoT5koPlZ7/uiv/Gz/lsVlVVzT8Zf6+i9+fVXwvD7kRnymDf5jCvxsrz7fHu3rC67Ojt+BpLFoVx5//yvGrGx8Nu3W6H+csd8bO6sb/8Lm/pXh12Lz77N8wnMripPE+rqqqGLp/75Gts/e7HMK+Xdod59KxaK3ri+7h0Nn7N4Iyoqqr680F5LvUM7wq7E/3GthbeC/MPOh9tAABfNWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJKrm6ZppvomAICp4z8DAJCcMQAAyRkDAJCcMQAAyRkDAJCcMQAAyRkDAJCcMQAAyRkDAJDce8KUekPxlC+cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(input_tensor.numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_1 = model.generate(\n",
    "                input_1.to(model.device), \n",
    "                max_new_tokens=1000, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=40,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 249])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_1.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let'sdenotethenumberofpurpleflowersasX.Accordingtotheproblem,thereare80%morepurpleflowersthanyellowones,sothereare1.8*Ten=<b>18</b>purpleflowers.\\n\\nLet'sdenotethenumberofgreenflowersasG.Theproblemstatesthatthereareonly25%asmanygreenflowersasyellowandpurpleflowerscombined.So,wehave:\\n\\nG=0.25*(10+18)=0.25*28=<b>7</b>\\n\\nTherefore,Markhasatotalof<b>10+18+7=35</b>flowersinhisgarden.</s>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([tokenizer.decode(tok.item()) for tok in outputs_1.sequences[:, input_1.shape[1]:][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAdCAYAAABMxnvIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE30lEQVR4nO3cy6tVdRQH8HXO8RVFYiaU6RU1LDAt6I0VZoh6J0ERFVSDMqigSdC0pkUji4woaBJBRRHSvV4RoiIrK81eJFk+MhzowEc2Se85/QlrQXsfL/L5jL/81r777PP7/fbvLG5nMBgMAgAAAAAAAAAAoCXdc30BAAAAAAAAAADA+U2TEgAAAAAAAAAA0CpNSgAAAAAAAAAAQKs0KQEAAAAAAAAAAK3SpAQAAAAAAAAAALRKkxIAAAAAAAAAANAqTUoAAAAAAAAAAECrNCkBAAAAAAAAAACt0qQEAAAAAAAAAAC0alo1uLZ7X5rpzppVGmvv5mvSzIH1b6aZ0avvKNUb3/t5Ptba+9NM//eDpXqDM2fTzImHbkozc975tlTv8LP5WL88vTnNrF98c6neAz8cSDMvb7o3zcx7/ZtSvd7cS/LQoJ9Gjt69rFRv7lv5dXW6nTQz8ed3pXrLX3kqzSx4cWea6c6YXqo3mMzv1bFHr08zu55/rVRv3YJ8rMq1V647ImLs4NdpptcZbn/mhiW3pJn+v2dKY/VmX5xmJk+eSjOnxxeV6u1Y+WGaGV1+Z5rpn/6nVK8zPV+WPt73RZoZ9mccEbF+5IY0073owjRzZsWSUr0jt12QZha+VJjPptW2Ak3NHZe+UZv7K8/oic8uSzOV9S8iYnTlXXnobL6+H3pyeaneold/zkMLL08j/d/yNTkiYjA5mWY6vV6aOf5g/hlHROx8IV8j1s2/Ls1sO7KnVK9idHW+Nxn/9IPG6m246vY00xmZXxqr8jl3ly1OM+Pb3y3Vq6xbW/fn6+3iiY2leqV9/5A/v6XvPZFmrnymtlfvzZmdZiaPn0wz1bW7MjdW1oduYc8RETH+0ydpZvXGx9PMzIndpXoV00auSDNjX24pjVX5PlTWyMoeNWJq7lNPfZTPVZXnLiJiZNOeNHP04WvTzLxdf5fqTWx5O81U3oM7nfz9L6K5Z2HYz0FlHxtRO3uovLtW9hwREROHavvGYap8ZyJq62TFqh/vKeWa2hdX9mdVJx65Nc1U9oxVU3Ht7q9aWarX1LtdVeW8qjIv9BbU9rKT8/L7Gd//mkaafHdt6uwhYmrO2VPx7OHYYzeW6lXODJo6L4iImHl8kGZ2P5fPVaMr1pTqVfbO6xfl621VZS1tcu6vvMNXzorHDxfn63Nw/pepfEcH/fy5i4jY9teu/3s5EVH/3Wds/1dpZtj33PMSMVbcEzf191XueURtTz/s/XxT51lVTc6fZ9fk933Gzr2lsbbu25Fmhv2beG9pvnZP/nGoVG8qPnsVU3UurlxXk9fU1Lze5D0Y9tzRVF9ARHO/6VTfzSvz3vb++6Wxpt6qDAAAAAAAAAAAnFc0KQEAAAAAAAAAAK3SpAQAAAAAAAAAALRKkxIAAAAAAAAAANAqTUoAAAAAAAAAAECrNCkBAAAAAAAAAACt0qQEAAAAAAAAAAC0SpMSAAAAAAAAAADQqs5gMBic64sAAAAAAAAAAADOX/6TEgAAAAAAAAAA0CpNSgAAAAAAAAAAQKs0KQEAAAAAAAAAAK3SpAQAAAAAAAAAALRKkxIAAAAAAAAAANAqTUoAAAAAAAAAAECrNCkBAAAAAAAAAACt0qQEAAAAAAAAAAC0SpMSAAAAAAAAAADQqv8AF8IjQs1U+PQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs_1.sequences.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "                input_tensor.to(model.device), \n",
    "                max_new_tokens=1000, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=40,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 535])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs.sequences.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as X. Since there are 80% more purple flowers than yellow flowers, and there are 10 yellow flowers, we can write the equation: X = 10 * 1.8 = 18.\\n\\nLet's denote the number of green flowers as Y. Since there are 25% as many green flowers as there are yellow and purple flowers, we can write: Y = 10 + 10 + 18 * 0.25 = 15.\\n\\nTherefore, Mark has a total of 10 (yellow) + 18 (purple) + 15 (green) = <<10+18+15=43>>43 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as x. Since there are 80% more purple flowers than yellow ones, the number of yellow flowers is x/1.25. The number of green flowers is 25% of the number of yellow and purple flowers combined, so it's 0.25 * (x + x/1.25).\\n\\nWe have three equations based on the given information:\\n\\n1. x + (x/1.25) + 0.25 * (x + x/1.25) = total number of flowers\\n2. x + x/1.25 = 10 (yellow flowers) * 1.25\\n3. 0.25 * (x + x/1.25) = 25% of (x + x/1.25)\\n\\nLet's solve equation 2 for x:\\n\\nx + x/1.25 = 12.5\\nx(1 + 1/1.25) = 12.5\\nx = 12.5 * 1.25 / 2\\nx = 15\\n\\nNow let's calculate the number of yellow and purple flowers:\\n\\nYellow flowers = x/1.25 = 15/1.25 ≈ 12\\nPurple flowers = 1.25 * Yellow flowers = 1.25 * 12 ≈ 15\\n\\nLet's check if the total number of flowers is correct:\\n\\nTotal number of flowers = 12 (yellow) + 15 (purple) + 0.25 * (12 + 15) ≈ 12 + 15 + 6.25 = 23.25\\n\\nHowever, the total number of flowers should be a whole number. It seems like there is a mistake in the problem statement or our calculations. We cannot determine the exact number of flowers in the garden with the given information.</s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as X. According to the problem, there are 80% more purple flowers than yellow ones, so there are 1.8 * Ten = 18 yellow flowers.\\n\\nSince there are 25% as many green flowers as there are yellow and purple flowers, the number of green flowers is 0.25 * (10 + 18) = 6.5 * 18 = 117 (we round up because we can't have a fraction of a flower).\\n\\nSo, in total, Mark has 10 (yellow) + 18 (purple) + 117 (green) = <<10+18+117=135>>135 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "                input_tensor.to(model.device), \n",
    "                max_new_tokens=500, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=40,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 583])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(outputs.sequences.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs.sequences.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let x be the number of yellow flowers, y be the number of purple flowers, and z be the number of green flowers. We have three equations based on the given information:\\n\\n1. x + y + z = total number of flowers\\n2. x = 10 (the number of yellow flowers)\\n3. y = 1.8x (80% more purple flowers than yellow)\\n4. z = 0.25x (25% as many green flowers as yellow and purple)\\n\\nFirst, let's find the number of purple flowers (y). According to equation 3:\\n\\ny = 1.8 * 10\\ny = 18\\n\\nNow, let's find the number of green flowers (z). According to equation 4:\\n\\nz = 0.25 * (x + y)\\nz = 0.25 * (10 + 18)\\nz = 0.25 * 28\\nz = 7\\n\\nNow we have all the flower counts:\\n- Yellows: x = 10\\n- Purples: y = 18\\n- Greens: z = 7\\n\\nSo, the total number of flowers in Mark's garden is:\\n\\nx + y + z = 10 + 18 + 7 = 35\\n\\nTherefore, Mark has 35 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as x, and the number of green flowers as y. We know that:\\n\\n1. The number of yellow flowers is 10.\\n2. The number of purple flowers is 10 * 1.8 = 18 (since there are 80% more purple flowers than yellow).\\n3. The number of green flowers is 10 * 0.25 = 2.5, but since we cannot have a fraction of a flower, we need to round up or down to the nearest whole number. Let's assume there are 2 green flowers.\\n\\nTherefore, the total number of flowers in Mark's garden is:\\n\\nTotal Flowers = Yellow Flowers + Purple Flowers + Green Flowers\\nTotal Flowers = 10 + 18 + 2 = <box>30</box>\\n\\nSo, Mark has 30 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as X. We know that there are 80% more purple flowers than yellow ones, so the number of yellow flowers is X/1.8 (since 80% is equivalent to 0.8 or 1/1.8). We also know that there are 25% as many green flowers as there are yellow and purple flowers combined. Let's denote the number of green flowers as G.\\n\\nFirst, let's find the total number of yellow and purple flowers:\\nTotal yellow and purple flowers = X + X/1.8\\n\\nNow, let's find the number of green flowers:\\nGreen flowers = 0.25 * (X + X/1.8)\\n\\nSince we know that Mark has ten yellow flowers, we can set up an equation to find the value of X:\\nX + X/1.8 = 10 + X/1.8 + 0.25 * (10 + X/1.8)\\n\\nLet's simplify the equation:\\nX + X/1.8 = 10 + X/1.8 + 2.5 * (10 + X/1.8)\\nX + X/1.8 = 10 + X/1.8 + 25 + 2.5 * X\\nX = 25\\n\\nNow, we can find the number of yellow and purple flowers:\\nTotal yellow and purple flowers = 25 + 25/1.8 = 38.885… ≈ 39\\n\\nSince we can't have a fraction of a flower, we'll round up to 40.\\n\\nThe number of yellow flowers is 40/2 = 20.\\n\\nNow, we can find the number of purple flowers:\\nPurple flowers = 40 * 1.8 = 72\\n\\nFinally, we can find the number of green flowers:\\nGreen flowers = 0.25 * 40 * 1.8 = 18\\n\\nSo, Mark has approximately 40 yellow flowers, 72 purple flowers, and 18 green flowers in his garden. The total number of flowers is around 130.</s>\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 842], [1, 842, 28705], [1, 28705, 842, 28705], [1, 842, 415])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('.'), tokenizer.encode(\". \"), tokenizer.encode(\" . \"), tokenizer.encode(\". The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(842) == tokenizer.decode(28723)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28705, 13]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <s>\n",
      "28705 \n",
      "318 S\n",
      "308 ent\n",
      "636 ence\n",
      "28705 \n",
      "28740 1\n",
      "28723 .\n",
      "318 S\n",
      "308 ent\n",
      "636 ence\n",
      "28705 \n",
      "28750 2\n",
      "28723 .\n",
      "2909 Some\n",
      "1474 number\n",
      "28705 \n",
      "28770 3\n",
      "28723 .\n",
      "28782 5\n",
      "28725 ,\n",
      "318 S\n",
      "308 ent\n",
      "636 ence\n",
      "28705 \n",
      "28770 3\n",
      "28723 .\n"
     ]
    }
   ],
   "source": [
    "for tok in tokenizer.encode(\" Sentence 1. Sentence 2. Some number 3.5, Sentence 3.\"):\n",
    "    print(tok, tokenizer.decode(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 28705, 28734]\n",
      "1 [1, 28705, 28740]\n",
      "2 [1, 28705, 28750]\n",
      "3 [1, 28705, 28770]\n",
      "4 [1, 28705, 28781]\n",
      "5 [1, 28705, 28782]\n",
      "6 [1, 28705, 28784]\n",
      "7 [1, 28705, 28787]\n",
      "8 [1, 28705, 28783]\n",
      "9 [1, 28705, 28774]\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    print(n, tokenizer.encode(str(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28705, 28770, 28723, 28782]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"3.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   733, 16289, 28793,  1186, 28747,  3655,   659,   264,  8759,\n",
       "          395, 11888, 28723,   650, 24571,  9923,   302,  1712,  1581,  9304,\n",
       "          297,   378, 28723, 11819,   302,   706,   460,  9684, 28725,   304,\n",
       "          736,   460, 28705, 28783, 28734, 28823,   680,   302,  1395,   297,\n",
       "        19435, 28723,  1387,   460,   865, 28705, 28750, 28782, 28823,   390,\n",
       "         1287,  5344, 11888,   390,   736,   460,  9684,   304, 19435, 11888,\n",
       "        28723,  1602,  1287, 11888,  1235,  3655,   506,   297,   516,  8759,\n",
       "        28804,    13, 28741, 28747,  3169, 28742, 28713,  1073,  3707,   486,\n",
       "         3707, 28723,   733, 28748, 16289, 28793,  3169, 28742, 28713, 14543,\n",
       "          272,  1474,   302, 19435, 11888,   390,  1500, 28723,  6586,   298,\n",
       "          272,  2700, 28725,   736,   460, 28705, 28783, 28734, 28823,   680,\n",
       "        19435, 11888,   821,  9684,  4413, 28725,   579,   736,   460, 28705,\n",
       "        28740, 28723, 28783,   398, 11819,   327,   523, 28726, 28767, 28740,\n",
       "        28783,   700, 28726, 28767, 19435, 11888, 28723,    13,    13,  8779,\n",
       "        28742, 28713, 14543,   272,  1474,   302,  5344, 11888,   390,   420,\n",
       "        28723,   415,  2700,  4605,   369,   736,   460,   865, 28705, 28750,\n",
       "        28782, 28823,   390,  1287,  5344, 11888,   390,  9684,   304, 19435,\n",
       "        11888,  9837, 28723,  1537, 28725,   478,   506, 28747,    13,    13,\n",
       "        28777,   327, 28705, 28734, 28723, 28750, 28782,   398,   325, 28740,\n",
       "        28734,   648, 28705, 28740, 28783, 28731,   327, 28705, 28734, 28723,\n",
       "        28750, 28782,   398, 28705, 28750, 28783,   327,   523, 28726, 28767,\n",
       "        28787,   700, 28726, 28767,    13,    13,  5816,   994, 28725,  3655,\n",
       "          659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
       "        28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
       "          700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_1.sequences.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28723, Decoded: .\n",
      "Token: 6586, Decoded: According\n",
      "Token: 298, Decoded: to\n",
      "Token: 272, Decoded: the\n",
      "Token: 2700, Decoded: problem\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 4413, Decoded: ones\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 579, Decoded: so\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 398, Decoded: *\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8779, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 420, Decoded: G\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 2700, Decoded: problem\n",
      "Token: 4605, Decoded: states\n",
      "Token: 369, Decoded: that\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 9837, Decoded: combined\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1537, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 506, Decoded: have\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28777, Decoded: G\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28787, Decoded: 7\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 3102, Decoded: total\n",
      "Token: 302, Decoded: of\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "Token: 2, Decoded: </s>\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.decode(outputs_1.sequences)\n",
    "# Fix the TypeError by ensuring the input to tokenizer.decode is a list of integers\n",
    "# decoded_output = tokenizer.decode(outputs_1.sequences.flatten().tolist())\n",
    "# decoded_output\n",
    "for token in outputs_1.sequences.flatten():\n",
    "    decoded_token = tokenizer.decode([token.item()])\n",
    "    print(f\"Token: {token.item()}, Decoded: {decoded_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of period token: tensor([ 12,  22,  41,  60,  81,  97, 121, 136, 150, 172, 184, 199, 247],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "outputs_1.sequences.flatten()\n",
    "# Find the instances of the element 842 in the flattened sequences\n",
    "indices_of_period = (outputs_1.sequences.flatten() == 28723).nonzero(as_tuple=True)[0]\n",
    "print(\"Indices of period token:\", indices_of_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = outputs_1.sequences.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 249])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  12],\n",
       "        [  0,  22],\n",
       "        [  0,  41],\n",
       "        [  0,  60],\n",
       "        [  0,  81],\n",
       "        [  0,  97],\n",
       "        [  0, 121],\n",
       "        [  0, 136],\n",
       "        [  0, 150],\n",
       "        [  0, 172],\n",
       "        [  0, 184],\n",
       "        [  0, 199],\n",
       "        [  0, 247]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros_like(tensor_1)\n",
    "period_indices = (tensor_1 == 28723).nonzero()\n",
    "period_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  71],\n",
       "        [  0, 137],\n",
       "        [  0, 138],\n",
       "        [  0, 178],\n",
       "        [  0, 179],\n",
       "        [  0, 214],\n",
       "        [  0, 215]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tok2delim = {28723 : \".\", 13 : \"\\n\"}\n",
    "newline_indices = (tensor_1 == 13).nonzero()\n",
    "newline_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = tensor_1.size(1)\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 248]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_end = (tensor_1 == 2).nonzero()\n",
    "seq_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 199])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_start_idx = period_indices[-2] if len(period_indices) > 1 and  else None\n",
    "sentence_start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sentence_start_idx is not None:\n",
    "    mask[0, sentence_start_idx[1]+1:] = 1\n",
    "else:\n",
    "    mask[0, :] = 1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793,  1186, 28747,  3655,   659,   264,  8759,\n",
       "           395, 11888, 28723,   650, 24571,  9923,   302,  1712,  1581,  9304,\n",
       "           297,   378, 28723, 11819,   302,   706,   460,  9684, 28725,   304,\n",
       "           736,   460, 28705, 28783, 28734, 28823,   680,   302,  1395,   297,\n",
       "         19435, 28723,  1387,   460,   865, 28705, 28750, 28782, 28823,   390,\n",
       "          1287,  5344, 11888,   390,   736,   460,  9684,   304, 19435, 11888,\n",
       "         28723,  1602,  1287, 11888,  1235,  3655,   506,   297,   516,  8759,\n",
       "         28804,    13, 28741, 28747,  3169, 28742, 28713,  1073,  3707,   486,\n",
       "          3707, 28723,   733, 28748, 16289, 28793,  3169, 28742, 28713, 14543,\n",
       "           272,  1474,   302, 19435, 11888,   390,  1500, 28723,  6586,   298,\n",
       "           272,  2700, 28725,   736,   460, 28705, 28783, 28734, 28823,   680,\n",
       "         19435, 11888,   821,  9684,  4413, 28725,   579,   736,   460, 28705,\n",
       "         28740, 28723, 28783,   398, 11819,   327,   523, 28726, 28767, 28740,\n",
       "         28783,   700, 28726, 28767, 19435, 11888, 28723,    13,    13,  8779,\n",
       "         28742, 28713, 14543,   272,  1474,   302,  5344, 11888,   390,   420,\n",
       "         28723,   415,  2700,  4605,   369,   736,   460,   865, 28705, 28750,\n",
       "         28782, 28823,   390,  1287,  5344, 11888,   390,  9684,   304, 19435,\n",
       "         11888,  9837, 28723,  1537, 28725,   478,   506, 28747,    13,    13,\n",
       "         28777,   327, 28705, 28734, 28723, 28750, 28782,   398,   325, 28740,\n",
       "         28734,   648, 28705, 28740, 28783, 28731,   327, 28705, 28734, 28723,\n",
       "         28750, 28782,   398, 28705, 28750, 28783,   327,   523, 28726, 28767,\n",
       "         28787,   700, 28726, 28767,    13,    13,  5816,   994, 28725,  3655,\n",
       "           659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
       "         28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
       "           700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         28750, 28782,   398, 28705, 28750, 28783,   327,   523, 28726, 28767,\n",
       "         28787,   700, 28726, 28767,    13,    13,  5816,   994, 28725,  3655,\n",
       "           659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
       "         28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
       "           700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_tensor = tensor_1 * mask\n",
    "ans_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28787, Decoded: 7\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 3102, Decoded: total\n",
      "Token: 302, Decoded: of\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "Token: 2, Decoded: </s>\n"
     ]
    }
   ],
   "source": [
    "for token in ans_tensor.flatten():\n",
    "    decoded_token = tokenizer.decode([token.item()])\n",
    "    print(f\"Token: {token.item()}, Decoded: {decoded_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_end[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 247 too close to </s> at 248\n",
      "Filtered Period Indices: tensor([[  0,  12],\n",
      "        [  0,  22],\n",
      "        [  0,  41],\n",
      "        [  0,  60],\n",
      "        [  0,  81],\n",
      "        [  0,  97],\n",
      "        [  0, 136],\n",
      "        [  0, 150],\n",
      "        [  0, 172]])\n"
     ]
    }
   ],
   "source": [
    "period_indices = (tensor_1 == 28723).nonzero()\n",
    "\n",
    "digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "\n",
    "# Filter out periods that are part of decimal numbers\n",
    "filtered_indices = []\n",
    "for idx in period_indices:\n",
    "    batch_index, token_index = idx[0], idx[1]\n",
    "    seq_end = (tensor_1 == 2).nonzero()\n",
    "    seq_end_index = seq_end[0][1].item()\n",
    "    # Remove if period is near end of sequence (it is part of last sentence)\n",
    "    if seq_end_index - token_index < 5:\n",
    "        print(f\"index {token_index} too close to </s> at {seq_end_index}\")\n",
    "    # Check if the period is not at the start or end of the tensor\n",
    "    elif token_index > 0 and token_index < tensor_1.shape[1] - 1:\n",
    "        # Check the tokens before and after the period\n",
    "        token_before = tensor_1[batch_index, token_index - 1]\n",
    "        token_after = tensor_1[batch_index, token_index + 1]\n",
    "        # Check if both tokens are numeric\n",
    "        if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "            filtered_indices.append(idx)\n",
    "\n",
    "# Convert list to tensor\n",
    "filtered_indices = torch.stack(filtered_indices)\n",
    "\n",
    "print(\"Filtered Period Indices:\", filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  71],\n",
       "        [  0, 137],\n",
       "        [  0, 138],\n",
       "        [  0, 178],\n",
       "        [  0, 179],\n",
       "        [  0, 214],\n",
       "        [  0, 215]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline_indices = (tensor_1 == 13).nonzero()\n",
    "newline_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last sentence start index: tensor(215)\n"
     ]
    }
   ],
   "source": [
    "last_period_index = filtered_indices[-1, 1] if filtered_indices.size(0) > 0 else -1\n",
    "last_newline_index = newline_indices[-1, 1] if newline_indices.size(0) > 0 else -1\n",
    "\n",
    "# Determine the maximum index\n",
    "last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "print(\"Last sentence start index:\", last_sentence_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros_like(tensor_1)\n",
    "if sentence_start_idx is not None:\n",
    "    mask[0, last_sentence_start_index+1:] = 1\n",
    "else:\n",
    "    mask[0, :] = 1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 3102, Decoded: total\n",
      "Token: 302, Decoded: of\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "Token: 2, Decoded: </s>\n"
     ]
    }
   ],
   "source": [
    "ans_tensor = tensor_1 * mask\n",
    "for token in ans_tensor.flatten():\n",
    "    decoded_token = tokenizer.decode([token.item()])\n",
    "    print(f\"Token: {token.item()}, Decoded: {decoded_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,  5816,   994, 28725,  3655,\n",
      "           659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
      "         28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
      "           700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_length = tensor_1.shape\n",
    "digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "mask = torch.zeros_like(tensor_1)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    row_tensor = tensor_1[i]\n",
    "\n",
    "    # Find period indices\n",
    "    period_indices = (row_tensor == 28723).nonzero()\n",
    "\n",
    "    # Filter out periods that are part of decimal numbers\n",
    "    filtered_indices = []\n",
    "    for idx in period_indices:\n",
    "        token_index = idx[0]\n",
    "        seq_end = (row_tensor == 2).nonzero()\n",
    "        seq_end_index = seq_end[0].item()\n",
    "        # Remove if period is near end of sequence (it is part of last sentence)\n",
    "        if seq_end_index - token_index < 5:\n",
    "            continue\n",
    "        # Check if the period is not at the start or end of the tensor\n",
    "        elif token_index > 0 and token_index < seq_length - 1:\n",
    "            # Check the tokens before and after the period\n",
    "            token_before = row_tensor[token_index - 1]\n",
    "            token_after = row_tensor[token_index + 1]\n",
    "            # Check if both tokens are numeric\n",
    "            if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "                filtered_indices.append(token_index)\n",
    "\n",
    "    # Find newline indices\n",
    "    newline_indices = (row_tensor == 13).nonzero()\n",
    "\n",
    "    # Determine the last sentence start index\n",
    "    last_period_index = max(filtered_indices) if filtered_indices else -1\n",
    "    last_newline_index = newline_indices[-1] if newline_indices.size(0) > 0 else -1\n",
    "    last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "    # Set mask for the last sentence\n",
    "    if last_sentence_start_index != -1:\n",
    "        mask[i, last_sentence_start_index+1:] = 1\n",
    "\n",
    "# Apply mask to get only the last sentences\n",
    "last_sentence_only_tensor = tensor_1 * mask\n",
    "\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"Last Sentence Only Tensor:\\n\", last_sentence_only_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([249])\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    row_tensor = last_sentence_only_tensor[i]\n",
    "    print(row_tensor.shape)\n",
    "    # decoded_token = tokenizer.decode([token.item()])\n",
    "    # print(f\"Token: {token.item()}, Decoded: {decoded_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_sentence(tensor):\n",
    "    batch_size, seq_length = tensor.shape\n",
    "    digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "    mask = torch.zeros_like(tensor)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        row_tensor = tensor[i]\n",
    "\n",
    "        period_indices = (row_tensor == 28723).nonzero()\n",
    "\n",
    "        filtered_indices = []\n",
    "        for idx in period_indices:\n",
    "            token_index = idx[0]\n",
    "            seq_end = (row_tensor == 2).nonzero()\n",
    "            seq_end_index = seq_end[0].item()\n",
    "            # Remove if period is near end of sequence (it is part of last sentence)\n",
    "            if seq_end_index - token_index < 5:\n",
    "                continue\n",
    "            # Check if the period is not at the start or end of the tensor\n",
    "            elif token_index > 0 and token_index < seq_length - 1:\n",
    "                # Check the tokens before and after the period\n",
    "                token_before = row_tensor[token_index - 1]\n",
    "                token_after = row_tensor[token_index + 1]\n",
    "                # Check if both tokens are numeric\n",
    "                if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "                    filtered_indices.append(token_index)\n",
    "\n",
    "        newline_indices = (row_tensor == 13).nonzero()\n",
    "\n",
    "        # Determine the last sentence start index\n",
    "        last_period_index = max(filtered_indices) if filtered_indices else -1\n",
    "        last_newline_index = newline_indices[-1] if newline_indices.size(0) > 0 else -1\n",
    "        last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "        # Set mask for the last sentence\n",
    "        if last_sentence_start_index != -1:\n",
    "            mask[i, last_sentence_start_index+1:] = 1\n",
    "\n",
    "    # Apply mask to get only the last sentences\n",
    "    last_sentence_only_tensor = tensor * mask\n",
    "\n",
    "    print(\"Mask:\\n\", mask)\n",
    "    print(\"Last Sentence Only Tensor:\\n\", last_sentence_only_tensor)\n",
    "    \n",
    "    return last_sentence_only_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0')\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ..., 28734, 28723,     2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "lsot = find_last_sentence(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs.sequences.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAfCAYAAAABDtrDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACfElEQVR4nO3csUocURQGYGd2SecLLCqktlmwsEgXGBbsF/EVfIG8gpLSwtZ+eyPZIlWKSR8QFLSwdpM+mWsnnEKYCRwWNt9XzQ/nzj0P8HOrUkrZAgAAAAAAAAAASFKvewEAAAAAAAAAAGCzKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKQa9x1s6nnmHgAAAAAAwJr9+XgQ8rv2NuQvd99DPmqOQ+7uH0O+eWgH3b9/cRryznk8X41GIa9O4r7t2WXIs8l00P0AAMBwy27Ra85LSgAAAAAAAAAAQColJQAAAAAAAAAAIJWSEgAAAAAAAAAAkKoqpZQ+g009z94FAAAA4L9TPkxD/rq4CvmoOQ65u38M+eahHXTf/sVpyDvn8Xw1GoW8OjkIuT27fP2eTaaD7gYAAABg8yy7Ra85LykBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAECqqpRS+gw29Tx7FwAAAAAAAAAA2GjVeBxy/X4vDjz/CvHv6nfIT58OQ979/CP+b3s75Ouf3/5hy7fNJtOQl92i1zkvKQEAAAAAAAAAAKmUlAAAAAAAAAAAgFRKSgAAAAAAAAAAQKqqlFLWvQQAAAAAAAAAALC5vKQEAAAAAAAAAACkUlICAAAAAAAAAABSKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKR6AdGuRi1/V93/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(lsot.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0')\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ..., 28734, 28723,     2]], device='mps:0')\n",
      "ROW  0\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  1\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  2\n",
      "Token: 415, Decoded: The\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 1401, Decoded: around\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n"
     ]
    }
   ],
   "source": [
    "def print_decoded_tokens(lsot, tokenizer):\n",
    "    # Iterate over each row in the lsot tensor\n",
    "    for idx, row_tensor in enumerate(lsot):\n",
    "        print(\"ROW \", idx)\n",
    "        # Flatten the row tensor to iterate over each token\n",
    "        for token in row_tensor.flatten():\n",
    "            # Check if the token is not one of the excluded tokens\n",
    "            if token.item() not in [0, 2, 28734]:\n",
    "                # Decode the token using the tokenizer\n",
    "                decoded_token = tokenizer.decode([token.item()])\n",
    "                # Print the token and its decoded value\n",
    "                print(f\"Token: {token.item()}, Decoded: {decoded_token}\")\n",
    "\n",
    "# Assuming 'outputs.sequences' is your input tensor and 'tokenizer' is your tokenizer instance\n",
    "lsot = find_last_sentence(outputs.sequences)\n",
    "print_decoded_tokens(lsot, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def find_last_sentence(tensor):\n",
    "    batch_size, seq_length = tensor.shape\n",
    "    digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "    mask = torch.zeros_like(tensor)\n",
    "    inverse_mask = torch.zeros_like(tensor)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        row_tensor = tensor[i]\n",
    "\n",
    "        # Find period indices\n",
    "        period_indices = (row_tensor == 28723).nonzero()\n",
    "\n",
    "        seq_end = (row_tensor == 2).nonzero()\n",
    "        seq_end_index = seq_end[0].item()\n",
    "\n",
    "        # Filter out periods that are part of decimal numbers\n",
    "        filtered_indices = []\n",
    "        for idx in period_indices:\n",
    "            token_index = idx[0]\n",
    "            # Remove if period is near end of sequence (it is part of last sentence)\n",
    "            if seq_end_index - token_index < 5:\n",
    "                continue\n",
    "            # Check if the period is not at the start or end of the tensor\n",
    "            elif token_index > 0 and token_index < seq_length - 1:\n",
    "                # Check the tokens before and after the period\n",
    "                token_before = row_tensor[token_index - 1]\n",
    "                token_after = row_tensor[token_index + 1]\n",
    "                # Check if both tokens are numeric\n",
    "                if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "                    filtered_indices.append(token_index)\n",
    "\n",
    "        newline_indices = (row_tensor == 13).nonzero()\n",
    "\n",
    "        # Determine the last sentence start index\n",
    "        last_period_index = max(filtered_indices) if filtered_indices else -1\n",
    "        last_newline_index = newline_indices[-1] if newline_indices.size(0) > 0 else -1\n",
    "        last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "        # Set mask for the last sentence\n",
    "        if last_sentence_start_index != -1:\n",
    "            mask[i, last_sentence_start_index+1:seq_end_index+1] = 1\n",
    "            inverse_mask[i, :last_sentence_start_index+1] = 1\n",
    "\n",
    "    # Apply mask to get only the last sentences\n",
    "    last_sentence_only_tensor = tensor * mask\n",
    "    initial_sentences_tensor = tensor * inverse_mask\n",
    "\n",
    "    print(\"Mask:\\n\", mask)\n",
    "    print(\"Inverse Mask:\\n\", inverse_mask)\n",
    "    print(\"Last Sentence Only Tensor:\\n\", last_sentence_only_tensor)\n",
    "    print(\"Initial Sentences Tensor:\\n\", initial_sentences_tensor)\n",
    "    \n",
    "    return initial_sentences_tensor, last_sentence_only_tensor \n",
    "\n",
    "# ist, lsot = find_last_sentence(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW  0\n",
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 1318, Decoded: x\n",
      "Token: 347, Decoded: be\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 337, Decoded: y\n",
      "Token: 347, Decoded: be\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 686, Decoded: z\n",
      "Token: 347, Decoded: be\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 816, Decoded: We\n",
      "Token: 506, Decoded: have\n",
      "Token: 1712, Decoded: three\n",
      "Token: 12501, Decoded: equations\n",
      "Token: 2818, Decoded: based\n",
      "Token: 356, Decoded: on\n",
      "Token: 272, Decoded: the\n",
      "Token: 2078, Decoded: given\n",
      "Token: 1871, Decoded: information\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1318, Decoded: x\n",
      "Token: 648, Decoded: +\n",
      "Token: 337, Decoded: y\n",
      "Token: 648, Decoded: +\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1318, Decoded: x\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 325, Decoded: (\n",
      "Token: 1237, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n",
      "Token: 337, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28744, Decoded: x\n",
      "Token: 325, Decoded: (\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28723, Decoded: .\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28744, Decoded: x\n",
      "Token: 325, Decoded: (\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 7489, Decoded: First\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 325, Decoded: (\n",
      "Token: 28724, Decoded: y\n",
      "Token: 609, Decoded: ).\n",
      "Token: 6586, Decoded: According\n",
      "Token: 298, Decoded: to\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28724, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28724, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 325, Decoded: (\n",
      "Token: 28764, Decoded: z\n",
      "Token: 609, Decoded: ).\n",
      "Token: 6586, Decoded: According\n",
      "Token: 298, Decoded: to\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28744, Decoded: x\n",
      "Token: 648, Decoded: +\n",
      "Token: 337, Decoded: y\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 478, Decoded: we\n",
      "Token: 506, Decoded: have\n",
      "Token: 544, Decoded: all\n",
      "Token: 272, Decoded: the\n",
      "Token: 14994, Decoded: flower\n",
      "Token: 17938, Decoded: counts\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28733, Decoded: -\n",
      "Token: 627, Decoded: Y\n",
      "Token: 479, Decoded: ell\n",
      "Token: 3611, Decoded: ows\n",
      "Token: 28747, Decoded: :\n",
      "Token: 1318, Decoded: x\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28733, Decoded: -\n",
      "Token: 13673, Decoded: Pur\n",
      "Token: 2815, Decoded: ples\n",
      "Token: 28747, Decoded: :\n",
      "Token: 337, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28733, Decoded: -\n",
      "Token: 9261, Decoded: Gre\n",
      "Token: 596, Decoded: ens\n",
      "Token: 28747, Decoded: :\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 272, Decoded: the\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 349, Decoded: is\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28744, Decoded: x\n",
      "Token: 648, Decoded: +\n",
      "Token: 337, Decoded: y\n",
      "Token: 648, Decoded: +\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "ROW  1\n",
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 1318, Decoded: x\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 337, Decoded: y\n",
      "Token: 28723, Decoded: .\n",
      "Token: 816, Decoded: We\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 325, Decoded: (\n",
      "Token: 10793, Decoded: since\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 609, Decoded: ).\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 562, Decoded: but\n",
      "Token: 1854, Decoded: since\n",
      "Token: 478, Decoded: we\n",
      "Token: 3573, Decoded: cannot\n",
      "Token: 506, Decoded: have\n",
      "Token: 264, Decoded: a\n",
      "Token: 14005, Decoded: fraction\n",
      "Token: 302, Decoded: of\n",
      "Token: 264, Decoded: a\n",
      "Token: 14994, Decoded: flower\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 927, Decoded: need\n",
      "Token: 298, Decoded: to\n",
      "Token: 3713, Decoded: round\n",
      "Token: 582, Decoded: up\n",
      "Token: 442, Decoded: or\n",
      "Token: 1060, Decoded: down\n",
      "Token: 298, Decoded: to\n",
      "Token: 272, Decoded: the\n",
      "Token: 17403, Decoded: nearest\n",
      "Token: 2894, Decoded: whole\n",
      "Token: 1474, Decoded: number\n",
      "Token: 28723, Decoded: .\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 7146, Decoded: assume\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 272, Decoded: the\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 349, Decoded: is\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 327, Decoded: =\n",
      "Token: 24275, Decoded: Yellow\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 648, Decoded: +\n",
      "Token: 13673, Decoded: Pur\n",
      "Token: 792, Decoded: ple\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 648, Decoded: +\n",
      "Token: 6248, Decoded: Green\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 2858, Decoded: box\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 700, Decoded: </\n",
      "Token: 2858, Decoded: box\n",
      "Token: 28767, Decoded: >\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "ROW  2\n",
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28723, Decoded: .\n",
      "Token: 816, Decoded: We\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 4413, Decoded: ones\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 579, Decoded: so\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 325, Decoded: (\n",
      "Token: 10793, Decoded: since\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 349, Decoded: is\n",
      "Token: 9844, Decoded: equivalent\n",
      "Token: 298, Decoded: to\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 442, Decoded: or\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 609, Decoded: ).\n",
      "Token: 816, Decoded: We\n",
      "Token: 835, Decoded: also\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 9837, Decoded: combined\n",
      "Token: 28723, Decoded: .\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 420, Decoded: G\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 7489, Decoded: First\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 1500, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 22991, Decoded: Green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 24091, Decoded: Since\n",
      "Token: 478, Decoded: we\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 3095, Decoded: ten\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 808, Decoded: set\n",
      "Token: 582, Decoded: up\n",
      "Token: 396, Decoded: an\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 298, Decoded: to\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1192, Decoded: value\n",
      "Token: 302, Decoded: of\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8779, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 9878, Decoded: simpl\n",
      "Token: 1575, Decoded: ify\n",
      "Token: 272, Decoded: the\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 1500, Decoded: X\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28878, Decoded: …\n",
      "Token: 28705, Decoded: \n",
      "Token: 29988, Decoded: ≈\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28774, Decoded: 9\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 24091, Decoded: Since\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28707, Decoded: t\n",
      "Token: 506, Decoded: have\n",
      "Token: 264, Decoded: a\n",
      "Token: 14005, Decoded: fraction\n",
      "Token: 302, Decoded: of\n",
      "Token: 264, Decoded: a\n",
      "Token: 14994, Decoded: flower\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 28742, Decoded: '\n",
      "Token: 584, Decoded: ll\n",
      "Token: 3713, Decoded: round\n",
      "Token: 582, Decoded: up\n",
      "Token: 298, Decoded: to\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 1014, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28753, Decoded: P\n",
      "Token: 324, Decoded: ur\n",
      "Token: 792, Decoded: ple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 11491, Decoded: Fin\n",
      "Token: 578, Decoded: ally\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 22991, Decoded: Green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 10870, Decoded: approximately\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n"
     ]
    }
   ],
   "source": [
    "print_decoded_tokens(ist, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW  0\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  1\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  2\n",
      "Token: 415, Decoded: The\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 1401, Decoded: around\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n"
     ]
    }
   ],
   "source": [
    "print_decoded_tokens(lsot, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let x be the number of yellow flowers, \"\n",
      " 'y be the number of purple flowers, and z be the number of green flowers. We '\n",
      " 'have three equations based on the given information:\\n'\n",
      " '\\n'\n",
      " '1. x + y + z = total number of flowers\\n'\n",
      " '2. x = 10 (the number of yellow flowers)\\n'\n",
      " '3. y = 1.8x (80% more purple flowers than yellow)\\n'\n",
      " '4. z = 0.25x (25% as many green flowers as yellow and purple)\\n'\n",
      " '\\n'\n",
      " \"First, let's find the number of purple flowers (y). According to equation \"\n",
      " '3:\\n'\n",
      " '\\n'\n",
      " 'y = 1.8 * 10\\n'\n",
      " 'y = 18\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers (z). According to equation 4:\\n\"\n",
      " '\\n'\n",
      " 'z = 0.25 * (x + y)\\n'\n",
      " 'z = 0.25 * (10 + 18)\\n'\n",
      " 'z = 0.25 * 28\\n'\n",
      " 'z = 7\\n'\n",
      " '\\n'\n",
      " 'Now we have all the flower counts:\\n'\n",
      " '- Yellows: x = 10\\n'\n",
      " '- Purples: y = 18\\n'\n",
      " '- Greens: z = 7\\n'\n",
      " '\\n'\n",
      " \"So, the total number of flowers in Mark's garden is:\\n\"\n",
      " '\\n'\n",
      " 'x + y + z = 10 + 18 + 7 = 35\\n'\n",
      " '\\n'\n",
      " 'Therefore, Mark has 35 flowers in his '\n",
      " 'garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(outputs.sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let's denote the number of purple \"\n",
      " 'flowers as X. We know that there are 80% more purple flowers than yellow '\n",
      " 'ones, so the number of yellow flowers is X/1.8 (since 80% is equivalent to '\n",
      " '0.8 or 1/1.8). We also know that there are 25% as many green flowers as '\n",
      " \"there are yellow and purple flowers combined. Let's denote the number of \"\n",
      " 'green flowers as G.\\n'\n",
      " '\\n'\n",
      " \"First, let's find the total number of yellow and purple flowers:\\n\"\n",
      " 'Total yellow and purple flowers = X + X/1.8\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers:\\n\"\n",
      " 'Green flowers = 0.25 * (X + X/1.8)\\n'\n",
      " '\\n'\n",
      " 'Since we know that Mark has ten yellow flowers, we can set up an equation to '\n",
      " 'find the value of X:\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 0.25 * (10 + X/1.8)\\n'\n",
      " '\\n'\n",
      " \"Let's simplify the equation:\\n\"\n",
      " 'X + X/1.8 = 10 + X/1.8 + 2.5 * (10 + X/1.8)\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 25 + 2.5 * X\\n'\n",
      " 'X = 25\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of yellow and purple flowers:\\n'\n",
      " 'Total yellow and purple flowers = 25 + 25/1.8 = 38.885… ≈ 39\\n'\n",
      " '\\n'\n",
      " \"Since we can't have a fraction of a flower, we'll round up to 40.\\n\"\n",
      " '\\n'\n",
      " 'The number of yellow flowers is 40/2 = 20.\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of purple flowers:\\n'\n",
      " 'Purple flowers = 40 * 1.8 = 72\\n'\n",
      " '\\n'\n",
      " 'Finally, we can find the number of green flowers:\\n'\n",
      " 'Green flowers = 0.25 * 40 * 1.8 = 18\\n'\n",
      " '\\n'\n",
      " 'So, Mark has approximately 40 yellow flowers, 72 purple flowers, and 18 '\n",
      " 'green flowers in his '\n",
      " 'garden.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(ist[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> '\n",
      " 'The total number of flowers is around 130.</s>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(lsot[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankensteining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_last_sentence_of_first_row(ist, lsot, lsot_index):\n",
    "    # Extract the last sentence from the first row of lsot\n",
    "    last_sentence_first_row = lsot[lsot_index]\n",
    "    # Filter out zeros (masked tokens)\n",
    "    last_sentence_first_row = last_sentence_first_row[last_sentence_first_row != 0]\n",
    "    last_sentence_first_row = last_sentence_first_row[last_sentence_first_row != 2]\n",
    "    last_sentence_first_row = torch.cat((last_sentence_first_row, torch.tensor([2]).to(last_sentence_first_row.device)))\n",
    "    print(\"lsfr\", last_sentence_first_row)\n",
    "\n",
    "    # Determine the maximum possible length of the new rows\n",
    "    max_length = ist.size(1) + last_sentence_first_row.size(0)\n",
    "\n",
    "    # Create a new tensor to hold the result with the expanded size\n",
    "    new_tensor = torch.zeros((ist.size(0), max_length), dtype=ist.dtype)\n",
    "\n",
    "    for i in range(ist.size(0)):\n",
    "        # Get the initial sentence tokens from ist for the current row\n",
    "        initial_sentence = ist[i]\n",
    "        initial_sentence = initial_sentence[initial_sentence != 0]\n",
    "\n",
    "        # Concatenate the initial sentence with the last sentence of the first row\n",
    "        combined_sentence = torch.cat((initial_sentence, last_sentence_first_row))\n",
    "\n",
    "        # Place the combined sentence into the new tensor, respecting the original row index\n",
    "        new_tensor[i, :combined_sentence.size(0)] = combined_sentence\n",
    "\n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def append_last_sentence_of_first_row_with_mask(ist, lsot):\n",
    "    # Extract the last sentence from the first row of lsot\n",
    "    last_sentence_first_row = lsot[0]\n",
    "    # Filter out zeros (masked tokens)\n",
    "    last_sentence_first_row = last_sentence_first_row[last_sentence_first_row != 0]\n",
    "\n",
    "    # Determine the maximum possible length of the new rows\n",
    "    max_length = ist.size(1) + last_sentence_first_row.size(0)\n",
    "\n",
    "    # Create a new tensor to hold the result with the expanded size\n",
    "    new_tensor = torch.zeros((ist.size(0), max_length), dtype=ist.dtype)\n",
    "    new_mask = torch.zeros((ist.size(0), max_length), dtype=torch.uint8)\n",
    "\n",
    "    for i in range(ist.size(0)):\n",
    "        # Get the initial sentence tokens from ist for the current row\n",
    "        initial_sentence = ist[i]\n",
    "        initial_sentence = initial_sentence[initial_sentence != 0]\n",
    "\n",
    "        # Concatenate the initial sentence with the last sentence of the first row\n",
    "        combined_sentence = torch.cat((initial_sentence, last_sentence_first_row))\n",
    "\n",
    "        # Place the combined sentence into the new tensor, respecting the original row index\n",
    "        new_tensor[i, :combined_sentence.size(0)] = combined_sentence\n",
    "\n",
    "        # Create mask for the appended last sentence\n",
    "        start_index_of_last_sentence = initial_sentence.size(0)\n",
    "        end_index_of_last_sentence = start_index_of_last_sentence + last_sentence_first_row.size(0)\n",
    "        new_mask[i, start_index_of_last_sentence:end_index_of_last_sentence] = 1\n",
    "\n",
    "    return new_tensor, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsfr tensor([ 5816,   994, 28725,  3655,   659, 28705, 28770, 28782, 11888,   297,\n",
      "          516,  8759, 28723,     2], device='mps:0')\n",
      "tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = append_last_sentence_of_first_row(ist, lsot, 0)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let's denote the number of purple \"\n",
      " 'flowers as X. We know that there are 80% more purple flowers than yellow '\n",
      " 'ones, so the number of yellow flowers is X/1.8 (since 80% is equivalent to '\n",
      " '0.8 or 1/1.8). We also know that there are 25% as many green flowers as '\n",
      " \"there are yellow and purple flowers combined. Let's denote the number of \"\n",
      " 'green flowers as G.\\n'\n",
      " '\\n'\n",
      " \"First, let's find the total number of yellow and purple flowers:\\n\"\n",
      " 'Total yellow and purple flowers = X + X/1.8\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers:\\n\"\n",
      " 'Green flowers = 0.25 * (X + X/1.8)\\n'\n",
      " '\\n'\n",
      " 'Since we know that Mark has ten yellow flowers, we can set up an equation to '\n",
      " 'find the value of X:\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 0.25 * (10 + X/1.8)\\n'\n",
      " '\\n'\n",
      " \"Let's simplify the equation:\\n\"\n",
      " 'X + X/1.8 = 10 + X/1.8 + 2.5 * (10 + X/1.8)\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 25 + 2.5 * X\\n'\n",
      " 'X = 25\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of yellow and purple flowers:\\n'\n",
      " 'Total yellow and purple flowers = 25 + 25/1.8 = 38.885… ≈ 39\\n'\n",
      " '\\n'\n",
      " \"Since we can't have a fraction of a flower, we'll round up to 40.\\n\"\n",
      " '\\n'\n",
      " 'The number of yellow flowers is 40/2 = 20.\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of purple flowers:\\n'\n",
      " 'Purple flowers = 40 * 1.8 = 72\\n'\n",
      " '\\n'\n",
      " 'Finally, we can find the number of green flowers:\\n'\n",
      " 'Green flowers = 0.25 * 40 * 1.8 = 18\\n'\n",
      " '\\n'\n",
      " 'So, Mark has approximately 40 yellow flowers, 72 purple flowers, and 18 '\n",
      " 'green flowers in his garden.Therefore, Mark has 35 flowers in his '\n",
      " 'garden.</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(new_tensor[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsfr tensor([  415,  3102,  1474,   302, 11888,   349,  1401, 28705, 28740, 28770,\n",
      "        28734, 28723,     2], device='mps:0')\n",
      "tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = append_last_sentence_of_first_row(ist, lsot, 2)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let x be the number of yellow flowers, \"\n",
      " 'y be the number of purple flowers, and z be the number of green flowers. We '\n",
      " 'have three equations based on the given information:\\n'\n",
      " '\\n'\n",
      " '1. x + y + z = total number of flowers\\n'\n",
      " '2. x = 10 (the number of yellow flowers)\\n'\n",
      " '3. y = 1.8x (80% more purple flowers than yellow)\\n'\n",
      " '4. z = 0.25x (25% as many green flowers as yellow and purple)\\n'\n",
      " '\\n'\n",
      " \"First, let's find the number of purple flowers (y). According to equation \"\n",
      " '3:\\n'\n",
      " '\\n'\n",
      " 'y = 1.8 * 10\\n'\n",
      " 'y = 18\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers (z). According to equation 4:\\n\"\n",
      " '\\n'\n",
      " 'z = 0.25 * (x + y)\\n'\n",
      " 'z = 0.25 * (10 + 18)\\n'\n",
      " 'z = 0.25 * 28\\n'\n",
      " 'z = 7\\n'\n",
      " '\\n'\n",
      " 'Now we have all the flower counts:\\n'\n",
      " '- Yellows: x = 10\\n'\n",
      " '- Purples: y = 18\\n'\n",
      " '- Greens: z = 7\\n'\n",
      " '\\n'\n",
      " \"So, the total number of flowers in Mark's garden is:\\n\"\n",
      " '\\n'\n",
      " 'x + y + z = 10 + 18 + 7 = 35\\n'\n",
      " '\\n'\n",
      " ' The total number of flowers is around '\n",
      " '130.</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(new_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(outputs.sequences, 'tensors/flowers_3_ans_test.pt')\n",
    "print(\"Tensor saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CPU to offload from GPU\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekvajipey/miniconda3/envs/nightly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from util import get_prompt_message, extract_last_integer, extract_last_number\n",
    "from util import remove_last_sentence\n",
    "\n",
    "gpt35_df = pd.read_csv('../conditional/data/112_gsm8k_gpt35_cot_onesent_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [25:07<00:00, 753.67s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.57s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(base_model_name)\n",
    "base_model.generation_config.pad_token_id =base_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tensor: tensor([[    1,   733, 16289,  ...,     2,     2,     2],\n",
      "        [    1,   733, 16289,  ...,     2,     2,     2],\n",
      "        [    1,   733, 16289,  ..., 28734, 28723,     2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "gen_outputs = torch.load('tensors/flowers_3_ans_test.pt')\n",
    "print(\"Loaded tensor:\", gen_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0')\n",
      "Inverse Mask:\n",
      " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='mps:0')\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ..., 28734, 28723,     2]], device='mps:0')\n",
      "Initial Sentences Tensor:\n",
      " tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "ist, lsot = find_last_sentence(gen_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAfCAYAAAABDtrDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACfElEQVR4nO3csUocURQGYGd2SecLLCqktlmwsEgXGBbsF/EVfIG8gpLSwtZ+eyPZIlWKSR8QFLSwdpM+mWsnnEKYCRwWNt9XzQ/nzj0P8HOrUkrZAgAAAAAAAAAASFKvewEAAAAAAAAAAGCzKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKQa9x1s6nnmHgAAAAAAwJr9+XgQ8rv2NuQvd99DPmqOQ+7uH0O+eWgH3b9/cRryznk8X41GIa9O4r7t2WXIs8l00P0AAMBwy27Ra85LSgAAAAAAAAAAQColJQAAAAAAAAAAIJWSEgAAAAAAAAAAkKoqpZQ+g009z94FAAAA4L9TPkxD/rq4CvmoOQ65u38M+eahHXTf/sVpyDvn8Xw1GoW8OjkIuT27fP2eTaaD7gYAAABg8yy7Ra85LykBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAECqqpRS+gw29Tx7FwAAAAAAAAAA2GjVeBxy/X4vDjz/CvHv6nfIT58OQ979/CP+b3s75Ouf3/5hy7fNJtOQl92i1zkvKQEAAAAAAAAAAKmUlAAAAAAAAAAAgFRKSgAAAAAAAAAAQKqqlFLWvQQAAAAAAAAAALC5vKQEAAAAAAAAAACkUlICAAAAAAAAAABSKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKR6AdGuRi1/V93/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(lsot.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Tensor:\n",
      " tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]])\n",
      "New Mask:\n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# Append the last sentence of the first row to each row of ist and create a mask\n",
    "new_tensor, new_mask = append_last_sentence_of_first_row_with_mask(ist, lsot)\n",
    "print(\"New Tensor:\\n\", new_tensor)\n",
    "print(\"New Mask:\\n\", new_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 597])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(new_tensor.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAfCAYAAAABDtrDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB1klEQVR4nO3csQ2DUBBEQUBUQRU0YVGBq3QFyE1QBWX4O3VIsj4EM/EPNiJAT9e31loHAAAAAAAAAAAQMlQPAAAAAAAAAAAArk2kBAAAAAAAAAAARImUAAAAAAAAAACAKJESAAAAAAAAAAAQJVICAAAAAAAAAACiREoAAAAAAAAAAECUSAkAAAAAAAAAAIgSKQEAAAAAAAAAAFHj0YeP4ZncAQAAAAAAt7PuW/WEmGWaqycAAAB/8P68Dr1zSQkAAAAAAAAAAIgSKQEAAAAAAAAAAFEiJQAAAAAAAAAAIGqsHgAAAABwNuu+VU+IWaa5egIAP3yXAQCAu3BJCQAAAAAAAAAAiBIpAQAAAAAAAAAAUSIlAAAAAAAAAAAgaqweAAAAAHA2yzRXTwAAAACg67p136onRNzx/5NLSgAAAAAAAAAAQJRICQAAAAAAAAAAiBIpAQAAAAAAAAAAUX1rrVWPAAAAAAAAAAAArsslJQAAAAAAAAAAIEqkBAAAAAAAAAAARImUAAAAAAAAAACAKJESAAAAAAAAAAAQJVICAAAAAAAAAACiREoAAAAAAAAAAECUSAkAAAAAAAAAAIgSKQEAAAAAAAAAAFEiJQAAAAAAAAAAIOoLsacVXQT4p/YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(new_mask.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens_and_logprobs(model, tokenizer, input_ids, answer_mask):\n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "    # Adjust indices to ignore the first token's log prob as it corresponds to the second token\n",
    "    probs = probs[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    batch = []\n",
    "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
    "        text_sequence = []\n",
    "        for token, p in zip(input_sentence, input_probs):\n",
    "            if token not in tokenizer.all_special_ids:\n",
    "                print((tokenizer.decode(token), p.item()))\n",
    "                text_sequence.append((tokenizer.decode(token), p.item()))\n",
    "        batch.append(text_sequence)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', -6.6210126876831055)\n",
      "('INST', -11.904136657714844)\n",
      "(']', -4.8453288078308105)\n",
      "('Q', -7.979267597198486)\n",
      "(':', -2.536184072494507)\n",
      "('Mark', -8.783544540405273)\n",
      "('has', -6.009649276733398)\n",
      "('a', -1.7091089487075806)\n",
      "('garden', -7.529897212982178)\n",
      "('with', -1.8171358108520508)\n",
      "('flowers', -4.945797443389893)\n",
      "('.', -1.3876492977142334)\n",
      "('He', -1.4655823707580566)\n",
      "('planted', -3.3684661388397217)\n",
      "('plants', -7.450453281402588)\n",
      "('of', -1.8496564626693726)\n",
      "('three', -2.0832767486572266)\n",
      "('different', -0.7003024220466614)\n",
      "('colors', -1.8695447444915771)\n",
      "('in', -2.3152263164520264)\n",
      "('it', -1.873545527458191)\n",
      "('.', -0.6123096942901611)\n",
      "('Ten', -8.892126083374023)\n",
      "('of', -1.2373195886611938)\n",
      "('them', -1.92722487449646)\n",
      "('are', -0.32159745693206787)\n",
      "('yellow', -2.268256187438965)\n",
      "(',', -0.3485907018184662)\n",
      "('and', -3.0524158477783203)\n",
      "('there', -3.0309739112854004)\n",
      "('are', -0.13086363673210144)\n",
      "('', -1.6398487091064453)\n",
      "('8', -2.800138473510742)\n",
      "('0', -2.380390167236328)\n",
      "('%', -0.8139203190803528)\n",
      "('more', -0.3197389543056488)\n",
      "('of', -1.813336730003357)\n",
      "('those', -4.40558385848999)\n",
      "('in', -4.542832374572754)\n",
      "('purple', -7.778336048126221)\n",
      "('.', -1.0508742332458496)\n",
      "('There', -2.0358195304870605)\n",
      "('are', -0.10865125060081482)\n",
      "('only', -4.447059631347656)\n",
      "('', -0.7551405429840088)\n",
      "('2', -1.4833123683929443)\n",
      "('5', -2.3355507850646973)\n",
      "('%', -0.4344860017299652)\n",
      "('as', -4.011203765869141)\n",
      "('many', -0.016702106222510338)\n",
      "('green', -2.60488224029541)\n",
      "('flowers', -1.3441193103790283)\n",
      "('as', -0.20549799501895905)\n",
      "('there', -1.3967803716659546)\n",
      "('are', -0.020905621349811554)\n",
      "('yellow', -0.6963631510734558)\n",
      "('and', -3.5187976360321045)\n",
      "('purple', -0.2720828354358673)\n",
      "('flowers', -1.0190436840057373)\n",
      "('.', -1.9251680374145508)\n",
      "('How', -0.6719295382499695)\n",
      "('many', -0.010257150046527386)\n",
      "('flowers', -0.8930326700210571)\n",
      "('does', -1.2475383281707764)\n",
      "('Mark', -0.5434073805809021)\n",
      "('have', -0.09688587486743927)\n",
      "('in', -0.4096667170524597)\n",
      "('his', -0.5959460735321045)\n",
      "('garden', -0.008123807609081268)\n",
      "('?', -0.1642107218503952)\n",
      "('\\n', -1.0198670625686646)\n",
      "('A', -2.2747488021850586)\n",
      "(':', -0.1398182064294815)\n",
      "('Let', -5.492328643798828)\n",
      "(\"'\", -2.7145376205444336)\n",
      "('s', -0.00506463460624218)\n",
      "('think', -5.845344066619873)\n",
      "('step', -5.557998180389404)\n",
      "('by', -0.2061532735824585)\n",
      "('step', -0.0013319916324689984)\n",
      "('.', -0.44106510281562805)\n",
      "('[', -5.587679862976074)\n",
      "('/', -3.288726329803467)\n",
      "('INST', -0.44636476039886475)\n",
      "(']', -0.03333093225955963)\n",
      "('Let', -5.828550815582275)\n",
      "('x', -3.43473744392395)\n",
      "('be', -0.4634881317615509)\n",
      "('the', -0.07121027261018753)\n",
      "('number', -0.1117946058511734)\n",
      "('of', -0.007316580042243004)\n",
      "('yellow', -0.669685959815979)\n",
      "('flowers', -0.09521200507879257)\n",
      "(',', -0.9941616654396057)\n",
      "('y', -0.7640271782875061)\n",
      "('be', -0.6187477707862854)\n",
      "('the', -0.06643599271774292)\n",
      "('number', -0.02516849897801876)\n",
      "('of', -0.01290329173207283)\n",
      "('purple', -0.06637832522392273)\n",
      "('flowers', -0.09242517501115799)\n",
      "(',', -0.6061540246009827)\n",
      "('and', -0.12368829548358917)\n",
      "('z', -0.04070647805929184)\n",
      "('be', -0.065103679895401)\n",
      "('the', -0.005413396749645472)\n",
      "('number', -0.005689853802323341)\n",
      "('of', -0.003020369913429022)\n",
      "('green', -0.018883222714066505)\n",
      "('flowers', -0.025438619777560234)\n",
      "('.', -0.11529168486595154)\n",
      "('We', -1.9837579727172852)\n",
      "('have', -1.7480672597885132)\n",
      "('three', -3.7444517612457275)\n",
      "('equations', -0.1527807116508484)\n",
      "('based', -4.57830810546875)\n",
      "('on', -0.02442343160510063)\n",
      "('the', -0.33725738525390625)\n",
      "('given', -1.4082143306732178)\n",
      "('information', -0.4408095180988312)\n",
      "(':', -0.48732998967170715)\n",
      "('\\n', -1.1113431453704834)\n",
      "('\\n', -1.1457267999649048)\n",
      "('1', -1.1446987390518188)\n",
      "('.', -0.296260803937912)\n",
      "('x', -0.5656635165214539)\n",
      "('+', -0.1490868180990219)\n",
      "('y', -0.01362276915460825)\n",
      "('+', -0.056324753910303116)\n",
      "('z', -0.0030083658639341593)\n",
      "('=', -0.005211459007114172)\n",
      "('total', -2.3381187915802)\n",
      "('number', -0.2693980038166046)\n",
      "('of', -0.017546894028782845)\n",
      "('flowers', -0.03936348855495453)\n",
      "('\\n', -0.7375124096870422)\n",
      "('2', -0.2150629311800003)\n",
      "('.', -0.0009944261983036995)\n",
      "('x', -0.9933474063873291)\n",
      "('=', -0.7685874700546265)\n",
      "('', -0.1577741950750351)\n",
      "('1', -0.5229145288467407)\n",
      "('0', -0.02783280238509178)\n",
      "('(', -2.78770112991333)\n",
      "('the', -2.749433994293213)\n",
      "('number', -0.11386912316083908)\n",
      "('of', -0.0044067418202757835)\n",
      "('yellow', -0.01639830321073532)\n",
      "('flowers', -0.018743421882390976)\n",
      "(')', -0.2569896876811981)\n",
      "('\\n', -0.006983633153140545)\n",
      "('3', -0.002972709946334362)\n",
      "('.', -0.00047291061491705477)\n",
      "('y', -1.068387508392334)\n",
      "('=', -0.12141046673059464)\n",
      "('', -0.197194442152977)\n",
      "('1', -0.9917259812355042)\n",
      "('.', -0.2922806143760681)\n",
      "('8', -0.26795122027397156)\n",
      "('x', -0.22482413053512573)\n",
      "('(', -0.1172550767660141)\n",
      "('8', -0.9347504377365112)\n",
      "('0', -0.0013491347199305892)\n",
      "('%', -0.012446703389286995)\n",
      "('more', -0.3234022855758667)\n",
      "('purple', -1.2588824033737183)\n",
      "('flowers', -0.2884260416030884)\n",
      "('than', -0.2260589599609375)\n",
      "('yellow', -0.06236127018928528)\n",
      "(')', -0.8798214197158813)\n",
      "('\\n', -0.14398254454135895)\n",
      "('4', -1.0269628763198853)\n",
      "('.', -0.0012010273057967424)\n",
      "('z', -0.20221218466758728)\n",
      "('=', -0.007847432047128677)\n",
      "('', -0.10643714666366577)\n",
      "('0', -0.033943574875593185)\n",
      "('.', -0.001384253497235477)\n",
      "('2', -0.024283938109874725)\n",
      "('5', -0.0025774373207241297)\n",
      "('x', -1.3567373752593994)\n",
      "('(', -1.357057809829712)\n",
      "('2', -0.18427065014839172)\n",
      "('5', -0.0009804923320189118)\n",
      "('%', -0.0026406915858387947)\n",
      "('as', -0.3138733208179474)\n",
      "('many', -0.008847558870911598)\n",
      "('green', -0.022638337686657906)\n",
      "('flowers', -0.08718906342983246)\n",
      "('as', -0.056414894759655)\n",
      "('yellow', -0.49157053232192993)\n",
      "('and', -0.1757676899433136)\n",
      "('purple', -0.003044377313926816)\n",
      "(')', -0.566173791885376)\n",
      "('\\n', -0.21518944203853607)\n",
      "('\\n', -0.26935863494873047)\n",
      "('First', -4.750057220458984)\n",
      "(',', -0.2614625096321106)\n",
      "('let', -1.1876213550567627)\n",
      "(\"'\", -0.036516014486551285)\n",
      "('s', -0.00013755806139670312)\n",
      "('find', -2.7533864974975586)\n",
      "('the', -0.574931263923645)\n",
      "('number', -2.12248158454895)\n",
      "('of', -0.0035923488903790712)\n",
      "('purple', -0.29477962851524353)\n",
      "('flowers', -0.04271644726395607)\n",
      "('(', -2.055607318878174)\n",
      "('y', -0.03783769533038139)\n",
      "(').', -0.8777990341186523)\n",
      "('According', -4.271822929382324)\n",
      "('to', -0.00103586888872087)\n",
      "('equation', -0.8099150061607361)\n",
      "('', -0.3302137553691864)\n",
      "('3', -0.1505948156118393)\n",
      "(':', -3.564267873764038)\n",
      "('\\n', -0.17149201035499573)\n",
      "('\\n', -0.2863572835922241)\n",
      "('y', -0.44583630561828613)\n",
      "('=', -0.009265990927815437)\n",
      "('', -0.0037925951182842255)\n",
      "('1', -0.0034023988991975784)\n",
      "('.', -0.0019766809418797493)\n",
      "('8', -0.0002858230145648122)\n",
      "('*', -3.199889898300171)\n",
      "('', -0.35988640785217285)\n",
      "('1', -0.0004956685588695109)\n",
      "('0', -0.00019762947340495884)\n",
      "('\\n', -1.5763436555862427)\n",
      "('y', -0.3403741419315338)\n",
      "('=', -0.0023384150117635727)\n",
      "('', -0.0011277989251539111)\n",
      "('1', -0.0012960375752300024)\n",
      "('8', -0.007439172826707363)\n",
      "('\\n', -0.10471340268850327)\n",
      "('\\n', -0.06222192570567131)\n",
      "('Now', -0.9518144726753235)\n",
      "(',', -0.3626267910003662)\n",
      "('let', -0.2339809089899063)\n",
      "(\"'\", -0.002666015876457095)\n",
      "('s', -6.09140915912576e-05)\n",
      "('find', -0.056609682738780975)\n",
      "('the', -0.010663316585123539)\n",
      "('number', -0.04165279492735863)\n",
      "('of', -0.00045372682507149875)\n",
      "('green', -0.033102262765169144)\n",
      "('flowers', -0.003032017033547163)\n",
      "('(', -0.021016286686062813)\n",
      "('z', -0.0032517435029149055)\n",
      "(').', -0.07647573202848434)\n",
      "('According', -0.1164240688085556)\n",
      "('to', -0.00039748396375216544)\n",
      "('equation', -0.00831215362995863)\n",
      "('', -0.001598986447788775)\n",
      "('4', -0.004490173887461424)\n",
      "(':', -0.06408751755952835)\n",
      "('\\n', -0.0016882466152310371)\n",
      "('\\n', -0.009434047155082226)\n",
      "('z', -0.01526518166065216)\n",
      "('=', -0.0010942192748188972)\n",
      "('', -0.004447687417268753)\n",
      "('0', -0.002043066080659628)\n",
      "('.', -0.0002910667099058628)\n",
      "('2', -0.0005452338373288512)\n",
      "('5', -0.00010406429646536708)\n",
      "('*', -0.023610280826687813)\n",
      "('(', -1.3324047327041626)\n",
      "('x', -1.9650614261627197)\n",
      "('+', -0.0146860983222723)\n",
      "('y', -0.18355901539325714)\n",
      "(')', -0.011456416919827461)\n",
      "('\\n', -0.035795342177152634)\n",
      "('z', -0.028531311079859734)\n",
      "('=', -0.0003483880718704313)\n",
      "('', -0.0035455478355288506)\n",
      "('0', -0.0029357450548559427)\n",
      "('.', -0.00011789103882620111)\n",
      "('2', -0.0005918181850574911)\n",
      "('5', -9.321732068201527e-05)\n",
      "('*', -0.02362285554409027)\n",
      "('(', -0.07528478652238846)\n",
      "('1', -0.10623414814472198)\n",
      "('0', -0.00406308937817812)\n",
      "('+', -0.0026475873310118914)\n",
      "('', -0.002049133414402604)\n",
      "('1', -0.0004748170613311231)\n",
      "('8', -0.0007320346776396036)\n",
      "(')', -0.0009726322605274618)\n",
      "('\\n', -0.015378117561340332)\n",
      "('z', -0.004971167538315058)\n",
      "('=', -0.00046969347749836743)\n",
      "('', -0.002768852049484849)\n",
      "('0', -0.8122367262840271)\n",
      "('.', -0.00018571082910057157)\n",
      "('2', -0.004015953745692968)\n",
      "('5', -0.00020382710499688983)\n",
      "('*', -0.0024077491834759712)\n",
      "('', -0.011380517855286598)\n",
      "('2', -0.0010811204556375742)\n",
      "('8', -0.0005389191792346537)\n",
      "('\\n', -0.029472777619957924)\n",
      "('z', -0.009865573607385159)\n",
      "('=', -0.0009545299108140171)\n",
      "('', -0.001082311267964542)\n",
      "('7', -0.010023260489106178)\n",
      "('\\n', -0.019765209406614304)\n",
      "('\\n', -0.004953018855303526)\n",
      "('Now', -2.1901798248291016)\n",
      "('we', -2.3932206630706787)\n",
      "('have', -1.125974416732788)\n",
      "('all', -1.1017591953277588)\n",
      "('the', -0.23880630731582642)\n",
      "('flower', -4.82960844039917)\n",
      "('counts', -1.0974758863449097)\n",
      "(':', -2.4961795806884766)\n",
      "('\\n', -0.41132792830467224)\n",
      "('-', -3.8103060722351074)\n",
      "('Y', -4.783005237579346)\n",
      "('ell', -1.3240420818328857)\n",
      "('ows', -0.030728192999958992)\n",
      "(':', -0.2866758406162262)\n",
      "('x', -1.0630449056625366)\n",
      "('=', -0.012474252842366695)\n",
      "('', -0.0030976191628724337)\n",
      "('1', -0.0005304598598740995)\n",
      "('0', -0.00032884435495361686)\n",
      "('\\n', -0.04325646534562111)\n",
      "('-', -0.00139639584813267)\n",
      "('Pur', -0.004402469377964735)\n",
      "('ples', -0.11351300776004791)\n",
      "(':', -0.0009971652179956436)\n",
      "('y', -0.005833149887621403)\n",
      "('=', -0.0010151477763429284)\n",
      "('', -0.001100768567994237)\n",
      "('1', -0.000433112756581977)\n",
      "('8', -0.0010829067323356867)\n",
      "('\\n', -0.003001472447067499)\n",
      "('-', -0.00045599075383506715)\n",
      "('Gre', -0.015451600775122643)\n",
      "('ens', -3.123234637314454e-05)\n",
      "(':', -0.0010419422760605812)\n",
      "('z', -0.0034265159629285336)\n",
      "('=', -0.0009783486602827907)\n",
      "('', -0.0005142558366060257)\n",
      "('7', -0.0010027624666690826)\n",
      "('\\n', -0.49500399827957153)\n",
      "('\\n', -0.5176385045051575)\n",
      "('So', -2.2203927040100098)\n",
      "(',', -0.6722031235694885)\n",
      "('the', -0.39770016074180603)\n",
      "('total', -0.04863297939300537)\n",
      "('number', -0.028027905151247978)\n",
      "('of', -0.002666966989636421)\n",
      "('flowers', -0.00869311299175024)\n",
      "('in', -1.0385684967041016)\n",
      "('Mark', -0.7032866477966309)\n",
      "(\"'\", -0.012869163416326046)\n",
      "('s', -0.0002215855201939121)\n",
      "('garden', -0.0013583013787865639)\n",
      "('is', -0.0585569404065609)\n",
      "(':', -0.7495228052139282)\n",
      "('\\n', -0.20332613587379456)\n",
      "('\\n', -0.306758850812912)\n",
      "('x', -0.5678300261497498)\n",
      "('+', -0.00356300943531096)\n",
      "('y', -0.000731558189727366)\n",
      "('+', -0.00045348849380388856)\n",
      "('z', -0.000764792668633163)\n",
      "('=', -0.5794926285743713)\n",
      "('', -0.03526405245065689)\n",
      "('1', -0.006164702586829662)\n",
      "('0', -0.0035501806996762753)\n",
      "('+', -0.0012965138303115964)\n",
      "('', -0.0005727558163926005)\n",
      "('1', -0.0002044230350293219)\n",
      "('8', -0.0001998939987970516)\n",
      "('+', -0.00044288364006206393)\n",
      "('', -0.0007256020326167345)\n",
      "('7', -0.0005178302526473999)\n",
      "('=', -0.578375518321991)\n",
      "('', -0.018948612734675407)\n",
      "('3', -0.0016975291073322296)\n",
      "('5', -0.000536655425094068)\n",
      "('\\n', -0.478780061006546)\n",
      "('\\n', -0.03417953848838806)\n",
      "('There', -0.7517578601837158)\n",
      "('fore', -0.017400581389665604)\n",
      "(',', -0.008370318450033665)\n",
      "('Mark', -0.1533832848072052)\n",
      "('has', -0.022976014763116837)\n",
      "('', -0.11488719284534454)\n",
      "('3', -0.0004002247005701065)\n",
      "('5', -9.417090768693015e-05)\n",
      "('flowers', -0.0060415975749492645)\n",
      "('in', -0.010941509157419205)\n",
      "('his', -0.020663348957896233)\n",
      "('garden', -0.000634111522231251)\n",
      "('.', -0.29401230812072754)\n",
      "('[', -6.6210126876831055)\n",
      "('INST', -11.904136657714844)\n",
      "(']', -4.8453288078308105)\n",
      "('Q', -7.979267597198486)\n",
      "(':', -2.536184072494507)\n",
      "('Mark', -8.783544540405273)\n",
      "('has', -6.009649276733398)\n",
      "('a', -1.7091089487075806)\n",
      "('garden', -7.529897212982178)\n",
      "('with', -1.8171358108520508)\n",
      "('flowers', -4.945797443389893)\n",
      "('.', -1.3876492977142334)\n",
      "('He', -1.4655823707580566)\n",
      "('planted', -3.3684661388397217)\n",
      "('plants', -7.450453281402588)\n",
      "('of', -1.8496564626693726)\n",
      "('three', -2.0832767486572266)\n",
      "('different', -0.7003024220466614)\n",
      "('colors', -1.8695447444915771)\n",
      "('in', -2.3152263164520264)\n",
      "('it', -1.873545527458191)\n",
      "('.', -0.6123096942901611)\n",
      "('Ten', -8.892126083374023)\n",
      "('of', -1.2373195886611938)\n",
      "('them', -1.92722487449646)\n",
      "('are', -0.32159745693206787)\n",
      "('yellow', -2.268256187438965)\n",
      "(',', -0.3485907018184662)\n",
      "('and', -3.0524158477783203)\n",
      "('there', -3.0309739112854004)\n",
      "('are', -0.13086363673210144)\n",
      "('', -1.6398487091064453)\n",
      "('8', -2.800138473510742)\n",
      "('0', -2.380390167236328)\n",
      "('%', -0.8139203190803528)\n",
      "('more', -0.3197389543056488)\n",
      "('of', -1.813336730003357)\n",
      "('those', -4.40558385848999)\n",
      "('in', -4.542832374572754)\n",
      "('purple', -7.778336048126221)\n",
      "('.', -1.0508742332458496)\n",
      "('There', -2.0358195304870605)\n",
      "('are', -0.10865125060081482)\n",
      "('only', -4.447059631347656)\n",
      "('', -0.7551405429840088)\n",
      "('2', -1.4833123683929443)\n",
      "('5', -2.3355507850646973)\n",
      "('%', -0.4344860017299652)\n",
      "('as', -4.011203765869141)\n",
      "('many', -0.016702106222510338)\n",
      "('green', -2.60488224029541)\n",
      "('flowers', -1.3441193103790283)\n",
      "('as', -0.20549799501895905)\n",
      "('there', -1.3967803716659546)\n",
      "('are', -0.020905621349811554)\n",
      "('yellow', -0.6963631510734558)\n",
      "('and', -3.5187976360321045)\n",
      "('purple', -0.2720828354358673)\n",
      "('flowers', -1.0190436840057373)\n",
      "('.', -1.9251680374145508)\n",
      "('How', -0.6719295382499695)\n",
      "('many', -0.010257150046527386)\n",
      "('flowers', -0.8930326700210571)\n",
      "('does', -1.2475383281707764)\n",
      "('Mark', -0.5434073805809021)\n",
      "('have', -0.09688587486743927)\n",
      "('in', -0.4096667170524597)\n",
      "('his', -0.5959460735321045)\n",
      "('garden', -0.008123807609081268)\n",
      "('?', -0.1642107218503952)\n",
      "('\\n', -1.0198670625686646)\n",
      "('A', -2.2747488021850586)\n",
      "(':', -0.1398182064294815)\n",
      "('Let', -5.492328643798828)\n",
      "(\"'\", -2.7145376205444336)\n",
      "('s', -0.00506463460624218)\n",
      "('think', -5.845344066619873)\n",
      "('step', -5.557998180389404)\n",
      "('by', -0.2061532735824585)\n",
      "('step', -0.0013319916324689984)\n",
      "('.', -0.44106510281562805)\n",
      "('[', -5.587679862976074)\n",
      "('/', -3.288726329803467)\n",
      "('INST', -0.44636476039886475)\n",
      "(']', -0.03333093225955963)\n",
      "('Let', -5.828550815582275)\n",
      "(\"'\", -0.3097374141216278)\n",
      "('s', -0.0006478118011727929)\n",
      "('denote', -3.3618202209472656)\n",
      "('the', -0.552661657333374)\n",
      "('number', -0.30010750889778137)\n",
      "('of', -0.00716071343049407)\n",
      "('purple', -2.0062437057495117)\n",
      "('flowers', -0.2460932731628418)\n",
      "('as', -1.3509364128112793)\n",
      "('x', -1.7672120332717896)\n",
      "(',', -1.2356057167053223)\n",
      "('and', -0.86167973279953)\n",
      "('the', -0.3021091818809509)\n",
      "('number', -0.027479710057377815)\n",
      "('of', -0.003751385258510709)\n",
      "('green', -0.18689166009426117)\n",
      "('flowers', -0.14141900837421417)\n",
      "('as', -0.045704539865255356)\n",
      "('y', -0.04799780249595642)\n",
      "('.', -0.09483497589826584)\n",
      "('We', -2.0995755195617676)\n",
      "('know', -0.8878611326217651)\n",
      "('that', -0.30773159861564636)\n",
      "(':', -2.2730820178985596)\n",
      "('\\n', -0.988530695438385)\n",
      "('\\n', -1.4332751035690308)\n",
      "('1', -1.8347084522247314)\n",
      "('.', -0.9663175344467163)\n",
      "('The', -2.4251790046691895)\n",
      "('number', -0.5741516947746277)\n",
      "('of', -0.006236023269593716)\n",
      "('yellow', -0.5306484699249268)\n",
      "('flowers', -0.033186912536621094)\n",
      "('is', -0.2373960167169571)\n",
      "('', -0.20544588565826416)\n",
      "('1', -0.022745082154870033)\n",
      "('0', -0.005837179720401764)\n",
      "('.', -0.5255669355392456)\n",
      "('\\n', -0.14469493925571442)\n",
      "('2', -0.15193405747413635)\n",
      "('.', -0.0004217927053105086)\n",
      "('The', -0.6607859134674072)\n",
      "('number', -0.14912603795528412)\n",
      "('of', -0.005560051649808884)\n",
      "('purple', -0.14980275928974152)\n",
      "('flowers', -0.02159545011818409)\n",
      "('is', -0.04083557426929474)\n",
      "('', -1.4541354179382324)\n",
      "('1', -0.5002666115760803)\n",
      "('0', -0.9374160170555115)\n",
      "('*', -3.184492349624634)\n",
      "('', -0.2099706530570984)\n",
      "('1', -0.7515836358070374)\n",
      "('.', -0.22884339094161987)\n",
      "('8', -0.9267001152038574)\n",
      "('=', -0.2260112315416336)\n",
      "('', -0.027095982804894447)\n",
      "('1', -0.0040926518850028515)\n",
      "('8', -0.003371984465047717)\n",
      "('(', -4.207042694091797)\n",
      "('since', -3.08538818359375)\n",
      "('there', -0.6282772421836853)\n",
      "('are', -0.10296032577753067)\n",
      "('', -0.0315559059381485)\n",
      "('8', -0.09026589244604111)\n",
      "('0', -0.0013184197014197707)\n",
      "('%', -0.016630707308650017)\n",
      "('more', -0.016620270907878876)\n",
      "('purple', -0.7008737325668335)\n",
      "('flowers', -0.18285876512527466)\n",
      "('than', -0.2643957734107971)\n",
      "('yellow', -0.10652513802051544)\n",
      "(').', -1.3630602359771729)\n",
      "('\\n', -0.021710356697440147)\n",
      "('3', -0.005066532175987959)\n",
      "('.', -0.00010680581908673048)\n",
      "('The', -0.1271505057811737)\n",
      "('number', -0.06418870389461517)\n",
      "('of', -0.0014340127818286419)\n",
      "('green', -0.05860022455453873)\n",
      "('flowers', -0.006661233492195606)\n",
      "('is', -0.01678076758980751)\n",
      "('', -0.317746639251709)\n",
      "('1', -0.16568712890148163)\n",
      "('0', -0.13732905685901642)\n",
      "('*', -0.27861952781677246)\n",
      "('', -0.07598164677619934)\n",
      "('0', -0.3575723469257355)\n",
      "('.', -0.004020703490823507)\n",
      "('2', -0.06499075144529343)\n",
      "('5', -0.0027558940928429365)\n",
      "('=', -0.09534411877393723)\n",
      "('', -0.0027027528267353773)\n",
      "('2', -0.004616080317646265)\n",
      "('.', -0.0060808174312114716)\n",
      "('5', -0.0026702960021793842)\n",
      "(',', -4.929721355438232)\n",
      "('but', -5.506434440612793)\n",
      "('since', -2.128706455230713)\n",
      "('we', -1.3121626377105713)\n",
      "('cannot', -4.6136956214904785)\n",
      "('have', -0.10167776048183441)\n",
      "('a', -1.1231476068496704)\n",
      "('fraction', -0.8309322595596313)\n",
      "('of', -0.7188529372215271)\n",
      "('a', -0.19193996489048004)\n",
      "('flower', -0.0545649416744709)\n",
      "(',', -0.1763959676027298)\n",
      "('we', -0.5804754495620728)\n",
      "('need', -2.357558012008667)\n",
      "('to', -0.02214209921658039)\n",
      "('round', -0.3583391308784485)\n",
      "('up', -1.4446156024932861)\n",
      "('or', -5.4864501953125)\n",
      "('down', -0.06636538356542587)\n",
      "('to', -1.5033538341522217)\n",
      "('the', -0.7963510155677795)\n",
      "('nearest', -0.2681770920753479)\n",
      "('whole', -0.5579563975334167)\n",
      "('number', -0.06767896562814713)\n",
      "('.', -0.3363698422908783)\n",
      "('Let', -2.5653114318847656)\n",
      "(\"'\", -0.027932386845350266)\n",
      "('s', -0.00021193164866417646)\n",
      "('assume', -2.7866060733795166)\n",
      "('there', -3.5990660190582275)\n",
      "('are', -0.2384880781173706)\n",
      "('', -0.26581650972366333)\n",
      "('2', -1.1663936376571655)\n",
      "('green', -0.29709237813949585)\n",
      "('flowers', -0.010388467460870743)\n",
      "('.', -0.8449415564537048)\n",
      "('\\n', -0.4989442825317383)\n",
      "('\\n', -1.1748487949371338)\n",
      "('There', -2.93402361869812)\n",
      "('fore', -0.06451275199651718)\n",
      "(',', -0.21824809908866882)\n",
      "('the', -0.647118866443634)\n",
      "('total', -0.4027046859264374)\n",
      "('number', -0.030777670443058014)\n",
      "('of', -0.008066102862358093)\n",
      "('flowers', -0.021024225279688835)\n",
      "('in', -1.0998163223266602)\n",
      "('Mark', -0.720078706741333)\n",
      "(\"'\", -0.014588592574000359)\n",
      "('s', -0.00030060065910220146)\n",
      "('garden', -0.0030841899570077658)\n",
      "('is', -0.09687840193510056)\n",
      "(':', -1.5234549045562744)\n",
      "('\\n', -0.6548973917961121)\n",
      "('\\n', -0.21039707958698273)\n",
      "('Total', -4.871701717376709)\n",
      "('Flow', -3.900055408477783)\n",
      "('ers', -6.723177648382261e-05)\n",
      "('=', -0.0753835141658783)\n",
      "('Yellow', -1.6184951066970825)\n",
      "('Flow', -0.6110444664955139)\n",
      "('ers', -4.9828242481453344e-05)\n",
      "('+', -0.016943462193012238)\n",
      "('Pur', -0.14346648752689362)\n",
      "('ple', -0.00031871485407464206)\n",
      "('Flow', -0.008202910423278809)\n",
      "('ers', -3.123234637314454e-05)\n",
      "('+', -0.0023030920419842005)\n",
      "('Green', -0.0031343402806669474)\n",
      "('Flow', -0.011670058593153954)\n",
      "('ers', -8.4638240878121e-06)\n",
      "('\\n', -1.1021206378936768)\n",
      "('Total', -1.0666297674179077)\n",
      "('Flow', -0.006717361975461245)\n",
      "('ers', -1.6689286894688848e-06)\n",
      "('=', -0.0022820401936769485)\n",
      "('', -0.015529656782746315)\n",
      "('1', -0.004457537550479174)\n",
      "('0', -0.003942219074815512)\n",
      "('+', -0.006903368514031172)\n",
      "('', -0.005850571673363447)\n",
      "('1', -0.002132048597559333)\n",
      "('8', -0.006008774973452091)\n",
      "('+', -0.001904817996546626)\n",
      "('', -0.003185557434335351)\n",
      "('2', -0.0032619622070342302)\n",
      "('=', -0.5566851496696472)\n",
      "('<', -10.928194999694824)\n",
      "('box', -6.075511932373047)\n",
      "('>', -0.4910196363925934)\n",
      "('3', -0.8624932765960693)\n",
      "('0', -0.000764792668633163)\n",
      "('</', -0.38650062680244446)\n",
      "('box', -0.0017551269847899675)\n",
      "('>', -0.10822095721960068)\n",
      "('\\n', -0.8016119003295898)\n",
      "('\\n', -0.17140543460845947)\n",
      "('So', -3.2901601791381836)\n",
      "(',', -0.6379988193511963)\n",
      "('Mark', -0.9601368308067322)\n",
      "('has', -0.08240184187889099)\n",
      "('', -0.442310631275177)\n",
      "('3', -0.002370525849983096)\n",
      "('0', -0.0006847421173006296)\n",
      "('flowers', -0.02426823042333126)\n",
      "('in', -0.0542198084294796)\n",
      "('his', -0.08174370974302292)\n",
      "('garden', -0.0018842339050024748)\n",
      "('.', -0.1617657095193863)\n",
      "('[', -6.6210126876831055)\n",
      "('INST', -11.904136657714844)\n",
      "(']', -4.8453288078308105)\n",
      "('Q', -7.979267597198486)\n",
      "(':', -2.536184072494507)\n",
      "('Mark', -8.783544540405273)\n",
      "('has', -6.009649276733398)\n",
      "('a', -1.7091089487075806)\n",
      "('garden', -7.529897212982178)\n",
      "('with', -1.8171358108520508)\n",
      "('flowers', -4.945797443389893)\n",
      "('.', -1.3876492977142334)\n",
      "('He', -1.4655823707580566)\n",
      "('planted', -3.3684661388397217)\n",
      "('plants', -7.450453281402588)\n",
      "('of', -1.8496564626693726)\n",
      "('three', -2.0832767486572266)\n",
      "('different', -0.7003024220466614)\n",
      "('colors', -1.8695447444915771)\n",
      "('in', -2.3152263164520264)\n",
      "('it', -1.873545527458191)\n",
      "('.', -0.6123096942901611)\n",
      "('Ten', -8.892126083374023)\n",
      "('of', -1.2373195886611938)\n",
      "('them', -1.92722487449646)\n",
      "('are', -0.32159745693206787)\n",
      "('yellow', -2.268256187438965)\n",
      "(',', -0.3485907018184662)\n",
      "('and', -3.0524158477783203)\n",
      "('there', -3.0309739112854004)\n",
      "('are', -0.13086363673210144)\n",
      "('', -1.6398487091064453)\n",
      "('8', -2.800138473510742)\n",
      "('0', -2.380390167236328)\n",
      "('%', -0.8139203190803528)\n",
      "('more', -0.3197389543056488)\n",
      "('of', -1.813336730003357)\n",
      "('those', -4.40558385848999)\n",
      "('in', -4.542832374572754)\n",
      "('purple', -7.778336048126221)\n",
      "('.', -1.0508742332458496)\n",
      "('There', -2.0358195304870605)\n",
      "('are', -0.10865125060081482)\n",
      "('only', -4.447059631347656)\n",
      "('', -0.7551405429840088)\n",
      "('2', -1.4833123683929443)\n",
      "('5', -2.3355507850646973)\n",
      "('%', -0.4344860017299652)\n",
      "('as', -4.011203765869141)\n",
      "('many', -0.016702106222510338)\n",
      "('green', -2.60488224029541)\n",
      "('flowers', -1.3441193103790283)\n",
      "('as', -0.20549799501895905)\n",
      "('there', -1.3967803716659546)\n",
      "('are', -0.020905621349811554)\n",
      "('yellow', -0.6963631510734558)\n",
      "('and', -3.5187976360321045)\n",
      "('purple', -0.2720828354358673)\n",
      "('flowers', -1.0190436840057373)\n",
      "('.', -1.9251680374145508)\n",
      "('How', -0.6719295382499695)\n",
      "('many', -0.010257150046527386)\n",
      "('flowers', -0.8930326700210571)\n",
      "('does', -1.2475383281707764)\n",
      "('Mark', -0.5434073805809021)\n",
      "('have', -0.09688587486743927)\n",
      "('in', -0.4096667170524597)\n",
      "('his', -0.5959460735321045)\n",
      "('garden', -0.008123807609081268)\n",
      "('?', -0.1642107218503952)\n",
      "('\\n', -1.0198670625686646)\n",
      "('A', -2.2747488021850586)\n",
      "(':', -0.1398182064294815)\n",
      "('Let', -5.492328643798828)\n",
      "(\"'\", -2.7145376205444336)\n",
      "('s', -0.00506463460624218)\n",
      "('think', -5.845344066619873)\n",
      "('step', -5.557998180389404)\n",
      "('by', -0.2061532735824585)\n",
      "('step', -0.0013319916324689984)\n",
      "('.', -0.44106510281562805)\n",
      "('[', -5.587679862976074)\n",
      "('/', -3.288726329803467)\n",
      "('INST', -0.44636476039886475)\n",
      "(']', -0.03333093225955963)\n",
      "('Let', -5.828550815582275)\n",
      "(\"'\", -0.3097374141216278)\n",
      "('s', -0.0006478118011727929)\n",
      "('denote', -3.3618202209472656)\n",
      "('the', -0.552661657333374)\n",
      "('number', -0.30010750889778137)\n",
      "('of', -0.00716071343049407)\n",
      "('purple', -2.0062437057495117)\n",
      "('flowers', -0.2460932731628418)\n",
      "('as', -1.3509364128112793)\n",
      "('X', -3.5797119140625)\n",
      "('.', -0.8135250806808472)\n",
      "('We', -2.503535032272339)\n",
      "('know', -0.5466269254684448)\n",
      "('that', -0.235506072640419)\n",
      "('there', -1.188977599143982)\n",
      "('are', -0.05334747955203056)\n",
      "('', -0.23353438079357147)\n",
      "('8', -0.883377194404602)\n",
      "('0', -0.014104916714131832)\n",
      "('%', -0.023571161553263664)\n",
      "('more', -0.04177330806851387)\n",
      "('purple', -0.472845196723938)\n",
      "('flowers', -0.11467993259429932)\n",
      "('than', -0.1517363041639328)\n",
      "('yellow', -0.11299185454845428)\n",
      "('ones', -1.355689525604248)\n",
      "(',', -1.1884453296661377)\n",
      "('so', -0.6221081614494324)\n",
      "('the', -2.591391086578369)\n",
      "('number', -0.43318799138069153)\n",
      "('of', -0.017132030799984932)\n",
      "('yellow', -0.12759855389595032)\n",
      "('flowers', -0.15955732762813568)\n",
      "('is', -0.30194032192230225)\n",
      "('X', -1.7193713188171387)\n",
      "('/', -0.5019617676734924)\n",
      "('1', -1.2231287956237793)\n",
      "('.', -0.12854225933551788)\n",
      "('8', -1.3384007215499878)\n",
      "('(', -3.4969286918640137)\n",
      "('since', -2.789588689804077)\n",
      "('', -0.6394038796424866)\n",
      "('8', -0.32526037096977234)\n",
      "('0', -0.0084431366994977)\n",
      "('%', -0.06087927520275116)\n",
      "('is', -1.5326353311538696)\n",
      "('equivalent', -3.7312841415405273)\n",
      "('to', -0.018447380512952805)\n",
      "('', -0.12797170877456665)\n",
      "('0', -1.2837247848510742)\n",
      "('.', -0.006652115378528833)\n",
      "('8', -0.0025724435690790415)\n",
      "('or', -2.265021800994873)\n",
      "('', -0.10829123109579086)\n",
      "('1', -1.281385898590088)\n",
      "('/', -0.30694839358329773)\n",
      "('1', -0.01639373041689396)\n",
      "('.', -0.002160359639674425)\n",
      "('8', -1.31783926486969)\n",
      "(').', -0.20898382365703583)\n",
      "('We', -1.3961950540542603)\n",
      "('also', -0.3675118088722229)\n",
      "('know', -0.03580719232559204)\n",
      "('that', -0.04200857877731323)\n",
      "('there', -0.5255423784255981)\n",
      "('are', -0.052286479622125626)\n",
      "('', -0.24722802639007568)\n",
      "('2', -0.10498487204313278)\n",
      "('5', -0.005429995711892843)\n",
      "('%', -0.004343243315815926)\n",
      "('as', -0.1847400963306427)\n",
      "('many', -0.005498642101883888)\n",
      "('green', -0.0442594513297081)\n",
      "('flowers', -0.05251216143369675)\n",
      "('as', -0.022627614438533783)\n",
      "('there', -0.3381553888320923)\n",
      "('are', -0.005138405133038759)\n",
      "('yellow', -0.1432424634695053)\n",
      "('and', -0.12768343091011047)\n",
      "('purple', -0.006062570493668318)\n",
      "('flowers', -0.3920179307460785)\n",
      "('combined', -2.525510311126709)\n",
      "('.', -0.6573874354362488)\n",
      "('Let', -2.3014988899230957)\n",
      "(\"'\", -0.15510515868663788)\n",
      "('s', -0.00028010259848088026)\n",
      "('denote', -1.1069506406784058)\n",
      "('the', -0.09470952302217484)\n",
      "('number', -0.04556502401828766)\n",
      "('of', -0.0021849824115633965)\n",
      "('green', -0.03179578483104706)\n",
      "('flowers', -0.015162097290158272)\n",
      "('as', -0.11398836970329285)\n",
      "('G', -1.0922513008117676)\n",
      "('.', -0.11124012619256973)\n",
      "('\\n', -3.3520307540893555)\n",
      "('\\n', -0.9750208854675293)\n",
      "('First', -4.850003242492676)\n",
      "(',', -0.3171460032463074)\n",
      "('let', -0.8610642552375793)\n",
      "(\"'\", -0.021500833332538605)\n",
      "('s', -0.00013481661153491586)\n",
      "('find', -1.1472607851028442)\n",
      "('the', -0.5610641837120056)\n",
      "('total', -1.209100604057312)\n",
      "('number', -0.048487961292266846)\n",
      "('of', -0.011284810490906239)\n",
      "('yellow', -2.3883450031280518)\n",
      "('and', -0.16552883386611938)\n",
      "('purple', -0.0070386785082519054)\n",
      "('flowers', -0.015200842171907425)\n",
      "(':', -2.029690980911255)\n",
      "('\\n', -0.6364620327949524)\n",
      "('Total', -4.855271339416504)\n",
      "('yellow', -2.9772119522094727)\n",
      "('and', -0.7502543926239014)\n",
      "('purple', -0.0016746795736253262)\n",
      "('flowers', -0.12797003984451294)\n",
      "('=', -0.1827326864004135)\n",
      "('X', -0.5852670669555664)\n",
      "('+', -0.37871745228767395)\n",
      "('X', -0.3231300711631775)\n",
      "('/', -0.04018488898873329)\n",
      "('1', -0.002052345545962453)\n",
      "('.', -0.0012188870459794998)\n",
      "('8', -0.001007287879474461)\n",
      "('\\n', -1.8130601644515991)\n",
      "('\\n', -1.0716767311096191)\n",
      "('Now', -1.3060755729675293)\n",
      "(',', -0.6433623433113098)\n",
      "('let', -0.7257127165794373)\n",
      "(\"'\", -0.004644202534109354)\n",
      "('s', -0.0002393436006968841)\n",
      "('find', -0.42247432470321655)\n",
      "('the', -0.21281376481056213)\n",
      "('number', -0.6172942519187927)\n",
      "('of', -0.0029654596000909805)\n",
      "('green', -0.04911556467413902)\n",
      "('flowers', -0.006226189900189638)\n",
      "(':', -0.29971176385879517)\n",
      "('\\n', -0.06136821210384369)\n",
      "('Green', -1.7086803913116455)\n",
      "('flowers', -0.02608514577150345)\n",
      "('=', -0.02261188253760338)\n",
      "('', -0.3930356204509735)\n",
      "('0', -0.42469945549964905)\n",
      "('.', -0.0008231588872149587)\n",
      "('2', -0.0025572238955646753)\n",
      "('5', -0.00077622797107324)\n",
      "('*', -0.9059852957725525)\n",
      "('(', -0.2808191478252411)\n",
      "('X', -1.2121655941009521)\n",
      "('+', -0.02374812215566635)\n",
      "('X', -0.0121576227247715)\n",
      "('/', -0.006221214309334755)\n",
      "('1', -0.0009077242575585842)\n",
      "('.', -0.0013943722005933523)\n",
      "('8', -0.0005701346672140062)\n",
      "(')', -0.015522966161370277)\n",
      "('\\n', -0.3648027181625366)\n",
      "('\\n', -0.5836021304130554)\n",
      "('Since', -2.446453809738159)\n",
      "('we', -0.9470531940460205)\n",
      "('know', -0.707095205783844)\n",
      "('that', -0.26562657952308655)\n",
      "('Mark', -2.4929652214050293)\n",
      "('has', -0.21038877964019775)\n",
      "('ten', -3.7557098865509033)\n",
      "('yellow', -0.06802137941122055)\n",
      "('flowers', -0.016217222437262535)\n",
      "(',', -0.2800096869468689)\n",
      "('we', -0.45161330699920654)\n",
      "('can', -0.3006855547428131)\n",
      "('set', -3.604649066925049)\n",
      "('up', -0.6275855898857117)\n",
      "('an', -1.179775595664978)\n",
      "('equation', -0.02066848799586296)\n",
      "('to', -1.432875156402588)\n",
      "('find', -0.4768082797527313)\n",
      "('the', -0.5873129963874817)\n",
      "('value', -1.7522369623184204)\n",
      "('of', -0.04797246307134628)\n",
      "('X', -0.03728383034467697)\n",
      "(':', -0.4290975034236908)\n",
      "('\\n', -0.020696278661489487)\n",
      "('X', -1.0184401273727417)\n",
      "('+', -0.6676080226898193)\n",
      "('X', -0.05250537768006325)\n",
      "('/', -0.006523266900330782)\n",
      "('1', -0.0007325111655518413)\n",
      "('.', -0.0015728019643574953)\n",
      "('8', -0.0002771231811493635)\n",
      "('=', -0.7956451177597046)\n",
      "('', -0.05488366261124611)\n",
      "('1', -0.07186394184827805)\n",
      "('0', -0.05318019911646843)\n",
      "('+', -1.3287630081176758)\n",
      "('X', -4.1386895179748535)\n",
      "('/', -0.907884955406189)\n",
      "('1', -0.628294050693512)\n",
      "('.', -0.05970586836338043)\n",
      "('8', -0.12656961381435394)\n",
      "('+', -1.1109800338745117)\n",
      "('', -0.3165900707244873)\n",
      "('0', -0.026080965995788574)\n",
      "('.', -0.00025042734341695905)\n",
      "('2', -0.0024594792630523443)\n",
      "('5', -0.0002584123285487294)\n",
      "('*', -0.4565090835094452)\n",
      "('(', -0.0744672641158104)\n",
      "('1', -2.82025408744812)\n",
      "('0', -0.004930955357849598)\n",
      "('+', -0.009153190068900585)\n",
      "('X', -0.007505078334361315)\n",
      "('/', -0.011730501428246498)\n",
      "('1', -0.0007298904820345342)\n",
      "('.', -0.0009683448588475585)\n",
      "('8', -0.0002108589978888631)\n",
      "(')', -0.018277086317539215)\n",
      "('\\n', -0.047797124832868576)\n",
      "('\\n', -0.9328149557113647)\n",
      "('Let', -3.638230323791504)\n",
      "(\"'\", -0.00910026952624321)\n",
      "('s', -0.00022063204960431904)\n",
      "('simpl', -0.8704517483711243)\n",
      "('ify', -0.0005358214257284999)\n",
      "('the', -0.9764007925987244)\n",
      "('equation', -0.329451322555542)\n",
      "(':', -0.5419158339500427)\n",
      "('\\n', -0.014248539693653584)\n",
      "('X', -0.46265485882759094)\n",
      "('+', -0.2353830337524414)\n",
      "('X', -0.1783808320760727)\n",
      "('/', -0.033569347113370895)\n",
      "('1', -0.0007085673278197646)\n",
      "('.', -0.0006544831558130682)\n",
      "('8', -9.893881360767409e-05)\n",
      "('=', -0.17463752627372742)\n",
      "('', -0.08534608036279678)\n",
      "('1', -0.08362853527069092)\n",
      "('0', -0.04599467292428017)\n",
      "('+', -0.04905722662806511)\n",
      "('X', -0.4060584306716919)\n",
      "('/', -0.052348703145980835)\n",
      "('1', -0.0019819156732410192)\n",
      "('.', -0.0005080600967630744)\n",
      "('8', -0.0005277195014059544)\n",
      "('+', -0.016755446791648865)\n",
      "('', -0.0886412262916565)\n",
      "('2', -0.2272055596113205)\n",
      "('.', -0.004984571132808924)\n",
      "('5', -0.00139639584813267)\n",
      "('*', -2.018066167831421)\n",
      "('(', -0.36908233165740967)\n",
      "('1', -0.13159099221229553)\n",
      "('0', -0.0014559156261384487)\n",
      "('+', -0.005035100970417261)\n",
      "('X', -0.0013846105430275202)\n",
      "('/', -0.001955741085112095)\n",
      "('1', -0.00023314618738368154)\n",
      "('.', -0.0012729407753795385)\n",
      "('8', -6.687417771900073e-05)\n",
      "(')', -0.003438871121034026)\n",
      "('\\n', -0.016693664714694023)\n",
      "('X', -1.5197663307189941)\n",
      "('+', -0.44001078605651855)\n",
      "('X', -0.0937984511256218)\n",
      "('/', -0.043141841888427734)\n",
      "('1', -0.0005486890440806746)\n",
      "('.', -0.0006610353593714535)\n",
      "('8', -0.00017569905321579427)\n",
      "('=', -0.07775527238845825)\n",
      "('', -0.052745938301086426)\n",
      "('1', -0.31629678606987)\n",
      "('0', -0.0820809006690979)\n",
      "('+', -0.024495437741279602)\n",
      "('X', -0.4379728138446808)\n",
      "('/', -0.015157165005803108)\n",
      "('1', -0.0007052318542264402)\n",
      "('.', -0.0003084660565946251)\n",
      "('8', -0.00043644916149787605)\n",
      "('+', -0.0063441782258450985)\n",
      "('', -0.009156261570751667)\n",
      "('2', -0.014979968778789043)\n",
      "('5', -0.04090629518032074)\n",
      "('+', -0.2289908528327942)\n",
      "('', -0.31144189834594727)\n",
      "('2', -0.086663618683815)\n",
      "('.', -0.6394178867340088)\n",
      "('5', -0.0009098681039176881)\n",
      "('*', -0.34613776206970215)\n",
      "('X', -0.01493111439049244)\n",
      "('\\n', -0.2202674150466919)\n",
      "('X', -0.618500292301178)\n",
      "('=', -1.2604434490203857)\n",
      "('', -0.09959956258535385)\n",
      "('2', -1.777564287185669)\n",
      "('5', -0.24475890398025513)\n",
      "('\\n', -1.056727647781372)\n",
      "('\\n', -0.10919646918773651)\n",
      "('Now', -1.6876797676086426)\n",
      "(',', -0.5255047082901001)\n",
      "('we', -0.5702484250068665)\n",
      "('can', -0.7328281402587891)\n",
      "('find', -0.6785939335823059)\n",
      "('the', -0.02720770612359047)\n",
      "('number', -1.7258750200271606)\n",
      "('of', -0.0004182179400231689)\n",
      "('yellow', -2.6112112998962402)\n",
      "('and', -1.4880611896514893)\n",
      "('purple', -0.01625921204686165)\n",
      "('flowers', -0.003763380227610469)\n",
      "(':', -0.4186022877693176)\n",
      "('\\n', -0.0297038983553648)\n",
      "('Total', -0.816205620765686)\n",
      "('yellow', -0.05696333572268486)\n",
      "('and', -0.012065052054822445)\n",
      "('purple', -0.0006556744920089841)\n",
      "('flowers', -0.0066685751080513)\n",
      "('=', -0.003071236191317439)\n",
      "('', -0.8357715010643005)\n",
      "('2', -0.07957573235034943)\n",
      "('5', -0.009016748517751694)\n",
      "('+', -0.008148756809532642)\n",
      "('', -0.01897270791232586)\n",
      "('2', -0.0389673225581646)\n",
      "('5', -0.0007895689341239631)\n",
      "('/', -0.028134772554039955)\n",
      "('1', -0.0008506731828674674)\n",
      "('.', -0.0004273931554052979)\n",
      "('8', -0.00033623288618400693)\n",
      "('=', -1.065279245376587)\n",
      "('', -0.004858236759901047)\n",
      "('3', -1.630972146987915)\n",
      "('8', -3.8864965438842773)\n",
      "('.', -0.1444557160139084)\n",
      "('8', -0.21841216087341309)\n",
      "('8', -0.2662348747253418)\n",
      "('5', -3.4383645057678223)\n",
      "('…', -5.1646833419799805)\n",
      "('', -4.273046016693115)\n",
      "('≈', -0.03908884897828102)\n",
      "('', -0.007505196612328291)\n",
      "('3', -0.010601392947137356)\n",
      "('9', -0.0793653279542923)\n",
      "('\\n', -0.2635243833065033)\n",
      "('\\n', -0.04722634702920914)\n",
      "('Since', -3.432039976119995)\n",
      "('we', -0.7771400809288025)\n",
      "('can', -7.947739601135254)\n",
      "(\"'\", -3.5246949195861816)\n",
      "('t', -4.124556289752945e-05)\n",
      "('have', -0.08055253326892853)\n",
      "('a', -0.7832348942756653)\n",
      "('fraction', -0.5205565094947815)\n",
      "('of', -0.8603809475898743)\n",
      "('a', -0.06454918533563614)\n",
      "('flower', -0.04730162024497986)\n",
      "(',', -0.056467171758413315)\n",
      "('we', -0.7264766693115234)\n",
      "(\"'\", -2.1589910984039307)\n",
      "('ll', -0.013721075840294361)\n",
      "('round', -0.30511435866355896)\n",
      "('up', -1.500109076499939)\n",
      "('to', -0.33540642261505127)\n",
      "('', -0.27548903226852417)\n",
      "('4', -0.07956923544406891)\n",
      "('0', -0.0006687788991257548)\n",
      "('.', -0.22755493223667145)\n",
      "('\\n', -0.3609572649002075)\n",
      "('\\n', -0.1012909859418869)\n",
      "('The', -3.197481632232666)\n",
      "('number', -0.7398051619529724)\n",
      "('of', -0.0005073451902717352)\n",
      "('yellow', -2.7950870990753174)\n",
      "('flowers', -0.08349717408418655)\n",
      "('is', -0.30041590332984924)\n",
      "('', -0.4989774227142334)\n",
      "('4', -1.374996304512024)\n",
      "('0', -0.0010093123419210315)\n",
      "('/', -0.8349242806434631)\n",
      "('2', -4.523169040679932)\n",
      "('=', -0.4416908919811249)\n",
      "('', -0.0005480932886712253)\n",
      "('2', -0.0013180625392124057)\n",
      "('0', -0.0008715168223716319)\n",
      "('.', -0.43633610010147095)\n",
      "('\\n', -0.11561169475317001)\n",
      "('\\n', -0.9305533766746521)\n",
      "('Now', -2.7551183700561523)\n",
      "(',', -0.08355638384819031)\n",
      "('we', -0.7669018507003784)\n",
      "('can', -0.12004829943180084)\n",
      "('find', -0.15598084032535553)\n",
      "('the', -0.004750869236886501)\n",
      "('number', -0.05971361696720123)\n",
      "('of', -0.00028892155387438834)\n",
      "('purple', -3.65645694732666)\n",
      "('flowers', -0.036365438252687454)\n",
      "(':', -0.17483581602573395)\n",
      "('\\n', -0.029238130897283554)\n",
      "('P', -0.43833333253860474)\n",
      "('ur', -0.0010996968485414982)\n",
      "('ple', -0.00023112009512260556)\n",
      "('flowers', -0.004382292274385691)\n",
      "('=', -0.0030253613367676735)\n",
      "('', -0.11938582360744476)\n",
      "('4', -0.2536636292934418)\n",
      "('0', -0.0005376085755415261)\n",
      "('*', -3.062605857849121)\n",
      "('', -0.036936357617378235)\n",
      "('1', -0.4985239803791046)\n",
      "('.', -0.016468077898025513)\n",
      "('8', -0.020393455401062965)\n",
      "('=', -0.2872828245162964)\n",
      "('', -0.0008792586741037667)\n",
      "('7', -0.009515998885035515)\n",
      "('2', -0.0020965994335711002)\n",
      "('\\n', -0.21548613905906677)\n",
      "('\\n', -0.012428807094693184)\n",
      "('Fin', -0.9504711031913757)\n",
      "('ally', -1.5258672647178173e-05)\n",
      "(',', -0.0002836778585333377)\n",
      "('we', -0.18942323327064514)\n",
      "('can', -0.05144711211323738)\n",
      "('find', -0.034129537642002106)\n",
      "('the', -0.0013674680376425385)\n",
      "('number', -0.056096211075782776)\n",
      "('of', -0.0002150304353563115)\n",
      "('green', -0.00581229105591774)\n",
      "('flowers', -0.0008208957733586431)\n",
      "(':', -0.03529604151844978)\n",
      "('\\n', -0.0037942577619105577)\n",
      "('Green', -0.016595181077718735)\n",
      "('flowers', -0.0024574578274041414)\n",
      "('=', -0.0006206493126228452)\n",
      "('', -0.0052339909598231316)\n",
      "('0', -0.04239290952682495)\n",
      "('.', -8.821098163025454e-05)\n",
      "('2', -0.0009421439026482403)\n",
      "('5', -0.0001436368766007945)\n",
      "('*', -0.005666621029376984)\n",
      "('', -2.1287240982055664)\n",
      "('4', -0.3379165828227997)\n",
      "('0', -0.004556985571980476)\n",
      "('*', -2.6269099712371826)\n",
      "('', -0.01138546783477068)\n",
      "('1', -0.09019812941551208)\n",
      "('.', -0.0012955614365637302)\n",
      "('8', -0.004623912274837494)\n",
      "('=', -0.04392443597316742)\n",
      "('', -0.001947531825862825)\n",
      "('1', -0.8225299715995789)\n",
      "('8', -0.40046775341033936)\n",
      "('\\n', -0.0808078795671463)\n",
      "('\\n', -0.01496528834104538)\n",
      "('So', -1.6814546585083008)\n",
      "(',', -0.051948145031929016)\n",
      "('Mark', -0.7561362981796265)\n",
      "('has', -0.018149258568882942)\n",
      "('approximately', -7.074007034301758)\n",
      "('', -0.018240097910165787)\n",
      "('4', -0.460842490196228)\n",
      "('0', -0.009272133000195026)\n",
      "('yellow', -0.2063681036233902)\n",
      "('flowers', -0.12422864884138107)\n",
      "(',', -0.004031863994896412)\n",
      "('', -0.013766109943389893)\n",
      "('7', -0.004149280488491058)\n",
      "('2', -0.0048818439245224)\n",
      "('purple', -0.0009606037638150156)\n",
      "('flowers', -0.005370949395000935)\n",
      "(',', -0.026542285457253456)\n",
      "('and', -0.0014642480527982116)\n",
      "('', -0.0022192392498254776)\n",
      "('1', -0.0010724274907261133)\n",
      "('8', -0.0010631391778588295)\n",
      "('green', -0.0009201107313856483)\n",
      "('flowers', -0.002224234864115715)\n",
      "('in', -0.4637957215309143)\n",
      "('his', -0.02372169680893421)\n",
      "('garden', -0.0005757343024015427)\n",
      "('.', -0.166750967502594)\n",
      "('The', -2.6320748329162598)\n",
      "('total', -0.05176309123635292)\n",
      "('number', -0.054654788225889206)\n",
      "('of', -0.008242282085120678)\n",
      "('flowers', -0.003635822329670191)\n",
      "('is', -0.2574571967124939)\n",
      "('around', -4.298410415649414)\n",
      "('', -0.010366994887590408)\n",
      "('1', -0.11316784471273422)\n",
      "('3', -0.13915666937828064)\n",
      "('0', -0.0034249715972691774)\n",
      "('.', -0.0566878542304039)\n"
     ]
    }
   ],
   "source": [
    "btch = to_tokens_and_logprobs(base_model, base_tokenizer, input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
