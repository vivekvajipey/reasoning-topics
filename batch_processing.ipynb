{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekvajipey/miniconda3/envs/nightly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.31s/it]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from util import get_prompt_message, extract_last_integer, extract_last_number\n",
    "from util import remove_last_sentence\n",
    "\n",
    "gpt35_df = pd.read_csv('../conditional/data/112_gsm8k_gpt35_cot_onesent_responses.csv')\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mark has a garden with flowers. He planted plants of three different colors '\n",
      " 'in it. Ten of them are yellow, and there are 80% more of those in purple. '\n",
      " 'There are only 25% as many green flowers as there are yellow and purple '\n",
      " 'flowers. How many flowers does Mark have in his garden?')\n"
     ]
    }
   ],
   "source": [
    "row = gpt35_df.iloc[5]\n",
    "question = row['Question']\n",
    "from pprint import pprint\n",
    "pprint(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 3\n",
    "num_fewshot = 0\n",
    "temp = 0.7\n",
    "direct_prompt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:  Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\n",
      "A: Let's think step by step.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_outputs = []\n",
    "unique_answers = {}\n",
    "messages = get_prompt_message(question, num_fewshot, direct_prompt)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 86]), torch.Size([3, 86]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "input_tensor = input_1.repeat(3, 1)\n",
    "input_1.shape, input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAlCAYAAADfsepVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACiElEQVR4nO3YPWhVdxgG8HPujUVRFKuCBj+IgxVSg6BoBysWCcYsDqVUwam4KDg6d1WcVFREwU1wsJOmERcrCtoqtKAgVfwEBx38IC4mucdZ/m/wg0DA9/cbHx7OPTn33v99SN00TVMBAGm1pvoGAICpZQwAQHLGAAAkZwwAQHLGAAAkZwwAQHLGAAAkZwwAQHLGAAAk1/Wpxf7WL2Hemj49zO8e+77IHg6cCruDKzeG+dDdK3G//9ci69x/FHab0bEwf7VzXZjPPfNPkT3dF3fv7D0W5gM968N8+38Pw/zwoZ+LbMGJv8Nue963YV41nTB+vm1Fkc07HV+7btVhPvzkZpj3HtlTZIsP3Ai7rW+mhXkzHt/3i9/WFNmt34+H3S2Ly+6XvOaFR9eLrF1Pzl7euvyHIuu8Gw277Tmzw3z89ZswHxlaVmTX+v4Iu4O9P4V5Z+RtmNfT4iPi/L2rRTZZz2pg6doia82aGXZHVy0P82cbZoT5koPlZ7/uiv/Gz/lsVlVVzT8Zf6+i9+fVXwvD7kRnymDf5jCvxsrz7fHu3rC67Ojt+BpLFoVx5//yvGrGx8Nu3W6H+csd8bO6sb/8Lm/pXh12Lz77N8wnMripPE+rqqqGLp/75Gts/e7HMK+Xdod59KxaK3ri+7h0Nn7N4Iyoqqr680F5LvUM7wq7E/3GthbeC/MPOh9tAABfNWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJIzBgAgOWMAAJKrm6ZppvomAICp4z8DAJCcMQAAyRkDAJCcMQAAyRkDAJCcMQAAyRkDAJCcMQAAyRkDAJDce8KUekPxlC+cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(input_tensor.numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_1 = model.generate(\n",
    "                input_1.to(model.device), \n",
    "                max_new_tokens=1000, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=40,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 249])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_1.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let'sdenotethenumberofpurpleflowersasX.Accordingtotheproblem,thereare80%morepurpleflowersthanyellowones,sothereare1.8*Ten=<b>18</b>purpleflowers.\\n\\nLet'sdenotethenumberofgreenflowersasG.Theproblemstatesthatthereareonly25%asmanygreenflowersasyellowandpurpleflowerscombined.So,wehave:\\n\\nG=0.25*(10+18)=0.25*28=<b>7</b>\\n\\nTherefore,Markhasatotalof<b>10+18+7=35</b>flowersinhisgarden.</s>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([tokenizer.decode(tok.item()) for tok in outputs_1.sequences[:, input_1.shape[1]:][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAdCAYAAABMxnvIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE30lEQVR4nO3cy6tVdRQH8HXO8RVFYiaU6RU1LDAt6I0VZoh6J0ERFVSDMqigSdC0pkUji4woaBJBRRHSvV4RoiIrK81eJFk+MhzowEc2Se85/QlrQXsfL/L5jL/81r777PP7/fbvLG5nMBgMAgAAAAAAAAAAoCXdc30BAAAAAAAAAADA+U2TEgAAAAAAAAAA0CpNSgAAAAAAAAAAQKs0KQEAAAAAAAAAAK3SpAQAAAAAAAAAALRKkxIAAAAAAAAAANAqTUoAAAAAAAAAAECrNCkBAAAAAAAAAACt0qQEAAAAAAAAAAC0alo1uLZ7X5rpzppVGmvv5mvSzIH1b6aZ0avvKNUb3/t5Ptba+9NM//eDpXqDM2fTzImHbkozc975tlTv8LP5WL88vTnNrF98c6neAz8cSDMvb7o3zcx7/ZtSvd7cS/LQoJ9Gjt69rFRv7lv5dXW6nTQz8ed3pXrLX3kqzSx4cWea6c6YXqo3mMzv1bFHr08zu55/rVRv3YJ8rMq1V647ImLs4NdpptcZbn/mhiW3pJn+v2dKY/VmX5xmJk+eSjOnxxeV6u1Y+WGaGV1+Z5rpn/6nVK8zPV+WPt73RZoZ9mccEbF+5IY0073owjRzZsWSUr0jt12QZha+VJjPptW2Ak3NHZe+UZv7K8/oic8uSzOV9S8iYnTlXXnobL6+H3pyeaneold/zkMLL08j/d/yNTkiYjA5mWY6vV6aOf5g/hlHROx8IV8j1s2/Ls1sO7KnVK9idHW+Nxn/9IPG6m246vY00xmZXxqr8jl3ly1OM+Pb3y3Vq6xbW/fn6+3iiY2leqV9/5A/v6XvPZFmrnymtlfvzZmdZiaPn0wz1bW7MjdW1oduYc8RETH+0ydpZvXGx9PMzIndpXoV00auSDNjX24pjVX5PlTWyMoeNWJq7lNPfZTPVZXnLiJiZNOeNHP04WvTzLxdf5fqTWx5O81U3oM7nfz9L6K5Z2HYz0FlHxtRO3uovLtW9hwREROHavvGYap8ZyJq62TFqh/vKeWa2hdX9mdVJx65Nc1U9oxVU3Ht7q9aWarX1LtdVeW8qjIv9BbU9rKT8/L7Gd//mkaafHdt6uwhYmrO2VPx7OHYYzeW6lXODJo6L4iImHl8kGZ2P5fPVaMr1pTqVfbO6xfl621VZS1tcu6vvMNXzorHDxfn63Nw/pepfEcH/fy5i4jY9teu/3s5EVH/3Wds/1dpZtj33PMSMVbcEzf191XueURtTz/s/XxT51lVTc6fZ9fk933Gzr2lsbbu25Fmhv2beG9pvnZP/nGoVG8qPnsVU3UurlxXk9fU1Lze5D0Y9tzRVF9ARHO/6VTfzSvz3vb++6Wxpt6qDAAAAAAAAAAAnFc0KQEAAAAAAAAAAK3SpAQAAAAAAAAAALRKkxIAAAAAAAAAANAqTUoAAAAAAAAAAECrNCkBAAAAAAAAAACt0qQEAAAAAAAAAAC0SpMSAAAAAAAAAADQqs5gMBic64sAAAAAAAAAAADOX/6TEgAAAAAAAAAA0CpNSgAAAAAAAAAAQKs0KQEAAAAAAAAAAK3SpAQAAAAAAAAAALRKkxIAAAAAAAAAANAqTUoAAAAAAAAAAECrNCkBAAAAAAAAAACt0qQEAAAAAAAAAAC0SpMSAAAAAAAAAADQqv8AF8IjQs1U+PQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs_1.sequences.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "                input_tensor.to(model.device), \n",
    "                max_new_tokens=1000, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=40,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 535])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs.sequences.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as X. Since there are 80% more purple flowers than yellow flowers, and there are 10 yellow flowers, we can write the equation: X = 10 * 1.8 = 18.\\n\\nLet's denote the number of green flowers as Y. Since there are 25% as many green flowers as there are yellow and purple flowers, we can write: Y = 10 + 10 + 18 * 0.25 = 15.\\n\\nTherefore, Mark has a total of 10 (yellow) + 18 (purple) + 15 (green) = <<10+18+15=43>>43 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as x. Since there are 80% more purple flowers than yellow ones, the number of yellow flowers is x/1.25. The number of green flowers is 25% of the number of yellow and purple flowers combined, so it's 0.25 * (x + x/1.25).\\n\\nWe have three equations based on the given information:\\n\\n1. x + (x/1.25) + 0.25 * (x + x/1.25) = total number of flowers\\n2. x + x/1.25 = 10 (yellow flowers) * 1.25\\n3. 0.25 * (x + x/1.25) = 25% of (x + x/1.25)\\n\\nLet's solve equation 2 for x:\\n\\nx + x/1.25 = 12.5\\nx(1 + 1/1.25) = 12.5\\nx = 12.5 * 1.25 / 2\\nx = 15\\n\\nNow let's calculate the number of yellow and purple flowers:\\n\\nYellow flowers = x/1.25 = 15/1.25 ≈ 12\\nPurple flowers = 1.25 * Yellow flowers = 1.25 * 12 ≈ 15\\n\\nLet's check if the total number of flowers is correct:\\n\\nTotal number of flowers = 12 (yellow) + 15 (purple) + 0.25 * (12 + 15) ≈ 12 + 15 + 6.25 = 23.25\\n\\nHowever, the total number of flowers should be a whole number. It seems like there is a mistake in the problem statement or our calculations. We cannot determine the exact number of flowers in the garden with the given information.</s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as X. According to the problem, there are 80% more purple flowers than yellow ones, so there are 1.8 * Ten = 18 yellow flowers.\\n\\nSince there are 25% as many green flowers as there are yellow and purple flowers, the number of green flowers is 0.25 * (10 + 18) = 6.5 * 18 = 117 (we round up because we can't have a fraction of a flower).\\n\\nSo, in total, Mark has 10 (yellow) + 18 (purple) + 117 (green) = <<10+18+117=135>>135 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "                input_tensor.to(model.device), \n",
    "                max_new_tokens=500, \n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_k=40,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 583])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(outputs.sequences.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs.sequences.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let x be the number of yellow flowers, y be the number of purple flowers, and z be the number of green flowers. We have three equations based on the given information:\\n\\n1. x + y + z = total number of flowers\\n2. x = 10 (the number of yellow flowers)\\n3. y = 1.8x (80% more purple flowers than yellow)\\n4. z = 0.25x (25% as many green flowers as yellow and purple)\\n\\nFirst, let's find the number of purple flowers (y). According to equation 3:\\n\\ny = 1.8 * 10\\ny = 18\\n\\nNow, let's find the number of green flowers (z). According to equation 4:\\n\\nz = 0.25 * (x + y)\\nz = 0.25 * (10 + 18)\\nz = 0.25 * 28\\nz = 7\\n\\nNow we have all the flower counts:\\n- Yellows: x = 10\\n- Purples: y = 18\\n- Greens: z = 7\\n\\nSo, the total number of flowers in Mark's garden is:\\n\\nx + y + z = 10 + 18 + 7 = 35\\n\\nTherefore, Mark has 35 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as x, and the number of green flowers as y. We know that:\\n\\n1. The number of yellow flowers is 10.\\n2. The number of purple flowers is 10 * 1.8 = 18 (since there are 80% more purple flowers than yellow).\\n3. The number of green flowers is 10 * 0.25 = 2.5, but since we cannot have a fraction of a flower, we need to round up or down to the nearest whole number. Let's assume there are 2 green flowers.\\n\\nTherefore, the total number of flowers in Mark's garden is:\\n\\nTotal Flowers = Yellow Flowers + Purple Flowers + Green Flowers\\nTotal Flowers = 10 + 18 + 2 = <box>30</box>\\n\\nSo, Mark has 30 flowers in his garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"<s> [INST] Q: Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\\nA: Let's think step by step. [/INST] Let's denote the number of purple flowers as X. We know that there are 80% more purple flowers than yellow ones, so the number of yellow flowers is X/1.8 (since 80% is equivalent to 0.8 or 1/1.8). We also know that there are 25% as many green flowers as there are yellow and purple flowers combined. Let's denote the number of green flowers as G.\\n\\nFirst, let's find the total number of yellow and purple flowers:\\nTotal yellow and purple flowers = X + X/1.8\\n\\nNow, let's find the number of green flowers:\\nGreen flowers = 0.25 * (X + X/1.8)\\n\\nSince we know that Mark has ten yellow flowers, we can set up an equation to find the value of X:\\nX + X/1.8 = 10 + X/1.8 + 0.25 * (10 + X/1.8)\\n\\nLet's simplify the equation:\\nX + X/1.8 = 10 + X/1.8 + 2.5 * (10 + X/1.8)\\nX + X/1.8 = 10 + X/1.8 + 25 + 2.5 * X\\nX = 25\\n\\nNow, we can find the number of yellow and purple flowers:\\nTotal yellow and purple flowers = 25 + 25/1.8 = 38.885… ≈ 39\\n\\nSince we can't have a fraction of a flower, we'll round up to 40.\\n\\nThe number of yellow flowers is 40/2 = 20.\\n\\nNow, we can find the number of purple flowers:\\nPurple flowers = 40 * 1.8 = 72\\n\\nFinally, we can find the number of green flowers:\\nGreen flowers = 0.25 * 40 * 1.8 = 18\\n\\nSo, Mark has approximately 40 yellow flowers, 72 purple flowers, and 18 green flowers in his garden. The total number of flowers is around 130.</s>\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 842], [1, 842, 28705], [1, 28705, 842, 28705], [1, 842, 415])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('.'), tokenizer.encode(\". \"), tokenizer.encode(\" . \"), tokenizer.encode(\". The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(842) == tokenizer.decode(28723)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28705, 13]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <s>\n",
      "28705 \n",
      "318 S\n",
      "308 ent\n",
      "636 ence\n",
      "28705 \n",
      "28740 1\n",
      "28723 .\n",
      "318 S\n",
      "308 ent\n",
      "636 ence\n",
      "28705 \n",
      "28750 2\n",
      "28723 .\n",
      "2909 Some\n",
      "1474 number\n",
      "28705 \n",
      "28770 3\n",
      "28723 .\n",
      "28782 5\n",
      "28725 ,\n",
      "318 S\n",
      "308 ent\n",
      "636 ence\n",
      "28705 \n",
      "28770 3\n",
      "28723 .\n"
     ]
    }
   ],
   "source": [
    "for tok in tokenizer.encode(\" Sentence 1. Sentence 2. Some number 3.5, Sentence 3.\"):\n",
    "    print(tok, tokenizer.decode(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 28705, 28734]\n",
      "1 [1, 28705, 28740]\n",
      "2 [1, 28705, 28750]\n",
      "3 [1, 28705, 28770]\n",
      "4 [1, 28705, 28781]\n",
      "5 [1, 28705, 28782]\n",
      "6 [1, 28705, 28784]\n",
      "7 [1, 28705, 28787]\n",
      "8 [1, 28705, 28783]\n",
      "9 [1, 28705, 28774]\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    print(n, tokenizer.encode(str(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28705, 28770, 28723, 28782]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"3.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   733, 16289, 28793,  1186, 28747,  3655,   659,   264,  8759,\n",
       "          395, 11888, 28723,   650, 24571,  9923,   302,  1712,  1581,  9304,\n",
       "          297,   378, 28723, 11819,   302,   706,   460,  9684, 28725,   304,\n",
       "          736,   460, 28705, 28783, 28734, 28823,   680,   302,  1395,   297,\n",
       "        19435, 28723,  1387,   460,   865, 28705, 28750, 28782, 28823,   390,\n",
       "         1287,  5344, 11888,   390,   736,   460,  9684,   304, 19435, 11888,\n",
       "        28723,  1602,  1287, 11888,  1235,  3655,   506,   297,   516,  8759,\n",
       "        28804,    13, 28741, 28747,  3169, 28742, 28713,  1073,  3707,   486,\n",
       "         3707, 28723,   733, 28748, 16289, 28793,  3169, 28742, 28713, 14543,\n",
       "          272,  1474,   302, 19435, 11888,   390,  1500, 28723,  6586,   298,\n",
       "          272,  2700, 28725,   736,   460, 28705, 28783, 28734, 28823,   680,\n",
       "        19435, 11888,   821,  9684,  4413, 28725,   579,   736,   460, 28705,\n",
       "        28740, 28723, 28783,   398, 11819,   327,   523, 28726, 28767, 28740,\n",
       "        28783,   700, 28726, 28767, 19435, 11888, 28723,    13,    13,  8779,\n",
       "        28742, 28713, 14543,   272,  1474,   302,  5344, 11888,   390,   420,\n",
       "        28723,   415,  2700,  4605,   369,   736,   460,   865, 28705, 28750,\n",
       "        28782, 28823,   390,  1287,  5344, 11888,   390,  9684,   304, 19435,\n",
       "        11888,  9837, 28723,  1537, 28725,   478,   506, 28747,    13,    13,\n",
       "        28777,   327, 28705, 28734, 28723, 28750, 28782,   398,   325, 28740,\n",
       "        28734,   648, 28705, 28740, 28783, 28731,   327, 28705, 28734, 28723,\n",
       "        28750, 28782,   398, 28705, 28750, 28783,   327,   523, 28726, 28767,\n",
       "        28787,   700, 28726, 28767,    13,    13,  5816,   994, 28725,  3655,\n",
       "          659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
       "        28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
       "          700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_1.sequences.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28723, Decoded: .\n",
      "Token: 6586, Decoded: According\n",
      "Token: 298, Decoded: to\n",
      "Token: 272, Decoded: the\n",
      "Token: 2700, Decoded: problem\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 4413, Decoded: ones\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 579, Decoded: so\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 398, Decoded: *\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8779, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 420, Decoded: G\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 2700, Decoded: problem\n",
      "Token: 4605, Decoded: states\n",
      "Token: 369, Decoded: that\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 9837, Decoded: combined\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1537, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 506, Decoded: have\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28777, Decoded: G\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28734, Decoded: 0\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28787, Decoded: 7\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 3102, Decoded: total\n",
      "Token: 302, Decoded: of\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "Token: 2, Decoded: </s>\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.decode(outputs_1.sequences)\n",
    "# Fix the TypeError by ensuring the input to tokenizer.decode is a list of integers\n",
    "# decoded_output = tokenizer.decode(outputs_1.sequences.flatten().tolist())\n",
    "# decoded_output\n",
    "for token in outputs_1.sequences.flatten():\n",
    "    decoded_token = tokenizer.decode([token.item()])\n",
    "    print(f\"Token: {token.item()}, Decoded: {decoded_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of period token: tensor([ 12,  22,  41,  60,  81,  97, 121, 136, 150, 172, 184, 199, 247],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "outputs_1.sequences.flatten()\n",
    "# Find the instances of the element 842 in the flattened sequences\n",
    "indices_of_period = (outputs_1.sequences.flatten() == 28723).nonzero(as_tuple=True)[0]\n",
    "print(\"Indices of period token:\", indices_of_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = outputs_1.sequences.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 249])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  12],\n",
       "        [  0,  22],\n",
       "        [  0,  41],\n",
       "        [  0,  60],\n",
       "        [  0,  81],\n",
       "        [  0,  97],\n",
       "        [  0, 121],\n",
       "        [  0, 136],\n",
       "        [  0, 150],\n",
       "        [  0, 172],\n",
       "        [  0, 184],\n",
       "        [  0, 199],\n",
       "        [  0, 247]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros_like(tensor_1)\n",
    "period_indices = (tensor_1 == 28723).nonzero()\n",
    "period_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  71],\n",
       "        [  0, 137],\n",
       "        [  0, 138],\n",
       "        [  0, 178],\n",
       "        [  0, 179],\n",
       "        [  0, 214],\n",
       "        [  0, 215]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tok2delim = {28723 : \".\", 13 : \"\\n\"}\n",
    "newline_indices = (tensor_1 == 13).nonzero()\n",
    "newline_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = tensor_1.size(1)\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 248]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_end = (tensor_1 == 2).nonzero()\n",
    "seq_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 199])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_start_idx = period_indices[-2] if len(period_indices) > 1 and  else None\n",
    "sentence_start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sentence_start_idx is not None:\n",
    "    mask[0, sentence_start_idx[1]+1:] = 1\n",
    "else:\n",
    "    mask[0, :] = 1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793,  1186, 28747,  3655,   659,   264,  8759,\n",
       "           395, 11888, 28723,   650, 24571,  9923,   302,  1712,  1581,  9304,\n",
       "           297,   378, 28723, 11819,   302,   706,   460,  9684, 28725,   304,\n",
       "           736,   460, 28705, 28783, 28734, 28823,   680,   302,  1395,   297,\n",
       "         19435, 28723,  1387,   460,   865, 28705, 28750, 28782, 28823,   390,\n",
       "          1287,  5344, 11888,   390,   736,   460,  9684,   304, 19435, 11888,\n",
       "         28723,  1602,  1287, 11888,  1235,  3655,   506,   297,   516,  8759,\n",
       "         28804,    13, 28741, 28747,  3169, 28742, 28713,  1073,  3707,   486,\n",
       "          3707, 28723,   733, 28748, 16289, 28793,  3169, 28742, 28713, 14543,\n",
       "           272,  1474,   302, 19435, 11888,   390,  1500, 28723,  6586,   298,\n",
       "           272,  2700, 28725,   736,   460, 28705, 28783, 28734, 28823,   680,\n",
       "         19435, 11888,   821,  9684,  4413, 28725,   579,   736,   460, 28705,\n",
       "         28740, 28723, 28783,   398, 11819,   327,   523, 28726, 28767, 28740,\n",
       "         28783,   700, 28726, 28767, 19435, 11888, 28723,    13,    13,  8779,\n",
       "         28742, 28713, 14543,   272,  1474,   302,  5344, 11888,   390,   420,\n",
       "         28723,   415,  2700,  4605,   369,   736,   460,   865, 28705, 28750,\n",
       "         28782, 28823,   390,  1287,  5344, 11888,   390,  9684,   304, 19435,\n",
       "         11888,  9837, 28723,  1537, 28725,   478,   506, 28747,    13,    13,\n",
       "         28777,   327, 28705, 28734, 28723, 28750, 28782,   398,   325, 28740,\n",
       "         28734,   648, 28705, 28740, 28783, 28731,   327, 28705, 28734, 28723,\n",
       "         28750, 28782,   398, 28705, 28750, 28783,   327,   523, 28726, 28767,\n",
       "         28787,   700, 28726, 28767,    13,    13,  5816,   994, 28725,  3655,\n",
       "           659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
       "         28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
       "           700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         28750, 28782,   398, 28705, 28750, 28783,   327,   523, 28726, 28767,\n",
       "         28787,   700, 28726, 28767,    13,    13,  5816,   994, 28725,  3655,\n",
       "           659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
       "         28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
       "           700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_tensor = tensor_1 * mask\n",
    "ans_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28787, Decoded: 7\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 3102, Decoded: total\n",
      "Token: 302, Decoded: of\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "Token: 2, Decoded: </s>\n"
     ]
    }
   ],
   "source": [
    "for token in ans_tensor.flatten():\n",
    "    decoded_token = tokenizer.decode([token.item()])\n",
    "    print(f\"Token: {token.item()}, Decoded: {decoded_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_end[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 247 too close to </s> at 248\n",
      "Filtered Period Indices: tensor([[  0,  12],\n",
      "        [  0,  22],\n",
      "        [  0,  41],\n",
      "        [  0,  60],\n",
      "        [  0,  81],\n",
      "        [  0,  97],\n",
      "        [  0, 136],\n",
      "        [  0, 150],\n",
      "        [  0, 172]])\n"
     ]
    }
   ],
   "source": [
    "period_indices = (tensor_1 == 28723).nonzero()\n",
    "\n",
    "digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "\n",
    "# Filter out periods that are part of decimal numbers\n",
    "filtered_indices = []\n",
    "for idx in period_indices:\n",
    "    batch_index, token_index = idx[0], idx[1]\n",
    "    seq_end = (tensor_1 == 2).nonzero()\n",
    "    seq_end_index = seq_end[0][1].item()\n",
    "    # Remove if period is near end of sequence (it is part of last sentence)\n",
    "    if seq_end_index - token_index < 5:\n",
    "        print(f\"index {token_index} too close to </s> at {seq_end_index}\")\n",
    "    # Check if the period is not at the start or end of the tensor\n",
    "    elif token_index > 0 and token_index < tensor_1.shape[1] - 1:\n",
    "        # Check the tokens before and after the period\n",
    "        token_before = tensor_1[batch_index, token_index - 1]\n",
    "        token_after = tensor_1[batch_index, token_index + 1]\n",
    "        # Check if both tokens are numeric\n",
    "        if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "            filtered_indices.append(idx)\n",
    "\n",
    "# Convert list to tensor\n",
    "filtered_indices = torch.stack(filtered_indices)\n",
    "\n",
    "print(\"Filtered Period Indices:\", filtered_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  71],\n",
       "        [  0, 137],\n",
       "        [  0, 138],\n",
       "        [  0, 178],\n",
       "        [  0, 179],\n",
       "        [  0, 214],\n",
       "        [  0, 215]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline_indices = (tensor_1 == 13).nonzero()\n",
    "newline_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last sentence start index: tensor(215)\n"
     ]
    }
   ],
   "source": [
    "last_period_index = filtered_indices[-1, 1] if filtered_indices.size(0) > 0 else -1\n",
    "last_newline_index = newline_indices[-1, 1] if newline_indices.size(0) > 0 else -1\n",
    "\n",
    "# Determine the maximum index\n",
    "last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "print(\"Last sentence start index:\", last_sentence_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros_like(tensor_1)\n",
    "if sentence_start_idx is not None:\n",
    "    mask[0, last_sentence_start_index+1:] = 1\n",
    "else:\n",
    "    mask[0, :] = 1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 0, Decoded: <unk>\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 3102, Decoded: total\n",
      "Token: 302, Decoded: of\n",
      "Token: 523, Decoded: <\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28734, Decoded: 0\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 700, Decoded: </\n",
      "Token: 28726, Decoded: b\n",
      "Token: 28767, Decoded: >\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "Token: 2, Decoded: </s>\n"
     ]
    }
   ],
   "source": [
    "ans_tensor = tensor_1 * mask\n",
    "for token in ans_tensor.flatten():\n",
    "    decoded_token = tokenizer.decode([token.item()])\n",
    "    print(f\"Token: {token.item()}, Decoded: {decoded_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,  5816,   994, 28725,  3655,\n",
      "           659,   264,  3102,   302,   523, 28726, 28767, 28740, 28734,   648,\n",
      "         28705, 28740, 28783,   648, 28705, 28787,   327, 28705, 28770, 28782,\n",
      "           700, 28726, 28767, 11888,   297,   516,  8759, 28723,     2]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_length = tensor_1.shape\n",
    "digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "mask = torch.zeros_like(tensor_1)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    row_tensor = tensor_1[i]\n",
    "\n",
    "    # Find period indices\n",
    "    period_indices = (row_tensor == 28723).nonzero()\n",
    "\n",
    "    # Filter out periods that are part of decimal numbers\n",
    "    filtered_indices = []\n",
    "    for idx in period_indices:\n",
    "        token_index = idx[0]\n",
    "        seq_end = (row_tensor == 2).nonzero()\n",
    "        seq_end_index = seq_end[0].item()\n",
    "        # Remove if period is near end of sequence (it is part of last sentence)\n",
    "        if seq_end_index - token_index < 5:\n",
    "            continue\n",
    "        # Check if the period is not at the start or end of the tensor\n",
    "        elif token_index > 0 and token_index < seq_length - 1:\n",
    "            # Check the tokens before and after the period\n",
    "            token_before = row_tensor[token_index - 1]\n",
    "            token_after = row_tensor[token_index + 1]\n",
    "            # Check if both tokens are numeric\n",
    "            if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "                filtered_indices.append(token_index)\n",
    "\n",
    "    # Find newline indices\n",
    "    newline_indices = (row_tensor == 13).nonzero()\n",
    "\n",
    "    # Determine the last sentence start index\n",
    "    last_period_index = max(filtered_indices) if filtered_indices else -1\n",
    "    last_newline_index = newline_indices[-1] if newline_indices.size(0) > 0 else -1\n",
    "    last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "    # Set mask for the last sentence\n",
    "    if last_sentence_start_index != -1:\n",
    "        mask[i, last_sentence_start_index+1:] = 1\n",
    "\n",
    "# Apply mask to get only the last sentences\n",
    "last_sentence_only_tensor = tensor_1 * mask\n",
    "\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"Last Sentence Only Tensor:\\n\", last_sentence_only_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([249])\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "    row_tensor = last_sentence_only_tensor[i]\n",
    "    print(row_tensor.shape)\n",
    "    # decoded_token = tokenizer.decode([token.item()])\n",
    "    # print(f\"Token: {token.item()}, Decoded: {decoded_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_sentence(tensor):\n",
    "    batch_size, seq_length = tensor.shape\n",
    "    digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "    mask = torch.zeros_like(tensor)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        row_tensor = tensor[i]\n",
    "\n",
    "        period_indices = (row_tensor == 28723).nonzero()\n",
    "\n",
    "        filtered_indices = []\n",
    "        for idx in period_indices:\n",
    "            token_index = idx[0]\n",
    "            seq_end = (row_tensor == 2).nonzero()\n",
    "            seq_end_index = seq_end[0].item()\n",
    "            # Remove if period is near end of sequence (it is part of last sentence)\n",
    "            if seq_end_index - token_index < 5:\n",
    "                continue\n",
    "            # Check if the period is not at the start or end of the tensor\n",
    "            elif token_index > 0 and token_index < seq_length - 1:\n",
    "                # Check the tokens before and after the period\n",
    "                token_before = row_tensor[token_index - 1]\n",
    "                token_after = row_tensor[token_index + 1]\n",
    "                # Check if both tokens are numeric\n",
    "                if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "                    filtered_indices.append(token_index)\n",
    "\n",
    "        newline_indices = (row_tensor == 13).nonzero()\n",
    "\n",
    "        # Determine the last sentence start index\n",
    "        last_period_index = max(filtered_indices) if filtered_indices else -1\n",
    "        last_newline_index = newline_indices[-1] if newline_indices.size(0) > 0 else -1\n",
    "        last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "        # Set mask for the last sentence\n",
    "        if last_sentence_start_index != -1:\n",
    "            mask[i, last_sentence_start_index+1:] = 1\n",
    "\n",
    "    # Apply mask to get only the last sentences\n",
    "    last_sentence_only_tensor = tensor * mask\n",
    "\n",
    "    print(\"Mask:\\n\", mask)\n",
    "    print(\"Last Sentence Only Tensor:\\n\", last_sentence_only_tensor)\n",
    "    \n",
    "    return last_sentence_only_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0')\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ..., 28734, 28723,     2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "lsot = find_last_sentence(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(outputs.sequences.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAfCAYAAAABDtrDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACfElEQVR4nO3csUocURQGYGd2SecLLCqktlmwsEgXGBbsF/EVfIG8gpLSwtZ+eyPZIlWKSR8QFLSwdpM+mWsnnEKYCRwWNt9XzQ/nzj0P8HOrUkrZAgAAAAAAAAAASFKvewEAAAAAAAAAAGCzKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKQa9x1s6nnmHgAAAAAAwJr9+XgQ8rv2NuQvd99DPmqOQ+7uH0O+eWgH3b9/cRryznk8X41GIa9O4r7t2WXIs8l00P0AAMBwy27Ra85LSgAAAAAAAAAAQColJQAAAAAAAAAAIJWSEgAAAAAAAAAAkKoqpZQ+g009z94FAAAA4L9TPkxD/rq4CvmoOQ65u38M+eahHXTf/sVpyDvn8Xw1GoW8OjkIuT27fP2eTaaD7gYAAABg8yy7Ra85LykBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAECqqpRS+gw29Tx7FwAAAAAAAAAA2GjVeBxy/X4vDjz/CvHv6nfIT58OQ979/CP+b3s75Ouf3/5hy7fNJtOQl92i1zkvKQEAAAAAAAAAAKmUlAAAAAAAAAAAgFRKSgAAAAAAAAAAQKqqlFLWvQQAAAAAAAAAALC5vKQEAAAAAAAAAACkUlICAAAAAAAAAABSKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKR6AdGuRi1/V93/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(lsot.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0')\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ...,     2,     2,     2],\n",
      "        [    0,     0,     0,  ..., 28734, 28723,     2]], device='mps:0')\n",
      "ROW  0\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  1\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  2\n",
      "Token: 415, Decoded: The\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 1401, Decoded: around\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n"
     ]
    }
   ],
   "source": [
    "def print_decoded_tokens(lsot, tokenizer):\n",
    "    # Iterate over each row in the lsot tensor\n",
    "    for idx, row_tensor in enumerate(lsot):\n",
    "        print(\"ROW \", idx)\n",
    "        # Flatten the row tensor to iterate over each token\n",
    "        for token in row_tensor.flatten():\n",
    "            # Check if the token is not one of the excluded tokens\n",
    "            if token.item() not in [0, 2, 28734]:\n",
    "                # Decode the token using the tokenizer\n",
    "                decoded_token = tokenizer.decode([token.item()])\n",
    "                # Print the token and its decoded value\n",
    "                print(f\"Token: {token.item()}, Decoded: {decoded_token}\")\n",
    "\n",
    "# Assuming 'outputs.sequences' is your input tensor and 'tokenizer' is your tokenizer instance\n",
    "lsot = find_last_sentence(outputs.sequences)\n",
    "print_decoded_tokens(lsot, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_sentence(tensor):\n",
    "    batch_size, seq_length = tensor.shape\n",
    "    digit_tokens = [28734, 28740, 28750, 28770, 28781, 28782, 28784, 28787, 28783, 28774]\n",
    "    mask = torch.zeros_like(tensor)\n",
    "    inverse_mask = torch.zeros_like(tensor)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        row_tensor = tensor[i]\n",
    "\n",
    "        # Find period indices\n",
    "        period_indices = (row_tensor == 28723).nonzero()\n",
    "\n",
    "        seq_end = (row_tensor == 2).nonzero()\n",
    "        seq_end_index = seq_end[0].item()\n",
    "\n",
    "        # Filter out periods that are part of decimal numbers\n",
    "        filtered_indices = []\n",
    "        for idx in period_indices:\n",
    "            token_index = idx[0]\n",
    "            # Remove if period is near end of sequence (it is part of last sentence)\n",
    "            if seq_end_index - token_index < 5:\n",
    "                continue\n",
    "            # Check if the period is not at the start or end of the tensor\n",
    "            elif token_index > 0 and token_index < seq_length - 1:\n",
    "                # Check the tokens before and after the period\n",
    "                token_before = row_tensor[token_index - 1]\n",
    "                token_after = row_tensor[token_index + 1]\n",
    "                # Check if both tokens are numeric\n",
    "                if not (token_before in digit_tokens and token_after in digit_tokens):\n",
    "                    filtered_indices.append(token_index)\n",
    "\n",
    "        newline_indices = (row_tensor == 13).nonzero()\n",
    "\n",
    "        # Determine the last sentence start index\n",
    "        last_period_index = max(filtered_indices) if filtered_indices else -1\n",
    "        last_newline_index = newline_indices[-1] if newline_indices.size(0) > 0 else -1\n",
    "        last_sentence_start_index = max(last_period_index, last_newline_index)\n",
    "\n",
    "        # Set mask for the last sentence\n",
    "        if last_sentence_start_index != -1:\n",
    "            mask[i, last_sentence_start_index+1:seq_end_index+1] = 1\n",
    "            inverse_mask[i, :last_sentence_start_index+1] = 1\n",
    "\n",
    "    # Apply mask to get only the last sentences\n",
    "    last_sentence_only_tensor = tensor * mask\n",
    "    initial_sentences_tensor = tensor * inverse_mask\n",
    "\n",
    "    print(\"Mask:\\n\", mask)\n",
    "    print(\"Inverse Mask:\\n\", inverse_mask)\n",
    "    print(\"Last Sentence Only Tensor:\\n\", last_sentence_only_tensor)\n",
    "    print(\"Initial Sentences Tensor:\\n\", initial_sentences_tensor)\n",
    "    \n",
    "    return initial_sentences_tensor, last_sentence_only_tensor \n",
    "\n",
    "# ist, lsot = find_last_sentence(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW  0\n",
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 1318, Decoded: x\n",
      "Token: 347, Decoded: be\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 337, Decoded: y\n",
      "Token: 347, Decoded: be\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 686, Decoded: z\n",
      "Token: 347, Decoded: be\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 816, Decoded: We\n",
      "Token: 506, Decoded: have\n",
      "Token: 1712, Decoded: three\n",
      "Token: 12501, Decoded: equations\n",
      "Token: 2818, Decoded: based\n",
      "Token: 356, Decoded: on\n",
      "Token: 272, Decoded: the\n",
      "Token: 2078, Decoded: given\n",
      "Token: 1871, Decoded: information\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1318, Decoded: x\n",
      "Token: 648, Decoded: +\n",
      "Token: 337, Decoded: y\n",
      "Token: 648, Decoded: +\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1318, Decoded: x\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 325, Decoded: (\n",
      "Token: 1237, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n",
      "Token: 337, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28744, Decoded: x\n",
      "Token: 325, Decoded: (\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28723, Decoded: .\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28744, Decoded: x\n",
      "Token: 325, Decoded: (\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 7489, Decoded: First\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 325, Decoded: (\n",
      "Token: 28724, Decoded: y\n",
      "Token: 609, Decoded: ).\n",
      "Token: 6586, Decoded: According\n",
      "Token: 298, Decoded: to\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28724, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28724, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 325, Decoded: (\n",
      "Token: 28764, Decoded: z\n",
      "Token: 609, Decoded: ).\n",
      "Token: 6586, Decoded: According\n",
      "Token: 298, Decoded: to\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28744, Decoded: x\n",
      "Token: 648, Decoded: +\n",
      "Token: 337, Decoded: y\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28764, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 478, Decoded: we\n",
      "Token: 506, Decoded: have\n",
      "Token: 544, Decoded: all\n",
      "Token: 272, Decoded: the\n",
      "Token: 14994, Decoded: flower\n",
      "Token: 17938, Decoded: counts\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28733, Decoded: -\n",
      "Token: 627, Decoded: Y\n",
      "Token: 479, Decoded: ell\n",
      "Token: 3611, Decoded: ows\n",
      "Token: 28747, Decoded: :\n",
      "Token: 1318, Decoded: x\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28733, Decoded: -\n",
      "Token: 13673, Decoded: Pur\n",
      "Token: 2815, Decoded: ples\n",
      "Token: 28747, Decoded: :\n",
      "Token: 337, Decoded: y\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28733, Decoded: -\n",
      "Token: 9261, Decoded: Gre\n",
      "Token: 596, Decoded: ens\n",
      "Token: 28747, Decoded: :\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 272, Decoded: the\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 349, Decoded: is\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28744, Decoded: x\n",
      "Token: 648, Decoded: +\n",
      "Token: 337, Decoded: y\n",
      "Token: 648, Decoded: +\n",
      "Token: 686, Decoded: z\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "ROW  1\n",
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 1318, Decoded: x\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 337, Decoded: y\n",
      "Token: 28723, Decoded: .\n",
      "Token: 816, Decoded: We\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 325, Decoded: (\n",
      "Token: 10793, Decoded: since\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 609, Decoded: ).\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n",
      "Token: 415, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 562, Decoded: but\n",
      "Token: 1854, Decoded: since\n",
      "Token: 478, Decoded: we\n",
      "Token: 3573, Decoded: cannot\n",
      "Token: 506, Decoded: have\n",
      "Token: 264, Decoded: a\n",
      "Token: 14005, Decoded: fraction\n",
      "Token: 302, Decoded: of\n",
      "Token: 264, Decoded: a\n",
      "Token: 14994, Decoded: flower\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 927, Decoded: need\n",
      "Token: 298, Decoded: to\n",
      "Token: 3713, Decoded: round\n",
      "Token: 582, Decoded: up\n",
      "Token: 442, Decoded: or\n",
      "Token: 1060, Decoded: down\n",
      "Token: 298, Decoded: to\n",
      "Token: 272, Decoded: the\n",
      "Token: 17403, Decoded: nearest\n",
      "Token: 2894, Decoded: whole\n",
      "Token: 1474, Decoded: number\n",
      "Token: 28723, Decoded: .\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 7146, Decoded: assume\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 272, Decoded: the\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 349, Decoded: is\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 327, Decoded: =\n",
      "Token: 24275, Decoded: Yellow\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 648, Decoded: +\n",
      "Token: 13673, Decoded: Pur\n",
      "Token: 792, Decoded: ple\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 648, Decoded: +\n",
      "Token: 6248, Decoded: Green\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 21127, Decoded: Flow\n",
      "Token: 404, Decoded: ers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 327, Decoded: =\n",
      "Token: 523, Decoded: <\n",
      "Token: 2858, Decoded: box\n",
      "Token: 28767, Decoded: >\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 700, Decoded: </\n",
      "Token: 2858, Decoded: box\n",
      "Token: 28767, Decoded: >\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "ROW  2\n",
      "Token: 1, Decoded: <s>\n",
      "Token: 733, Decoded: [\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 1186, Decoded: Q\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 264, Decoded: a\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 395, Decoded: with\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 650, Decoded: He\n",
      "Token: 24571, Decoded: planted\n",
      "Token: 9923, Decoded: plants\n",
      "Token: 302, Decoded: of\n",
      "Token: 1712, Decoded: three\n",
      "Token: 1581, Decoded: different\n",
      "Token: 9304, Decoded: colors\n",
      "Token: 297, Decoded: in\n",
      "Token: 378, Decoded: it\n",
      "Token: 28723, Decoded: .\n",
      "Token: 11819, Decoded: Ten\n",
      "Token: 302, Decoded: of\n",
      "Token: 706, Decoded: them\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 302, Decoded: of\n",
      "Token: 1395, Decoded: those\n",
      "Token: 297, Decoded: in\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1387, Decoded: There\n",
      "Token: 460, Decoded: are\n",
      "Token: 865, Decoded: only\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28723, Decoded: .\n",
      "Token: 1602, Decoded: How\n",
      "Token: 1287, Decoded: many\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 1235, Decoded: does\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 506, Decoded: have\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28804, Decoded: ?\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28741, Decoded: A\n",
      "Token: 28747, Decoded: :\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1073, Decoded: think\n",
      "Token: 3707, Decoded: step\n",
      "Token: 486, Decoded: by\n",
      "Token: 3707, Decoded: step\n",
      "Token: 28723, Decoded: .\n",
      "Token: 733, Decoded: [\n",
      "Token: 28748, Decoded: /\n",
      "Token: 16289, Decoded: INST\n",
      "Token: 28793, Decoded: ]\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28723, Decoded: .\n",
      "Token: 816, Decoded: We\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 680, Decoded: more\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 821, Decoded: than\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 4413, Decoded: ones\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 579, Decoded: so\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 325, Decoded: (\n",
      "Token: 10793, Decoded: since\n",
      "Token: 28705, Decoded: \n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28823, Decoded: %\n",
      "Token: 349, Decoded: is\n",
      "Token: 9844, Decoded: equivalent\n",
      "Token: 298, Decoded: to\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 442, Decoded: or\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 609, Decoded: ).\n",
      "Token: 816, Decoded: We\n",
      "Token: 835, Decoded: also\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28823, Decoded: %\n",
      "Token: 390, Decoded: as\n",
      "Token: 1287, Decoded: many\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 736, Decoded: there\n",
      "Token: 460, Decoded: are\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 9837, Decoded: combined\n",
      "Token: 28723, Decoded: .\n",
      "Token: 3169, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 14543, Decoded: denote\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 390, Decoded: as\n",
      "Token: 420, Decoded: G\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 7489, Decoded: First\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 1500, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 1346, Decoded: let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 22991, Decoded: Green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 24091, Decoded: Since\n",
      "Token: 478, Decoded: we\n",
      "Token: 873, Decoded: know\n",
      "Token: 369, Decoded: that\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 3095, Decoded: ten\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 808, Decoded: set\n",
      "Token: 582, Decoded: up\n",
      "Token: 396, Decoded: an\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 298, Decoded: to\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1192, Decoded: value\n",
      "Token: 302, Decoded: of\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8779, Decoded: Let\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28713, Decoded: s\n",
      "Token: 9878, Decoded: simpl\n",
      "Token: 1575, Decoded: ify\n",
      "Token: 272, Decoded: the\n",
      "Token: 8777, Decoded: equation\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 325, Decoded: (\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28731, Decoded: )\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 648, Decoded: +\n",
      "Token: 1500, Decoded: X\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 1500, Decoded: X\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28814, Decoded: X\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 10240, Decoded: Total\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 304, Decoded: and\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 648, Decoded: +\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 28878, Decoded: …\n",
      "Token: 28705, Decoded: \n",
      "Token: 29988, Decoded: ≈\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28774, Decoded: 9\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 24091, Decoded: Since\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 28742, Decoded: '\n",
      "Token: 28707, Decoded: t\n",
      "Token: 506, Decoded: have\n",
      "Token: 264, Decoded: a\n",
      "Token: 14005, Decoded: fraction\n",
      "Token: 302, Decoded: of\n",
      "Token: 264, Decoded: a\n",
      "Token: 14994, Decoded: flower\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 28742, Decoded: '\n",
      "Token: 584, Decoded: ll\n",
      "Token: 3713, Decoded: round\n",
      "Token: 582, Decoded: up\n",
      "Token: 298, Decoded: to\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 1014, Decoded: The\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 28748, Decoded: /\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28723, Decoded: .\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 8479, Decoded: Now\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 28753, Decoded: P\n",
      "Token: 324, Decoded: ur\n",
      "Token: 792, Decoded: ple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 11491, Decoded: Fin\n",
      "Token: 578, Decoded: ally\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 478, Decoded: we\n",
      "Token: 541, Decoded: can\n",
      "Token: 1300, Decoded: find\n",
      "Token: 272, Decoded: the\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28747, Decoded: :\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 22991, Decoded: Green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28723, Decoded: .\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 398, Decoded: *\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28723, Decoded: .\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 327, Decoded: =\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 13, Decoded: \n",
      "\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 10870, Decoded: approximately\n",
      "Token: 28705, Decoded: \n",
      "Token: 28781, Decoded: 4\n",
      "Token: 9684, Decoded: yellow\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 28705, Decoded: \n",
      "Token: 28787, Decoded: 7\n",
      "Token: 28750, Decoded: 2\n",
      "Token: 19435, Decoded: purple\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 304, Decoded: and\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28783, Decoded: 8\n",
      "Token: 5344, Decoded: green\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n"
     ]
    }
   ],
   "source": [
    "print_decoded_tokens(ist, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW  0\n",
      "Token: 5816, Decoded: There\n",
      "Token: 994, Decoded: fore\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28782, Decoded: 5\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  1\n",
      "Token: 5142, Decoded: So\n",
      "Token: 28725, Decoded: ,\n",
      "Token: 3655, Decoded: Mark\n",
      "Token: 659, Decoded: has\n",
      "Token: 28705, Decoded: \n",
      "Token: 28770, Decoded: 3\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 297, Decoded: in\n",
      "Token: 516, Decoded: his\n",
      "Token: 8759, Decoded: garden\n",
      "Token: 28723, Decoded: .\n",
      "ROW  2\n",
      "Token: 415, Decoded: The\n",
      "Token: 3102, Decoded: total\n",
      "Token: 1474, Decoded: number\n",
      "Token: 302, Decoded: of\n",
      "Token: 11888, Decoded: flowers\n",
      "Token: 349, Decoded: is\n",
      "Token: 1401, Decoded: around\n",
      "Token: 28705, Decoded: \n",
      "Token: 28740, Decoded: 1\n",
      "Token: 28770, Decoded: 3\n",
      "Token: 28723, Decoded: .\n"
     ]
    }
   ],
   "source": [
    "print_decoded_tokens(lsot, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let x be the number of yellow flowers, \"\n",
      " 'y be the number of purple flowers, and z be the number of green flowers. We '\n",
      " 'have three equations based on the given information:\\n'\n",
      " '\\n'\n",
      " '1. x + y + z = total number of flowers\\n'\n",
      " '2. x = 10 (the number of yellow flowers)\\n'\n",
      " '3. y = 1.8x (80% more purple flowers than yellow)\\n'\n",
      " '4. z = 0.25x (25% as many green flowers as yellow and purple)\\n'\n",
      " '\\n'\n",
      " \"First, let's find the number of purple flowers (y). According to equation \"\n",
      " '3:\\n'\n",
      " '\\n'\n",
      " 'y = 1.8 * 10\\n'\n",
      " 'y = 18\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers (z). According to equation 4:\\n\"\n",
      " '\\n'\n",
      " 'z = 0.25 * (x + y)\\n'\n",
      " 'z = 0.25 * (10 + 18)\\n'\n",
      " 'z = 0.25 * 28\\n'\n",
      " 'z = 7\\n'\n",
      " '\\n'\n",
      " 'Now we have all the flower counts:\\n'\n",
      " '- Yellows: x = 10\\n'\n",
      " '- Purples: y = 18\\n'\n",
      " '- Greens: z = 7\\n'\n",
      " '\\n'\n",
      " \"So, the total number of flowers in Mark's garden is:\\n\"\n",
      " '\\n'\n",
      " 'x + y + z = 10 + 18 + 7 = 35\\n'\n",
      " '\\n'\n",
      " 'Therefore, Mark has 35 flowers in his '\n",
      " 'garden.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(outputs.sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let's denote the number of purple \"\n",
      " 'flowers as X. We know that there are 80% more purple flowers than yellow '\n",
      " 'ones, so the number of yellow flowers is X/1.8 (since 80% is equivalent to '\n",
      " '0.8 or 1/1.8). We also know that there are 25% as many green flowers as '\n",
      " \"there are yellow and purple flowers combined. Let's denote the number of \"\n",
      " 'green flowers as G.\\n'\n",
      " '\\n'\n",
      " \"First, let's find the total number of yellow and purple flowers:\\n\"\n",
      " 'Total yellow and purple flowers = X + X/1.8\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers:\\n\"\n",
      " 'Green flowers = 0.25 * (X + X/1.8)\\n'\n",
      " '\\n'\n",
      " 'Since we know that Mark has ten yellow flowers, we can set up an equation to '\n",
      " 'find the value of X:\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 0.25 * (10 + X/1.8)\\n'\n",
      " '\\n'\n",
      " \"Let's simplify the equation:\\n\"\n",
      " 'X + X/1.8 = 10 + X/1.8 + 2.5 * (10 + X/1.8)\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 25 + 2.5 * X\\n'\n",
      " 'X = 25\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of yellow and purple flowers:\\n'\n",
      " 'Total yellow and purple flowers = 25 + 25/1.8 = 38.885… ≈ 39\\n'\n",
      " '\\n'\n",
      " \"Since we can't have a fraction of a flower, we'll round up to 40.\\n\"\n",
      " '\\n'\n",
      " 'The number of yellow flowers is 40/2 = 20.\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of purple flowers:\\n'\n",
      " 'Purple flowers = 40 * 1.8 = 72\\n'\n",
      " '\\n'\n",
      " 'Finally, we can find the number of green flowers:\\n'\n",
      " 'Green flowers = 0.25 * 40 * 1.8 = 18\\n'\n",
      " '\\n'\n",
      " 'So, Mark has approximately 40 yellow flowers, 72 purple flowers, and 18 '\n",
      " 'green flowers in his '\n",
      " 'garden.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(ist[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> '\n",
      " 'The total number of flowers is around 130.</s>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(lsot[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankensteining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_last_sentence_of_first_row(ist, lsot, lsot_index):\n",
    "    # Extract the last sentence from the first row of lsot\n",
    "    last_sentence_first_row = lsot[lsot_index]\n",
    "    # Filter out zeros (masked tokens)\n",
    "    last_sentence_first_row = last_sentence_first_row[last_sentence_first_row != 0]\n",
    "    last_sentence_first_row = last_sentence_first_row[last_sentence_first_row != 2]\n",
    "    last_sentence_first_row = torch.cat((last_sentence_first_row, torch.tensor([2]).to(last_sentence_first_row.device)))\n",
    "    print(\"lsfr\", last_sentence_first_row)\n",
    "\n",
    "    # Determine the maximum possible length of the new rows\n",
    "    max_length = ist.size(1) + last_sentence_first_row.size(0)\n",
    "\n",
    "    # Create a new tensor to hold the result with the expanded size\n",
    "    new_tensor = torch.zeros((ist.size(0), max_length), dtype=ist.dtype)\n",
    "\n",
    "    for i in range(ist.size(0)):\n",
    "        # Get the initial sentence tokens from ist for the current row\n",
    "        initial_sentence = ist[i]\n",
    "        initial_sentence = initial_sentence[initial_sentence != 0]\n",
    "\n",
    "        # Concatenate the initial sentence with the last sentence of the first row\n",
    "        combined_sentence = torch.cat((initial_sentence, last_sentence_first_row))\n",
    "\n",
    "        # Place the combined sentence into the new tensor, respecting the original row index\n",
    "        new_tensor[i, :combined_sentence.size(0)] = combined_sentence\n",
    "\n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def append_last_sentence_of_first_row_with_mask(ist, lsot):\n",
    "    # Extract the last sentence from the first row of lsot\n",
    "    last_sentence_first_row = lsot[0]\n",
    "    # Filter out zeros (masked tokens)\n",
    "    last_sentence_first_row = last_sentence_first_row[last_sentence_first_row != 0]\n",
    "\n",
    "    # Determine the maximum possible length of the new rows\n",
    "    max_length = ist.size(1) + last_sentence_first_row.size(0)\n",
    "\n",
    "    # Create a new tensor to hold the result with the expanded size\n",
    "    new_tensor = torch.zeros((ist.size(0), max_length), dtype=ist.dtype)\n",
    "    new_mask = torch.zeros((ist.size(0), max_length), dtype=torch.uint8)\n",
    "\n",
    "    for i in range(ist.size(0)):\n",
    "        # Get the initial sentence tokens from ist for the current row\n",
    "        initial_sentence = ist[i]\n",
    "        initial_sentence = initial_sentence[initial_sentence != 0]\n",
    "\n",
    "        # Concatenate the initial sentence with the last sentence of the first row\n",
    "        combined_sentence = torch.cat((initial_sentence, last_sentence_first_row))\n",
    "\n",
    "        # Place the combined sentence into the new tensor, respecting the original row index\n",
    "        new_tensor[i, :combined_sentence.size(0)] = combined_sentence\n",
    "\n",
    "        # Create mask for the appended last sentence\n",
    "        start_index_of_last_sentence = initial_sentence.size(0)\n",
    "        end_index_of_last_sentence = start_index_of_last_sentence + last_sentence_first_row.size(0)\n",
    "        new_mask[i, start_index_of_last_sentence:end_index_of_last_sentence] = 1\n",
    "\n",
    "    return new_tensor, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsfr tensor([ 5816,   994, 28725,  3655,   659, 28705, 28770, 28782, 11888,   297,\n",
      "          516,  8759, 28723,     2], device='mps:0')\n",
      "tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = append_last_sentence_of_first_row(ist, lsot, 0)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let's denote the number of purple \"\n",
      " 'flowers as X. We know that there are 80% more purple flowers than yellow '\n",
      " 'ones, so the number of yellow flowers is X/1.8 (since 80% is equivalent to '\n",
      " '0.8 or 1/1.8). We also know that there are 25% as many green flowers as '\n",
      " \"there are yellow and purple flowers combined. Let's denote the number of \"\n",
      " 'green flowers as G.\\n'\n",
      " '\\n'\n",
      " \"First, let's find the total number of yellow and purple flowers:\\n\"\n",
      " 'Total yellow and purple flowers = X + X/1.8\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers:\\n\"\n",
      " 'Green flowers = 0.25 * (X + X/1.8)\\n'\n",
      " '\\n'\n",
      " 'Since we know that Mark has ten yellow flowers, we can set up an equation to '\n",
      " 'find the value of X:\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 0.25 * (10 + X/1.8)\\n'\n",
      " '\\n'\n",
      " \"Let's simplify the equation:\\n\"\n",
      " 'X + X/1.8 = 10 + X/1.8 + 2.5 * (10 + X/1.8)\\n'\n",
      " 'X + X/1.8 = 10 + X/1.8 + 25 + 2.5 * X\\n'\n",
      " 'X = 25\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of yellow and purple flowers:\\n'\n",
      " 'Total yellow and purple flowers = 25 + 25/1.8 = 38.885… ≈ 39\\n'\n",
      " '\\n'\n",
      " \"Since we can't have a fraction of a flower, we'll round up to 40.\\n\"\n",
      " '\\n'\n",
      " 'The number of yellow flowers is 40/2 = 20.\\n'\n",
      " '\\n'\n",
      " 'Now, we can find the number of purple flowers:\\n'\n",
      " 'Purple flowers = 40 * 1.8 = 72\\n'\n",
      " '\\n'\n",
      " 'Finally, we can find the number of green flowers:\\n'\n",
      " 'Green flowers = 0.25 * 40 * 1.8 = 18\\n'\n",
      " '\\n'\n",
      " 'So, Mark has approximately 40 yellow flowers, 72 purple flowers, and 18 '\n",
      " 'green flowers in his garden.Therefore, Mark has 35 flowers in his '\n",
      " 'garden.</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(new_tensor[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsfr tensor([  415,  3102,  1474,   302, 11888,   349,  1401, 28705, 28740, 28770,\n",
      "        28734, 28723,     2], device='mps:0')\n",
      "tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = append_last_sentence_of_first_row(ist, lsot, 2)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> [INST] Q: Mark has a garden with flowers. He planted plants of three '\n",
      " 'different colors in it. Ten of them are yellow, and there are 80% more of '\n",
      " 'those in purple. There are only 25% as many green flowers as there are '\n",
      " 'yellow and purple flowers. How many flowers does Mark have in his garden?\\n'\n",
      " \"A: Let's think step by step. [/INST] Let x be the number of yellow flowers, \"\n",
      " 'y be the number of purple flowers, and z be the number of green flowers. We '\n",
      " 'have three equations based on the given information:\\n'\n",
      " '\\n'\n",
      " '1. x + y + z = total number of flowers\\n'\n",
      " '2. x = 10 (the number of yellow flowers)\\n'\n",
      " '3. y = 1.8x (80% more purple flowers than yellow)\\n'\n",
      " '4. z = 0.25x (25% as many green flowers as yellow and purple)\\n'\n",
      " '\\n'\n",
      " \"First, let's find the number of purple flowers (y). According to equation \"\n",
      " '3:\\n'\n",
      " '\\n'\n",
      " 'y = 1.8 * 10\\n'\n",
      " 'y = 18\\n'\n",
      " '\\n'\n",
      " \"Now, let's find the number of green flowers (z). According to equation 4:\\n\"\n",
      " '\\n'\n",
      " 'z = 0.25 * (x + y)\\n'\n",
      " 'z = 0.25 * (10 + 18)\\n'\n",
      " 'z = 0.25 * 28\\n'\n",
      " 'z = 7\\n'\n",
      " '\\n'\n",
      " 'Now we have all the flower counts:\\n'\n",
      " '- Yellows: x = 10\\n'\n",
      " '- Purples: y = 18\\n'\n",
      " '- Greens: z = 7\\n'\n",
      " '\\n'\n",
      " \"So, the total number of flowers in Mark's garden is:\\n\"\n",
      " '\\n'\n",
      " 'x + y + z = 10 + 18 + 7 = 35\\n'\n",
      " '\\n'\n",
      " ' The total number of flowers is around '\n",
      " '130.</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>')\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.decode(new_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(outputs.sequences, 'tensors/flowers_3_ans_test.pt')\n",
    "print(\"Tensor saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekvajipey/miniconda3/envs/nightly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from util import get_prompt_message, extract_last_integer, extract_last_number\n",
    "from util import remove_last_sentence\n",
    "from gsm8k_distributions import find_last_sentence, append_last_sentence_of_first_row_with_mask\n",
    "\n",
    "gpt35_df = pd.read_csv('../conditional/data/112_gsm8k_gpt35_cot_onesent_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.46s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(base_model_name)\n",
    "base_model.generation_config.pad_token_id =base_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tensor: tensor([[    1,   733, 16289,  ...,     2,     2,     2],\n",
      "        [    1,   733, 16289,  ...,     2,     2,     2],\n",
      "        [    1,   733, 16289,  ..., 28734, 28723,     2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "gen_outputs = torch.load('tensors/flowers_3_ans_test.pt')\n",
    "print(\"Loaded tensor:\", gen_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='mps:0')\n",
      "Inverse Mask:\n",
      " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='mps:0')\n",
      "Last Sentence Only Tensor:\n",
      " tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [    0,     0,     0,  ..., 28734, 28723,     2]], device='mps:0')\n",
      "Initial Sentences Tensor:\n",
      " tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "ist, lsot = find_last_sentence(gen_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAfCAYAAAABDtrDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAACfElEQVR4nO3csUocURQGYGd2SecLLCqktlmwsEgXGBbsF/EVfIG8gpLSwtZ+eyPZIlWKSR8QFLSwdpM+mWsnnEKYCRwWNt9XzQ/nzj0P8HOrUkrZAgAAAAAAAAAASFKvewEAAAAAAAAAAGCzKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKQa9x1s6nnmHgAAAAAAwJr9+XgQ8rv2NuQvd99DPmqOQ+7uH0O+eWgH3b9/cRryznk8X41GIa9O4r7t2WXIs8l00P0AAMBwy27Ra85LSgAAAAAAAAAAQColJQAAAAAAAAAAIJWSEgAAAAAAAAAAkKoqpZQ+g009z94FAAAA4L9TPkxD/rq4CvmoOQ65u38M+eahHXTf/sVpyDvn8Xw1GoW8OjkIuT27fP2eTaaD7gYAAABg8yy7Ra85LykBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAECqqpRS+gw29Tx7FwAAAAAAAAAA2GjVeBxy/X4vDjz/CvHv6nfIT58OQ979/CP+b3s75Ouf3/5hy7fNJtOQl92i1zkvKQEAAAAAAAAAAKmUlAAAAAAAAAAAgFRKSgAAAAAAAAAAQKqqlFLWvQQAAAAAAAAAALC5vKQEAAAAAAAAAACkUlICAAAAAAAAAABSKSkBAAAAAAAAAACplJQAAAAAAAAAAIBUSkoAAAAAAAAAAEAqJSUAAAAAAAAAACCVkhIAAAAAAAAAAJBKSQkAAAAAAAAAAEilpAQAAAAAAAAAAKR6AdGuRi1/V93/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(lsot.cpu().numpy(), cmap='viridis')\n",
    "# plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Tensor:\n",
      " tensor([[    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0],\n",
      "        [    1,   733, 16289,  ...,     0,     0,     0]])\n",
      "New Mask:\n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "new_tensor, new_mask = append_last_sentence_of_first_row_with_mask(ist, lsot)\n",
    "print(\"New Tensor:\\n\", new_tensor)\n",
    "print(\"New Mask:\\n\", new_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 597])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(new_tensor.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACSkAAAAfCAYAAAABDtrDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB1klEQVR4nO3csQ2DUBBEQUBUQRU0YVGBq3QFyE1QBWX4O3VIsj4EM/EPNiJAT9e31loHAAAAAAAAAAAQMlQPAAAAAAAAAAAArk2kBAAAAAAAAAAARImUAAAAAAAAAACAKJESAAAAAAAAAAAQJVICAAAAAAAAAACiREoAAAAAAAAAAECUSAkAAAAAAAAAAIgSKQEAAAAAAAAAAFHj0YeP4ZncAQAAAAAAt7PuW/WEmGWaqycAAAB/8P68Dr1zSQkAAAAAAAAAAIgSKQEAAAAAAAAAAFEiJQAAAAAAAAAAIGqsHgAAAABwNuu+VU+IWaa5egIAP3yXAQCAu3BJCQAAAAAAAAAAiBIpAQAAAAAAAAAAUSIlAAAAAAAAAAAgaqweAAAAAHA2yzRXTwAAAACg67p136onRNzx/5NLSgAAAAAAAAAAQJRICQAAAAAAAAAAiBIpAQAAAAAAAAAAUX1rrVWPAAAAAAAAAAAArsslJQAAAAAAAAAAIEqkBAAAAAAAAAAARImUAAAAAAAAAACAKJESAAAAAAAAAAAQJVICAAAAAAAAAACiREoAAAAAAAAAAECUSAkAAAAAAAAAAIgSKQEAAAAAAAAAAFEiJQAAAAAAAAAAIOoLsacVXQT4p/YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 3000x10000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30, 100))\n",
    "plt.imshow(new_mask.cpu().numpy(), cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens_and_logprobs(model, tokenizer, input_ids, answer_mask, print_logging=False):\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    answer_mask = answer_mask.to(model.device)\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    if print_logging:\n",
    "        print(\"outputs logits: \", outputs.logits.shape)\n",
    "\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "    # Adjust indices to ignore the first token's log prob as it corresponds to the second token\n",
    "    probs = probs[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "\n",
    "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    masked_probs = gen_probs * answer_mask[:, 1:].float()  # Ensure the mask is aligned and of the correct type\n",
    "    if print_logging:\n",
    "        print(\"masked_probs: \", masked_probs.shape)\n",
    "\n",
    "    summed_probs = masked_probs.sum(dim=1)\n",
    "    if print_logging:\n",
    "        print(\"summed_probs: \", summed_probs.shape)\n",
    "\n",
    "        batch = []\n",
    "        for input_sentence, input_probs in zip(input_ids, masked_probs):\n",
    "            text_sequence = []\n",
    "            for token, p in zip(input_sentence, input_probs):\n",
    "                if token not in tokenizer.all_special_ids:\n",
    "                    print((tokenizer.decode(token), p.item()))\n",
    "                    text_sequence.append((tokenizer.decode(token), p.item()))\n",
    "            batch.append(text_sequence)\n",
    "\n",
    "    return summed_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_probs = to_tokens_and_logprobs(base_model, base_tokenizer, new_tensor, new_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -2.4279, -15.7756, -22.9401], device='mps:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-13.7145, device='mps:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(summed_probs / 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-13.7145, device='mps:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_probs.sum() / 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
