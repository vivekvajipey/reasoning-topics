{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/jshen3/.cache/huggingface/token\n",
      "Login successful\n",
      "Loading  mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841fb0932bd9474abb3022782eaa1e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/jshen3/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/jshen3/.cache/huggingface'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login('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')\n",
    "import argparse\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from util import get_prompt_message, extract_last_integer, extract_last_number\n",
    "from util import remove_last_sentence\n",
    "from util import set_seed\n",
    "from util import print_tensors_on_cuda_gpu, print_tensors_on_mps_gpu\n",
    "import datetime\n",
    "\n",
    "\n",
    "name2base = {\"mistral-7b-v0.1\":\"mistralai/Mistral-7B-v0.1\"}\n",
    "\n",
    "base_model_name = name2base['mistral-7b-v0.1']\n",
    "print(\"Loading \", base_model_name)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(base_model_name)\n",
    "base_model.generation_config.pad_token_id =base_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for q in range(100):\n",
    "    print(q)\n",
    "    cot_path = f'mistral-7b-v0.1-samples25-fewshot0-temp0.7-topk40-CoT-gsm8k_p{q}.pt'\n",
    "    # direct_path = f'tensors/mistral-7b-v0.1-samples25-fewshot0-temp0.7-topk40-direct-gsm8k_p{q}.pt'\n",
    "    # log_prob_path = f'logprob_tensors/mistral-7b-v0.1-samples25-fewshot0-temp0.7-topk40-CoT-gsm8k_p{q}-logprobs.pt'\n",
    "    cot_tokens = torch.load('tensors/' + cot_path)\n",
    "    # direct_tokens = torch.load(direct_path)\n",
    "    # print('CoT Tokens Shape:', cot_tokens.shape)\n",
    "    # print(\"cot_tokens\", cot_tokens)\n",
    "    cot_tokens = cot_tokens.to(base_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model(cot_tokens)\n",
    "    logprobs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "    # print('Direct Tokens :', direct_tokens.shape)\n",
    "    logprobs = logprobs[:, :-1, :]\n",
    "    #print(\"log_probs\", logprobs.shape, logprobs)\n",
    "    # print(\"log_probs shape\", logprobs.shape)\n",
    "    \n",
    "    cot_tokens  = cot_tokens [:, 1:]\n",
    "    gen_logprobs = torch.gather(logprobs, 2, cot_tokens[:, :, None]).squeeze(-1)\n",
    "\n",
    "    torch.save(gen_logprobs, f\"other_logprob_tensors/{cot_path}-logprobs.pt\")\n",
    "\n",
    "    # print('LogProbs: ', probs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jshen3Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
