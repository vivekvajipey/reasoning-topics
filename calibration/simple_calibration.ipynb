{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import get_close_matches\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"mistral-7b-v0.1\"\n",
    "\n",
    "name2base = {\"mistral-7b-v0.1\":\"mistralai/Mistral-7B-v0.1\"}\n",
    "name2instruct = {\"mistral-7b-v0.1\": \"mistralai/Mistral-7B-Instruct-v0.1\"}\n",
    "\n",
    "base_model_name = name2base[model_name]\n",
    "print(\"Loading \", base_model_name)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(base_model_name)\n",
    "base_model.generation_config.pad_token_id =base_model.generation_config.eos_token_id\n",
    "\n",
    "name2instruct = {\"mistral-7b-v0.1\": \"mistralai/Mistral-7B-Instruct-v0.1\"}\n",
    "instruct_model_name = name2instruct[model_name]\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"A B C\n",
    "# A B C\n",
    "\n",
    "# X Y Z\n",
    "# X Y \n",
    "# \"\"\"\n",
    "\n",
    "# prompt = \"Complete the sequence: A, B, C, __.\"\n",
    "prompt = \"1 + 1 = \"\n",
    "\n",
    "prompt_tensor = base_tokenizer.encode(prompt, return_tensors='pt')\n",
    "print([(n, base_tokenizer.decode(n)) for n in list(n.item() for n in prompt_tensor.flatten())])\n",
    "\n",
    "n_samples = 3\n",
    "prompt_tensor = prompt_tensor.repeat(n_samples, 1)\n",
    "prompt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.0\n",
    "top_k = 50\n",
    "\n",
    "outputs = base_model.generate(\n",
    "                    prompt_tensor.to(base_model.device),\n",
    "                    min_new_tokens=0,\n",
    "                    max_new_tokens=2,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    do_sample=True,\n",
    "                    temperature=1.0,\n",
    "                    top_k=50,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in base_tokenizer.batch_decode(outputs.sequences):\n",
    "    pprint(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_log_probs = base_model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "og_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scores = base_model(outputs.sequences)\n",
    "\n",
    "scores.logits.shape\n",
    "logits = scores.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_scored_logprobs = torch.log_softmax(logits, dim=-1).detach()\n",
    "vocab_scored_logprobs = vocab_scored_logprobs[:, :-1, :]\n",
    "token_ids = outputs.sequences[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer.batch_decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_logprobs = torch.gather(vocab_scored_logprobs, 2, token_ids[:, :, None]).squeeze(-1)\n",
    "for input_sentence, input_probs in zip(token_ids , scored_logprobs):\n",
    "    # for input_sentence, input_probs in zip(batch_ids , gen_logprobs): # check all logprobs\n",
    "    print(\"---sequence---\")\n",
    "    for token, p in zip(input_sentence, input_probs):\n",
    "        # if token not in base_tokenizer.all_special_ids:\n",
    "        pprint(f\"{base_tokenizer.decode(token)} ({token}): {p.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_logprobs [:, -2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Marginalizing over B_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequence(tensor, model, tokenizer, print_logging=False):\n",
    "    tensor = tensor.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(tensor)\n",
    "\n",
    "    scores.logits.shape\n",
    "    logits = scores.logits\n",
    "    vocab_scored_logprobs = torch.log_softmax(logits, dim=-1).detach()\n",
    "    vocab_scored_logprobs = vocab_scored_logprobs[:, :-1, :]\n",
    "    token_ids = tensor[:, 1:]\n",
    "    scored_logprobs = torch.gather(vocab_scored_logprobs, 2, token_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    if print_logging:\n",
    "        for input_sentence, input_probs in zip(token_ids , scored_logprobs):\n",
    "            # for input_sentence, input_probs in zip(batch_ids , gen_logprobs): # check all logprobs\n",
    "            print(\"---sequence---\")\n",
    "            for token, p in zip(input_sentence, input_probs):\n",
    "                # if token not in base_tokenizer.all_special_ids:\n",
    "                pprint(f\"{tokenizer.decode(token)} ({token}): {p.item()}\")\n",
    "\n",
    "    return scored_logprobs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"1 + 1 = \"\n",
    "\n",
    "prompt_tensor = base_tokenizer.encode(prompt, return_tensors='pt')\n",
    "print([(n, base_tokenizer.decode(n)) for n in list(n.item() for n in prompt_tensor.flatten())])\n",
    "prompt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_A = score_sequence(prompt_tensor, base_model, base_tokenizer, print_logging=True)\n",
    "print(\"log P(A): \", prob_A.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 3\n",
    "prompt_tensor = prompt_tensor.repeat(n_samples, 1)\n",
    "prompt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.0\n",
    "top_k = 50\n",
    "\n",
    "completions = base_model.generate(\n",
    "                    prompt_tensor.to(base_model.device),\n",
    "                    min_new_tokens=0,\n",
    "                    max_new_tokens=2,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    do_sample=True,\n",
    "                    temperature=1.0,\n",
    "                    top_k=50,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer.batch_decode(completions.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_A_B = score_sequence(completions.sequences, base_model, base_tokenizer, print_logging=True)\n",
    "\n",
    "print(\"log sum P(A, Bi): \", torch.logsumexp(prob_A_B, dim=0).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.logsumexp(prob_A_B, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def score_sequence(tensor, model, tokenizer, print_logging=False):\n",
    "    tensor = tensor.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(tensor)\n",
    "\n",
    "    scores.logits.shape\n",
    "    logits = scores.logits\n",
    "    vocab_scored_logprobs = torch.log_softmax(logits, dim=-1).detach()\n",
    "    vocab_scored_logprobs = vocab_scored_logprobs[:, :-1, :]\n",
    "    token_ids = tensor[:, 1:]\n",
    "    scored_logprobs = torch.gather(vocab_scored_logprobs, 2, token_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    if print_logging:\n",
    "        for input_sentence, input_probs in zip(token_ids , scored_logprobs):\n",
    "            # for input_sentence, input_probs in zip(batch_ids , gen_logprobs): # check all logprobs\n",
    "            print(\"---sequence---\")\n",
    "            for token, p in zip(input_sentence, input_probs):\n",
    "                # if token not in base_tokenizer.all_special_ids:\n",
    "                pprint(f\"{tokenizer.decode(token)} ({token}): {p.item()}\")\n",
    "\n",
    "    return scored_logprobs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_logsumexp(model, tokenizer, prompt, n_samples, max_new_tokens, temp=1.0, top_k=50, print_logging=False):\n",
    "    prompt_tensor = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    prompt_tensor = prompt_tensor.repeat(n_samples, 1)\n",
    "    \n",
    "    completions = model.generate(\n",
    "                        prompt_tensor.to(base_model.device),\n",
    "                        min_new_tokens=0,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True,\n",
    "                        do_sample=True,\n",
    "                        temperature=temp,\n",
    "                        top_k=top_k,\n",
    "                    )\n",
    "    \n",
    "    logprob_A_B = score_sequence(completions.sequences, base_model, base_tokenizer, print_logging=print_logging)\n",
    "    return torch.logsumexp(logprob_A_B, dim=0).item()\n",
    "\n",
    "prompt = \"1 + 1 = \"\n",
    "prompt_tensor = base_tokenizer.encode(prompt, return_tensors='pt')\n",
    "logprob_A = score_sequence(prompt_tensor, base_model, base_tokenizer, print_logging=True)\n",
    "\n",
    "max_new_tokens = 2\n",
    "temp = 1.0\n",
    "top_k = 50\n",
    "# n_samples_values = list(range(1, 100))\n",
    "n_samples_values = [1, 5, 10, 50, 100, 500, 1000]\n",
    "logsumexp_values = [calculate_logsumexp(base_model, base_tokenizer, prompt, n, max_new_tokens, temp, top_k) for n in n_samples_values]\n",
    "\n",
    "plt.plot(n_samples_values, logsumexp_values)\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('logsum(P(A, B)')\n",
    "plt.axhline(y=logprob_A.cpu(), color='r', linestyle='--', label='log P(A)')\n",
    "plt.title(f\"log Sum of P(A, Bi): base model, {max_new_tokens} tokens, t={temp}, top {top_k}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def calculate_logsumexp(model, tokenizer, prompt, n_samples, max_new_tokens, temp=1.0, top_k=50, print_logging=False):\n",
    "    prompt_tensor = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    prompt_tensor = prompt_tensor.repeat(n_samples, 1)\n",
    "\n",
    "    if temp > 0:\n",
    "        completions = model.generate(\n",
    "                            prompt_tensor.to(base_model.device),\n",
    "                            min_new_tokens=0,\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            return_dict_in_generate=True,\n",
    "                            output_scores=True,\n",
    "                            do_sample=True,\n",
    "                            temperature=temp,\n",
    "                            top_k=top_k,\n",
    "                        )\n",
    "    else:\n",
    "        completions = model.generate(\n",
    "                                prompt_tensor.to(base_model.device),\n",
    "                                min_new_tokens=0,\n",
    "                                max_new_tokens=max_new_tokens,\n",
    "                                return_dict_in_generate=True,\n",
    "                                output_scores=True,\n",
    "                                do_sample=False,\n",
    "                            ) \n",
    "    \n",
    "    unique_completions, inverse_indices = torch.unique(completions.sequences, dim=0, return_inverse=True)\n",
    "    print(f\"Number of unique rows: {unique_completions.shape[0]} (out of {n_samples})\")\n",
    "\n",
    "    completions = completions.sequences.cpu()\n",
    "    logprob_A_B = score_sequence(unique_completions, base_model, base_tokenizer, print_logging=print_logging)\n",
    "    unique_completions = unique_completions.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    return torch.logsumexp(logprob_A_B, dim=0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"1 + 1 = \"\n",
    "prompt_tensor = base_tokenizer.encode(prompt, return_tensors='pt')\n",
    "logprob_A = score_sequence(prompt_tensor, base_model, base_tokenizer, print_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 2\n",
    "temp = 1.0\n",
    "top_k = 50\n",
    "# n_samples_values = list(range(1, 100))\n",
    "n_samples_values = [1, 5, 10, 50, 100, 500, 1000]\n",
    "logsumexp_values = [calculate_logsumexp(base_model, base_tokenizer, prompt, n, max_new_tokens, temp, top_k) for n in n_samples_values]\n",
    "\n",
    "plt.plot(n_samples_values, logsumexp_values)\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('logsum(P(A, B)')\n",
    "plt.axhline(y=logprob_A.cpu(), color='r', linestyle='--', label='log P(A)')\n",
    "plt.title(f\"log Sum of P(A, Bi): base model, {max_new_tokens} tokens, t={temp}, top {top_k}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different temperatures\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "n_samples_values = [50, 100, 500, 1000, 2000]\n",
    "temperatures = np.arange(0.0, 2.1, 0.2)\n",
    "top_k=50\n",
    "cmap = cm.get_cmap('coolwarm')\n",
    "\n",
    "for i, temp in enumerate(temperatures):\n",
    "    print(f\"====temp {temp}====\")\n",
    "    logsumexp_values = [calculate_logsumexp(base_model, base_tokenizer, prompt, n, max_new_tokens, temp, top_k) for n in n_samples_values]\n",
    "    plt.plot(n_samples_values, logsumexp_values, color=cmap(i / len(temperatures)), label=f'temp={temp:.3f}')\n",
    "\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('logsum(P(A, B)')\n",
    "plt.axhline(y=logprob_A.cpu(), color='k', linestyle='--', label=f'log P(A)={logprob_A.item().cpu()}')\n",
    "plt.title(f\"log Sum of P(A, Bi): base model, {max_new_tokens} tokens, top {top_k}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different top ks\n",
    "n_samples_values = [50, 100, 500, 1000]\n",
    "top_ks = [1, 10, 25, 50, 100, 200, 500, 1000, 2000]\n",
    "cmap = cm.get_cmap('viridis')\n",
    "\n",
    "temp=1.0\n",
    "\n",
    "for i, top_k in enumerate(top_ks):\n",
    "    print(f\"====top {top_k}====\")\n",
    "    logsumexp_values = [calculate_logsumexp(base_model, base_tokenizer, prompt, n, max_new_tokens, temp, top_k) for n in n_samples_values]\n",
    "    plt.plot(n_samples_values, logsumexp_values, color=cmap(i / len(top_ks)), label=f'top {top_k}')\n",
    "\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('logsum(P(A, B)')\n",
    "plt.axhline(y=logprob_A.cpu(), color='k', linestyle='--', label=f'log P(A)={logprob_A.item().cpu():.4f}')\n",
    "plt.title(f\"log Sum of P(A, Bi): base model, {max_new_tokens} tokens, temp {temp}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_values = [50, 100, 500, 1000, 2000]\n",
    "new_token_lengths = [1, 2, 3, 5, 10, 15]\n",
    "# new_token_lengths = [15, 20, 30]\n",
    "temp = 1.0\n",
    "top_k = 50\n",
    "cmap = cm.get_cmap('plasma')\n",
    "\n",
    "for i, new_tokens in enumerate(new_token_lengths):\n",
    "    print(f\"====tokens: {new_tokens}====\")\n",
    "    logsumexp_values = [calculate_logsumexp(base_model, base_tokenizer, prompt, n, new_tokens, temp, top_k) for n in n_samples_values]\n",
    "    plt.plot(n_samples_values, logsumexp_values, color=cmap(i / len(new_token_lengths)), label=f'{new_tokens} tokens')\n",
    "\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('logsum(P(A, B)')\n",
    "plt.axhline(y=logprob_A.cpu(), color='k', linestyle='--', label=f'log P(A)={logprob_A.item():.4f}')\n",
    "plt.title(f\"log Sum of P(A, Bi): base model, temp {temp}, top {top_k}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen Instruct, Score Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/vvajipey/.cache/huggingface/token\n",
      "Login successful\n",
      "Loading  mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c1613d750e4cc0ad766325550ef8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdc598036b54a4cb6601997207e31a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import get_close_matches\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"mistral-7b-v0.1\"\n",
    "\n",
    "name2base = {\"mistral-7b-v0.1\":\"mistralai/Mistral-7B-v0.1\"}\n",
    "name2instruct = {\"mistral-7b-v0.1\": \"mistralai/Mistral-7B-Instruct-v0.1\"}\n",
    "\n",
    "base_model_name = name2base[model_name]\n",
    "print(\"Loading \", base_model_name)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(base_model_name)\n",
    "base_model.generation_config.pad_token_id =base_model.generation_config.eos_token_id\n",
    "\n",
    "name2instruct = {\"mistral-7b-v0.1\": \"mistralai/Mistral-7B-Instruct-v0.1\"}\n",
    "instruct_model_name = name2instruct[model_name]\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "instruct_model.generation_config = GenerationConfig.from_pretrained(instruct_model_name)\n",
    "instruct_model.generation_config.pad_token_id = instruct_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequence(tensor, model, tokenizer, print_logging=False):\n",
    "    tensor = tensor.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(tensor)\n",
    "\n",
    "    scores.logits.shape\n",
    "    logits = scores.logits\n",
    "    vocab_scored_logprobs = torch.log_softmax(logits, dim=-1).detach()\n",
    "    vocab_scored_logprobs = vocab_scored_logprobs[:, :-1, :]\n",
    "    token_ids = tensor[:, 1:]\n",
    "    scored_logprobs = torch.gather(vocab_scored_logprobs, 2, token_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    if print_logging:\n",
    "        for input_sentence, input_probs in zip(token_ids , scored_logprobs):\n",
    "            # for input_sentence, input_probs in zip(batch_ids , gen_logprobs): # check all logprobs\n",
    "            print(\"---sequence---\")\n",
    "            for token, p in zip(input_sentence, input_probs):\n",
    "                # if token not in base_tokenizer.all_special_ids:\n",
    "                pprint(f\"{tokenizer.decode(token)} ({token}): {p.item()}\")\n",
    "\n",
    "    return scored_logprobs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def calculate_logsumexp(model, tokenizer, prompt, n_samples_values, max_new_tokens, temp=1.0, top_k=50, print_logging=False):\n",
    "    sequences = []\n",
    "    chat = [{\"role\":\"user\", \"content\": prompt}]\n",
    "    prompt_tensor = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors='pt')\n",
    "\n",
    "    for n_samples in n_samples_values:\n",
    "        prompt_tensor_repeated = prompt_tensor.repeat(n_samples, 1).to(model.device)\n",
    "        completions = model.generate(\n",
    "                            prompt_tensor_repeated,\n",
    "                            min_new_tokens=0,\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            return_dict_in_generate=True,\n",
    "                            output_scores=True,\n",
    "                            do_sample=True,\n",
    "                            temperature=temp,\n",
    "                            top_k=top_k,\n",
    "                        )\n",
    "        sequences.append(completions.sequences.cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    global base_model\n",
    "    base_model = base_model.to('cuda')\n",
    "\n",
    "    # Step 4: Score all sequences\n",
    "    logsumexps_values = []\n",
    "    for n_samples, seq in zip(n_samples_values, sequences):\n",
    "        unique_completions, inverse_indices = torch.unique(seq, dim=0, return_inverse=True)\n",
    "        print(f\"Number of unique rows: {unique_completions.shape[0]} (out of {n_samples})\")\n",
    "        logprob_A_B = score_sequence(unique_completions, base_model, base_tokenizer, print_logging=print_logging)\n",
    "        logsumexps_values.append(torch.logsumexp(logprob_A_B, dim=0).cpu().item())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return logsumexps_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer:'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/commonsense_qa_train.csv')\n",
    "df['choices'] = df['choices'].apply(ast.literal_eval)\n",
    "\n",
    "q_num = 1\n",
    "prompt = f\"\"\"Question: {df['question'][q_num]}\n",
    "    Choices:\n",
    "    (A) {df['choices'][q_num]['text'][0]}\n",
    "    (B) {df['choices'][q_num]['text'][1]}\n",
    "    (C) {df['choices'][q_num]['text'][2]}\n",
    "    (D) {df['choices'][q_num]['text'][3]}\n",
    "    (E) {df['choices'][q_num]['text'][4]}\n",
    "    Answer:\"\"\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer: [/INST]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"1 + 1 = \"\n",
    "chat = [{\"role\":\"user\", \"content\": prompt}]\n",
    "prompt = instruct_tokenizer.apply_chat_template(chat, tokenize=False, return_tensors='pt')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples_values = [50, 100, 500, 1000, 2000]\n",
    "# n_samples_values = [1, 5, 10, 100, 1000]\n",
    "n_samples_values = [1, 5, 10, 100, 1000]\n",
    "new_tokens = 2\n",
    "temp = 1.0\n",
    "top_k = 50\n",
    "\n",
    "res = calculate_logsumexp(instruct_model, instruct_tokenizer, prompt, n_samples_values, new_tokens, temp=temp, top_k=top_k, print_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tensor = instruct_tokenizer.apply_chat_template(chat, tokenize=True, return_tensors='pt')\n",
    "logprob_A = score_sequence(prompt_tensor, base_model, base_tokenizer, print_logging=True)\n",
    "logprob_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 2\n",
    "temp = 1.0\n",
    "top_k = 50\n",
    "# n_samples_values = list(range(1, 100))\n",
    "n_samples_values = [1, 5, 10, 50, 100, 500, 1000]\n",
    "logsumexp_values = calculate_logsumexp(instruct_model, instruct_tokenizer, prompt, n_samples_values, new_tokens, temp=temp, top_k=top_k, print_logging=True)\n",
    "\n",
    "plt.plot(n_samples_values, logsumexp_values)\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('logsum(P(A, B)')\n",
    "plt.axhline(y=logprob_A.cpu(), color='r', linestyle='--', label='log P(A)')\n",
    "plt.title(f\"log Sum of P(A, Bi): instruct model, {max_new_tokens} tokens, t={temp}, top {top_k}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 10\n",
    "temp = 1.0\n",
    "top_k = 50\n",
    "# n_samples_values = list(range(1, 100))\n",
    "n_samples_values = [1, 5, 10, 50, 100, 500, 1000]\n",
    "logsumexp_values = calculate_logsumexp(instruct_model, instruct_tokenizer, prompt, n_samples_values, max_new_tokens, temp=temp, top_k=top_k, print_logging=False)\n",
    "\n",
    "plt.plot(n_samples_values, logsumexp_values)\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('logsum(P(A, B)')\n",
    "plt.axhline(y=logprob_A.cpu(), color='r', linestyle='--', label=f'log P(A)={logprob_A.item():.4f}')\n",
    "plt.title(f\"log Sum of P(A, Bi): instruct model, {max_new_tokens} tokens, t={temp}, top {top_k}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
