{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/vvajipey/.cache/huggingface/token\n",
      "Login successful\n",
      "Loading  mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9698d1b9dd0e49b9a0ba6f47cacb2469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import get_close_matches\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"mistral-7b-v0.1\"\n",
    "\n",
    "name2base = {\"mistral-7b-v0.1\":\"mistralai/Mistral-7B-v0.1\"}\n",
    "name2instruct = {\"mistral-7b-v0.1\": \"mistralai/Mistral-7B-Instruct-v0.1\"}\n",
    "\n",
    "base_model_name = name2base[model_name]\n",
    "print(\"Loading \", base_model_name)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(base_model_name)\n",
    "base_model.generation_config.pad_token_id =base_model.generation_config.eos_token_id\n",
    "\n",
    "name2instruct = {\"mistral-7b-v0.1\": \"mistralai/Mistral-7B-Instruct-v0.1\"}\n",
    "instruct_model_name = name2instruct[model_name]\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '<s>'), (28705, ''), (28740, '1'), (648, '+'), (28705, ''), (28740, '1'), (327, '='), (28705, '')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"\"\"A B C\n",
    "# A B C\n",
    "\n",
    "# X Y Z\n",
    "# X Y \n",
    "# \"\"\"\n",
    "\n",
    "# prompt = \"Complete the sequence: A, B, C, __.\"\n",
    "prompt = \"1 + 1 = \"\n",
    "\n",
    "prompt_tensor = base_tokenizer.encode(prompt, return_tensors='pt')\n",
    "print([(n, base_tokenizer.decode(n)) for n in list(n.item() for n in prompt_tensor.flatten())])\n",
    "\n",
    "n_samples = 3\n",
    "prompt_tensor = prompt_tensor.repeat(n_samples, 1)\n",
    "prompt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.0\n",
    "top_k = 50\n",
    "\n",
    "outputs = base_model.generate(\n",
    "                    prompt_tensor.to(base_model.device),\n",
    "                    min_new_tokens=0,\n",
    "                    max_new_tokens=2,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    do_sample=True,\n",
    "                    temperature=1.0,\n",
    "                    top_k=50,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<s> 1 + 1 = 2\\n'\n",
      "'<s> 1 + 1 = 2.'\n",
      "'<s> 1 + 1 = 39'\n"
     ]
    }
   ],
   "source": [
    "for s in base_tokenizer.batch_decode(outputs.sequences):\n",
    "    pprint(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6725, -1.1066],\n",
       "        [-0.6725, -1.6066],\n",
       "        [-1.3600, -6.5624]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_log_probs = base_model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "og_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scores = base_model(outputs.sequences)\n",
    "\n",
    "scores.logits.shape\n",
    "logits = scores.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_scored_logprobs = torch.log_softmax(logits, dim=-1).detach()\n",
    "vocab_scored_logprobs = vocab_scored_logprobs[:, :-1, :]\n",
    "token_ids = outputs.sequences[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 + 1 = 2\\n', '1 + 1 = 2.', '1 + 1 = 39']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.batch_decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---sequence---\n",
      "' (28705): -4.431795120239258'\n",
      "'1 (28740): -0.975562334060669'\n",
      "'+ (648): -9.606401443481445'\n",
      "' (28705): -0.7148184776306152'\n",
      "'1 (28740): -0.33568018674850464'\n",
      "'= (327): -0.4048469364643097'\n",
      "' (28705): -0.21074119210243225'\n",
      "'2 (28750): -0.6747506856918335'\n",
      "'\\n (13): -1.2522019147872925'\n",
      "---sequence---\n",
      "' (28705): -4.431795120239258'\n",
      "'1 (28740): -0.975562334060669'\n",
      "'+ (648): -9.606401443481445'\n",
      "' (28705): -0.7148184776306152'\n",
      "'1 (28740): -0.33568018674850464'\n",
      "'= (327): -0.4048469364643097'\n",
      "' (28705): -0.21074119210243225'\n",
      "'2 (28750): -0.6747506856918335'\n",
      "'. (28723): -1.6897019147872925'\n",
      "---sequence---\n",
      "' (28705): -4.431795120239258'\n",
      "'1 (28740): -0.975562334060669'\n",
      "'+ (648): -9.606401443481445'\n",
      "' (28705): -0.7148184776306152'\n",
      "'1 (28740): -0.33568018674850464'\n",
      "'= (327): -0.4048469364643097'\n",
      "' (28705): -0.21074119210243225'\n",
      "'3 (28770): -1.3622506856918335'\n",
      "'9 (28774): -6.713685989379883'\n"
     ]
    }
   ],
   "source": [
    "scored_logprobs = torch.gather(vocab_scored_logprobs, 2, token_ids[:, :, None]).squeeze(-1)\n",
    "for input_sentence, input_probs in zip(token_ids , scored_logprobs):\n",
    "    # for input_sentence, input_probs in zip(batch_ids , gen_logprobs): # check all logprobs\n",
    "    print(\"---sequence---\")\n",
    "    for token, p in zip(input_sentence, input_probs):\n",
    "        # if token not in base_tokenizer.all_special_ids:\n",
    "        pprint(f\"{base_tokenizer.decode(token)} ({token}): {p.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6748, -1.2522],\n",
       "        [-0.6748, -1.6897],\n",
       "        [-1.3623, -6.7137]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_logprobs [:, -2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Marginalizing over B_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequence(tensor, model, tokenizer, print_logging=False):\n",
    "    tensor = tensor.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(tensor)\n",
    "\n",
    "    scores.logits.shape\n",
    "    logits = scores.logits\n",
    "    vocab_scored_logprobs = torch.log_softmax(logits, dim=-1).detach()\n",
    "    vocab_scored_logprobs = vocab_scored_logprobs[:, :-1, :]\n",
    "    token_ids = tensor[:, 1:]\n",
    "    scored_logprobs = torch.gather(vocab_scored_logprobs, 2, token_ids[:, :, None]).squeeze(-1)\n",
    "    \n",
    "    if print_logging:\n",
    "        for input_sentence, input_probs in zip(token_ids , scored_logprobs):\n",
    "            # for input_sentence, input_probs in zip(batch_ids , gen_logprobs): # check all logprobs\n",
    "            print(\"---sequence---\")\n",
    "            for token, p in zip(input_sentence, input_probs):\n",
    "                # if token not in base_tokenizer.all_special_ids:\n",
    "                pprint(f\"{tokenizer.decode(token)} ({token}): {p.item()}\")\n",
    "\n",
    "    return scored_logprobs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '<s>'), (28705, ''), (28740, '1'), (648, '+'), (28705, ''), (28740, '1'), (327, '='), (28705, '')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"1 + 1 = \"\n",
    "\n",
    "prompt_tensor = base_tokenizer.encode(prompt, return_tensors='pt')\n",
    "print([(n, base_tokenizer.decode(n)) for n in list(n.item() for n in prompt_tensor.flatten())])\n",
    "prompt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---sequence---\n",
      "' (28705): -4.384565353393555'\n",
      "'1 (28740): -0.9495829343795776'\n",
      "'+ (648): -9.594989776611328'\n",
      "' (28705): -0.6903921961784363'\n",
      "'1 (28740): -0.32114025950431824'\n",
      "'= (327): -0.39957746863365173'\n",
      "' (28705): -0.209198459982872'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-16.5494], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_A = score_sequence(prompt_tensor, base_model, base_tokenizer, print_logging=True)\n",
    "prob_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 3\n",
    "prompt_tensor = prompt_tensor.repeat(n_samples, 1)\n",
    "prompt_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.0\n",
    "top_k = 50\n",
    "\n",
    "completions = base_model.generate(\n",
    "                    prompt_tensor.to(base_model.device),\n",
    "                    min_new_tokens=0,\n",
    "                    max_new_tokens=2,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    do_sample=True,\n",
    "                    temperature=1.0,\n",
    "                    top_k=50,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---sequence---\n",
      "' (28705): -4.431795120239258'\n",
      "'1 (28740): -0.975562334060669'\n",
      "'+ (648): -9.606401443481445'\n",
      "' (28705): -0.7148184776306152'\n",
      "'1 (28740): -0.33568018674850464'\n",
      "'= (327): -0.4048469364643097'\n",
      "' (28705): -0.21074119210243225'\n",
      "'0 (28734): -4.299750804901123'\n",
      "'. (28723): -1.7196247577667236'\n",
      "---sequence---\n",
      "' (28705): -4.431795120239258'\n",
      "'1 (28740): -0.975562334060669'\n",
      "'+ (648): -9.606401443481445'\n",
      "' (28705): -0.7148184776306152'\n",
      "'1 (28740): -0.33568018674850464'\n",
      "'= (327): -0.4048469364643097'\n",
      "' (28705): -0.21074119210243225'\n",
      "'2 (28750): -0.6747506856918335'\n",
      "'is (349): -2.877202033996582'\n",
      "---sequence---\n",
      "' (28705): -4.431795120239258'\n",
      "'1 (28740): -0.975562334060669'\n",
      "'+ (648): -9.606401443481445'\n",
      "' (28705): -0.7148184776306152'\n",
      "'1 (28740): -0.33568018674850464'\n",
      "'= (327): -0.4048469364643097'\n",
      "' (28705): -0.21074119210243225'\n",
      "'2 (28750): -0.6747506856918335'\n",
      "'\\n (13): -1.2522019147872925'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-22.6992, -20.2318, -18.6068], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_A_B = score_sequence(completions.sequences, base_model, base_tokenizer, print_logging=True)\n",
    "prob_A_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-18.4132, device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(prob_A_B, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
