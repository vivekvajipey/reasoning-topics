{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/vvajipey/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/vvajipey/miniconda3/envs/reason/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791d8093a7a34dcb81d05b6b6d4465b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import get_close_matches\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# gsm_df = pd.read_csv('../distribution/data/gsm8kTest.csv')\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "train_df = pd.read_csv(\"data/commonsense_qa_train.csv\")\n",
    "train_df['choices'] = train_df['choices'].apply(ast.literal_eval)\n",
    "# train_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_completions_from_model(question, choices, tokenizer, n_samples=1):\n",
    "    # https://arxiv.org/html/2402.17302v1\n",
    "    \n",
    "    prompt = f\"\"\"Question: {question}\n",
    "    Choices:\n",
    "    A. {choices[0]}\n",
    "    B. {choices[1]}\n",
    "    C. {choices[2]}\n",
    "    D. {choices[3]}\n",
    "    E. {choices[4]}\n",
    "    Answer:\"\"\"\n",
    "    chat = [{\"role\":\"user\", \"content\":prompt}]\n",
    "\n",
    "    input_tensor = tokenizer.apply_chat_template(chat)\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        gen_outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        ).sequences\n",
    "    return new_paths, path_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['choices'][1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"Question: {train_df['question'][1]}\n",
    "    Choices:\n",
    "    (A) {train_df['choices'][1]['text'][0]}\n",
    "    (B) {train_df['choices'][1]['text'][1]}\n",
    "    (C) {train_df['choices'][1]['text'][2]}\n",
    "    (D) {train_df['choices'][1]['text'][3]}\n",
    "    (E) {train_df['choices'][1]['text'][4]}\n",
    "    Answer:\"\"\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer: [/INST]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [{\"role\":\"user\", \"content\":prompt}]\n",
    "\n",
    "input_tensor = tokenizer.apply_chat_template(chat, return_tensors='pt')\n",
    "tokenizer.apply_chat_template(chat, tokenize=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> [INST] Question: Sammy wanted to go to where the people were.  Where '\n",
      " 'might he go?\\n'\n",
      " '    Choices:\\n'\n",
      " '    (A) race track\\n'\n",
      " '    (B) populated areas\\n'\n",
      " '    (C) the desert\\n'\n",
      " '    (D) apartment\\n'\n",
      " '    (E) roadblock\\n'\n",
      " '    Answer: [/INST] Sammy might go to where the people are, so the correct '\n",
      " 'choice would be (B) populated areas.</s>']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(gen_outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2758e-01,  0.0000e+00, -3.8946e-01, -1.9602e-02, -5.8768e-05,\n",
       "         -3.8944e+00, -1.0339e-01, -2.3374e-04, -1.1094e-01, -9.0137e-02,\n",
       "         -3.1514e-01, -1.1663e-01, -1.7940e+00, -1.5629e-01, -2.5322e-01,\n",
       "         -1.7047e-05, -3.9002e-03, -1.9322e-03, -1.1921e-07, -5.1259e-05,\n",
       "          0.0000e+00,  0.0000e+00, -1.9518e-03, -2.8916e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores = model.compute_transition_scores(\n",
    "    gen_outputs.sequences, gen_outputs.scores, normalize_logits=True\n",
    ")\n",
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length = input_tensor.shape[1]\n",
    "input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  4157 | Sam      | -0.1276 | 88.02%\n",
      "|  1916 | my       | 0.0000 | 100.00%\n",
      "|  1659 | might    | -0.3895 | 67.74%\n",
      "|   576 | go       | -0.0196 | 98.06%\n",
      "|   298 | to       | -0.0001 | 99.99%\n",
      "|   970 | where    | -3.8944 | 2.04%\n",
      "|   272 | the      | -0.1034 | 90.18%\n",
      "|   905 | people   | -0.0002 | 99.98%\n",
      "|   460 | are      | -0.1109 | 89.50%\n",
      "| 28725 | ,        | -0.0901 | 91.38%\n",
      "|   579 | so       | -0.3151 | 72.97%\n",
      "|   272 | the      | -0.1166 | 88.99%\n",
      "|  4714 | correct  | -1.7940 | 16.63%\n",
      "|  4782 | choice   | -0.1563 | 85.53%\n",
      "|   682 | would    | -0.2532 | 77.63%\n",
      "|   347 | be       | -0.0000 | 100.00%\n",
      "|   325 | (        | -0.0039 | 99.61%\n",
      "| 28760 | B        | -0.0019 | 99.81%\n",
      "| 28731 | )        | -0.0000 | 100.00%\n",
      "|  1852 | pop      | -0.0001 | 99.99%\n",
      "|  6432 | ulated   | 0.0000 | 100.00%\n",
      "|  5020 | areas    | 0.0000 | 100.00%\n",
      "| 28723 | .        | -0.0020 | 99.81%\n",
      "|     2 | </s>     | -0.0003 | 99.97%\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = gen_outputs.sequences[:, input_length:]\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.4f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 = '<s>'\n",
      "325 = '('\n",
      "28741 = 'A'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28760 = 'B'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28743 = 'C'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28757 = 'D'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28749 = 'E'\n",
      "28731 = ')'\n",
      "13 = '\n",
      "'\n",
      "330 = 'A'\n",
      "330 = 'A'\n",
      "13 = '\n",
      "'\n",
      "1094 = 'An'\n",
      "26307 = 'Answer'\n",
      "264 = 'a'\n",
      "365 = 'B'\n",
      "334 = 'C'\n",
      "384 = 'D'\n",
      "413 = 'E'\n"
     ]
    }
   ],
   "source": [
    "letters = tokenizer.encode(\"(A)_(B)_(C)_(D)_(E)\\n A A\\n An Answer a B C D E\")\n",
    "for tok in letters:\n",
    "    print(f\"{tok} = '{tokenizer.decode(tok)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0, 28760,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0, 28760,     0,\n",
       "             0,     0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = gen_outputs.sequences\n",
    "\n",
    "answer_to_tok = {'A':[28741, 330], 'B':[28760, 365], 'C':[28743, 334], 'D':[28757, 384], 'E':[28749, 413]}\n",
    "\n",
    "masked_output_tensor = torch.where(\n",
    "    torch.isin(output_tensor, torch.tensor(answer_to_tok['B'], device=output_tensor.device)),\n",
    "    output_tensor,\n",
    "    torch.tensor(0, device=output_tensor.device)\n",
    ")\n",
    "masked_output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     0 | <unk>    | -0.1276 | 88.02%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.3895 | 67.74%\n",
      "|     0 | <unk>    | -0.0196 | 98.06%\n",
      "|     0 | <unk>    | -0.0001 | 99.99%\n",
      "|     0 | <unk>    | -3.8944 | 2.04%\n",
      "|     0 | <unk>    | -0.1034 | 90.18%\n",
      "|     0 | <unk>    | -0.0002 | 99.98%\n",
      "|     0 | <unk>    | -0.1109 | 89.50%\n",
      "|     0 | <unk>    | -0.0901 | 91.38%\n",
      "|     0 | <unk>    | -0.3151 | 72.97%\n",
      "|     0 | <unk>    | -0.1166 | 88.99%\n",
      "|     0 | <unk>    | -1.7940 | 16.63%\n",
      "|     0 | <unk>    | -0.1563 | 85.53%\n",
      "|     0 | <unk>    | -0.2532 | 77.63%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0039 | 99.61%\n",
      "| 28760 | B        | -0.0019 | 99.81%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0001 | 99.99%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0020 | 99.81%\n",
      "|     0 | <unk>    | -0.0003 | 99.97%\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = masked_output_tensor[:, input_length:]\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.4f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28760, 28760], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_values = masked_output_tensor[masked_output_tensor != 0]\n",
    "non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 28760,     0,     0,\n",
       "             0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_mask = torch.isin(generated_tokens, torch.tensor(answer_to_tok['B'], device=output_tensor.device)).int()\n",
    "masked_generated_tokens = generated_tokens * answer_mask\n",
    "masked_generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "| 28760 | B        | -0.0019 | 99.81%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n"
     ]
    }
   ],
   "source": [
    "masked_log_probs = transition_scores * answer_mask\n",
    "for tok, score in zip(masked_generated_tokens[0], masked_log_probs[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.4f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[28741, 330]\n",
      "['A', 'A']\n",
      "B\n",
      "[28760, 365]\n",
      "['B', 'B']\n",
      "C\n",
      "[28743, 334]\n",
      "['C', 'C']\n",
      "D\n",
      "[28757, 384]\n",
      "['D', 'D']\n",
      "E\n",
      "[28749, 413]\n",
      "['E', 'E']\n"
     ]
    }
   ],
   "source": [
    "for c in \"ABCDE\":\n",
    "    print(c)\n",
    "    print(answer_to_tok[c])\n",
    "    print(tokenizer.batch_decode(answer_to_tok[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_tensor = tokenizer.batch_decode(tokenizer.encode(tokenizer.apply_chat_template(chat, tokenize=False) + \" The answer is (B).\", return_tensors='pt', add_special_tokens=False))\n",
    "b_tensor = tokenizer.encode(tokenizer.apply_chat_template(chat, tokenize=False) + \" The answer is (B).\", return_tensors='pt', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_outputs = model.generate(\n",
    "            b_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> [INST] Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer: [/INST] The answer is (B). Sammy wanted to go to where the people were, and the best option among the given choices is the populated areas, where he is likely to find a large number of people.</s>']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(b_outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_output_tensor = b_outputs.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sammy wanted to go to where the people were, and the best option among the given choices is the populated areas, where he is likely to find a large number of people.</s>']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(b_output_tensor[:, b_tensor.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_num = 1\n",
    "n_samples = 3\n",
    "answer_letter = 'B'\n",
    "\n",
    "train_df = pd.read_csv(\"data/commonsense_qa_train.csv\")\n",
    "train_df['choices'] = train_df['choices'].apply(ast.literal_eval)\n",
    "\n",
    "prompt = f\"\"\"Question: {train_df['question'][q_num]}\n",
    "    Choices:\n",
    "    (A) {train_df['choices'][q_num]['text'][0]}\n",
    "    (B) {train_df['choices'][q_num]['text'][1]}\n",
    "    (C) {train_df['choices'][q_num]['text'][2]}\n",
    "    (D) {train_df['choices'][q_num]['text'][3]}\n",
    "    (E) {train_df['choices'][q_num]['text'][4]}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "chat = [{\"role\":\"user\", \"content\":prompt}]\n",
    "\n",
    "answer_prefix = tokenizer.encode(tokenizer.apply_chat_template(chat, tokenize=False) + \" The answer is (B).\", return_tensors='pt', add_special_tokens=False)\n",
    "input_tensor = answer_prefix.repeat(n_samples, 1)\n",
    "\n",
    "outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 109])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_answer = outputs.sequences\n",
    "full_answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 71, 71]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_last_occurrence(tensor, target, offset=2, repeated=False):\n",
    "    mask = (tensor == target)\n",
    "    last_occurrence_indices = []\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        indices = torch.nonzero(mask[i], as_tuple=False).squeeze()\n",
    "        last_occurrence_index = -1\n",
    "        if indices.numel() > 0:\n",
    "            last_occurrence_index = indices[-1].item() + offset \n",
    "        last_occurrence_indices.append(last_occurrence_index)\n",
    "        if repeated:\n",
    "            return last_occurrence_indices * tensor.shape[0]\n",
    "    \n",
    "    return last_occurrence_indices\n",
    "\n",
    "find_last_occurrence(full_answer, 16289)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_before_last_occurrence(tensor, target, offset=2):\n",
    "    last_occurrence_indices = find_last_occurrence(tensor, target, offset, repeated=True)\n",
    "    \n",
    "    mask = torch.ones_like(tensor, dtype=torch.bool)\n",
    "    masked_tokens_count = []\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        if last_occurrence_indices[i] != -1:\n",
    "            mask[i, :last_occurrence_indices[i]] = 0\n",
    "            masked_count = last_occurrence_indices[i]\n",
    "        else:\n",
    "            masked_count = 0\n",
    "        masked_tokens_count.append(masked_count)\n",
    "    \n",
    "    masked_tensor = tensor * mask\n",
    "    \n",
    "    return masked_tensor, mask, masked_tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
       "            298,   576,   298,   970,   272,   905,   654, 28725,   304,   272,\n",
       "           1080,  3917,  1633,   354,   713,   298,  1300,   264, 15987,   302,\n",
       "            905,   682,   347,   297,  1852,  6432,  5020, 28723,     2],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
       "            298,   576,   298,   970,   272,   905,   654, 28725,   579,   272,\n",
       "           1489,  4782,   682,   347,  1852,  6432,  5020, 28723,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
       "            298,   576,   298,   970,   272,   905,   654, 28725,   579,   272,\n",
       "           1489,  4782,   682,   347,   325, 28760, 28731,  1852,  6432,  5020,\n",
       "          28723,     2,     2,     2,     2,     2,     2,     2,     2]],\n",
       "        device='cuda:0'),\n",
       " ['The answer is (B). Sammy wanted to go to where the people were, and the most likely place for him to find a concentration of people would be in populated areas.</s>',\n",
       "  'The answer is (B). Sammy wanted to go to where the people were, so the best choice would be populated areas.</s></s></s></s></s></s></s></s></s></s></s>',\n",
       "  'The answer is (B). Sammy wanted to go to where the people were, so the best choice would be (B) populated areas.</s></s></s></s></s></s></s></s>'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tensor, answer_mask, prefix_lengths = mask_before_last_occurrence(full_answer, 16289)\n",
    "answer_tensor, tokenizer.batch_decode(answer_tensor[:, prefix_lengths[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(answer_tensor, full_answer * answer_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0853e-03,  0.0000e+00, -1.0460e-01, -5.9605e-07, -2.1458e-06,\n",
       "         -3.2986e-03, -6.2643e-01, -3.0676e-03, -3.0994e-06, -3.6955e-06,\n",
       "         -2.9903e-03, -1.5433e+00, -5.1340e-02, -6.1138e-01, -1.4793e-01,\n",
       "         -5.2463e-02, -1.4274e-01, -7.6380e-01, -5.9841e-05, -7.1613e-02,\n",
       "         -1.5414e+00, -7.6814e-01, -4.7684e-07, -1.9431e-05, -6.6422e-02,\n",
       "         -3.5763e-07, -3.1085e-04, -7.8949e-02, -1.1921e-07, -1.1921e-07,\n",
       "         -2.1015e-02, -9.6928e-01],\n",
       "        [-2.0853e-03,  0.0000e+00, -1.0460e-01, -5.9605e-07, -2.1458e-06,\n",
       "         -3.2986e-03, -6.2643e-01, -3.0676e-03, -3.0994e-06, -3.6955e-06,\n",
       "         -2.9903e-03, -2.9332e-01, -3.0152e-01, -1.0032e+00, -3.9964e-01,\n",
       "         -1.0010e-01, -4.0173e-05, -2.2662e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -7.3189e-03, -3.7680e-02, -1.3303e-02, -8.2801e-03, -4.9709e-03,\n",
       "         -1.6271e-03, -7.7468e-04, -3.7925e-04, -1.7224e-04, -1.0430e-04,\n",
       "         -6.9973e-05, -6.5563e-05],\n",
       "        [-2.0853e-03,  0.0000e+00, -1.0460e-01, -5.9605e-07, -2.1458e-06,\n",
       "         -3.2986e-03, -6.2643e-01, -3.0676e-03, -3.0994e-06, -3.6955e-06,\n",
       "         -2.9903e-03, -2.9332e-01, -3.0152e-01, -1.0032e+00, -3.9964e-01,\n",
       "         -1.0010e-01, -4.0173e-05, -6.5901e-01, -1.9789e-05, -3.9864e-03,\n",
       "         -2.4914e-05,  0.0000e+00,  0.0000e+00, -7.0071e-04, -2.4670e-03,\n",
       "         -1.3061e-02, -1.2135e-02, -9.6293e-03, -2.6074e-03, -1.1124e-03,\n",
       "         -5.3618e-04, -2.4292e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_log_probs = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "instruct_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(full_answer, f\"tensors/full_answer_tensors/columbus-csqa_q{q_num}_a{answer_letter}_n{n_samples}_logprobs.pt\")\n",
    "torch.save(answer_mask, f\"tensors/answer_masks/columbus-csqa_q{q_num}_a{answer_letter}_n{n_samples}_answer_mask.pt\")\n",
    "torch.save(instruct_log_probs, f\"tensors/instruct_log_probs/columbus-csqa_q{q_num}_a{answer_letter}_n{n_samples}_instruct_logprobs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess Log Probs with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/vvajipey/.cache/huggingface/token\n",
      "Login successful\n",
      "Loading  mistralai/Mistral-7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a970d67fae054beeb7e74a0a3022e56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import get_close_matches\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"mistral-7b-v0.1\"\n",
    "\n",
    "name2base = {\"mistral-7b-v0.1\":\"mistralai/Mistral-7B-v0.1\"}\n",
    "\n",
    "base_model_name = name2base[model_name]\n",
    "print(\"Loading \", base_model_name)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "base_model.generation_config = GenerationConfig.from_pretrained(base_model_name)\n",
    "base_model.generation_config.pad_token_id =base_model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_num = 1\n",
    "n_samples = 3\n",
    "answer_letter = 'B'\n",
    "\n",
    "full_answer = torch.load(f\"tensors/full_answer_tensors/columbus-csqa_q{q_num}_a{answer_letter}_n{n_samples}_logprobs.pt\")\n",
    "answer_mask = torch.load(f\"tensors/answer_masks/columbus-csqa_q{q_num}_a{answer_letter}_n{n_samples}_answer_mask.pt\")\n",
    "instruct_log_probs = torch.load(f\"tensors/instruct_log_probs/columbus-csqa_q{q_num}_a{answer_letter}_n{n_samples}_instruct_logprobs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> The answer is (B). Sammy wanted to go to where the people were, and the most likely place for him to find a concentration of people would be in populated areas.</s>',\n",
       " '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> The answer is (B). Sammy wanted to go to where the people were, so the best choice would be populated areas.</s></s></s></s></s></s></s></s></s></s></s>',\n",
       " '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> The answer is (B). Sammy wanted to go to where the people were, so the best choice would be (B) populated areas.</s></s></s></s></s></s></s></s>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.batch_decode(full_answer * answer_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def sum_answer_logprobs(model, tokenizer, input_ids, answer_mask, batch_size=5, run_problem_name=\"nameless_logprobs\", print_logging=False):\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    answer_mask = answer_mask.to(model.device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_summed_logprobs = torch.tensor([]).to(model.device)\n",
    "    norm_total_summed_logprobs = torch.tensor([]).to(model.device)\n",
    "\n",
    "    if print_logging:\n",
    "        print(\"input_ids (rk_ai) shape: \", input_ids.shape)\n",
    "        print(\"input_ids (rk_ai): \", input_ids)\n",
    "\n",
    "    for i in range(0, input_ids.shape[0], batch_size):\n",
    "        start_row_idx = i\n",
    "        end_row_idx = min(i + batch_size, input_ids.shape[0])\n",
    "        batch_ids = input_ids[start_row_idx:end_row_idx]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_ids)\n",
    "        forward_pass_duration = time.time() - start_time\n",
    "        if print_logging:\n",
    "            print(\"Forward pass duration: \", forward_pass_duration)\n",
    "\n",
    "        logprobs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "        # Adjust indices to ignore the first token's log prob as it corresponds to the second token\n",
    "        logprobs = logprobs[:, :-1, :]\n",
    "        batch_ids  = batch_ids [:, 1:]\n",
    "\n",
    "        # get logprobs corresponding to specific input_id tokens (out of all vocab logprob distribution)\n",
    "        gen_logprobs = torch.gather(logprobs, 2, batch_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "        SAVE_LOGPROBS = False\n",
    "        if SAVE_LOGPROBS:\n",
    "            torch.save(gen_logprobs, f\"logprob_tensors/{run_problem_name}-logprobs.pt\")\n",
    "\n",
    "        masked_logprobs = gen_logprobs * answer_mask[start_row_idx:end_row_idx, 1:].float() # extract logprobs from answer tokens\n",
    "        if print_logging:\n",
    "            print(\"Answer Mask shape: \", answer_mask.shape)            \n",
    "            print(\"Answer Mask: \", answer_mask)\n",
    "\n",
    "        if print_logging:\n",
    "            print(\"Masked logprobs shape: \", masked_logprobs.shape)\n",
    "        nonzero_elements_count = (masked_logprobs != 0).sum(dim=1)\n",
    "        if print_logging:\n",
    "            print(masked_logprobs)\n",
    "            print(\"Nonzero elements count per row (a_i length): \", nonzero_elements_count)\n",
    "\n",
    "        batch_summed_logprobs = masked_logprobs.sum(dim=1)\n",
    "        if print_logging:\n",
    "            print(\"Batch summed logprobs: \", batch_summed_logprobs.shape, \"\\n\", batch_summed_logprobs)\n",
    "\n",
    "        total_summed_logprobs = torch.cat((total_summed_logprobs, batch_summed_logprobs), dim=0)\n",
    "        norm_total_summed_logprobs = torch.cat((norm_total_summed_logprobs, batch_summed_logprobs / nonzero_elements_count), dim=0)\n",
    "        \n",
    "        if print_logging:\n",
    "            print(\"summed_probs: \", batch_summed_logprobs.shape)\n",
    "\n",
    "            # batch = []\n",
    "            for input_sentence, input_probs in zip(batch_ids , masked_logprobs):\n",
    "            # for input_sentence, input_probs in zip(batch_ids , gen_logprobs): # check all logprobs\n",
    "                # text_sequence = []\n",
    "                for token, p in zip(input_sentence, input_probs):\n",
    "                    if token not in tokenizer.all_special_ids:\n",
    "                        # print((tokenizer.decode(token), p.item()))\n",
    "                        print(f\"{tokenizer.decode(token)} ({token}): {p.item()}\")\n",
    "                        # text_sequence.append((tokenizer.decode(token), p.item()))\n",
    "                # batch.append(text_sequence)\n",
    "\n",
    "    # print(\"TOTAL SUMMED LOGPROBS: \", total_summed_logprobs)\n",
    "\n",
    "    return total_summed_logprobs, norm_total_summed_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids (rk_ai) shape:  torch.Size([3, 109])\n",
      "input_ids (rk_ai):  tensor([[    1,   733, 16289, 28793, 22478, 28747,  4157,  1916,  2613,   298,\n",
      "           576,   298,   970,   272,   905,   654, 28723, 28705,  6926,  1659,\n",
      "           400,   576, 28804,    13,  2287, 25405,  1214, 28747,    13,  2287,\n",
      "           325, 28741, 28731,  5941,  3508,    13,  2287,   325, 28760, 28731,\n",
      "          1852,  6432,  5020,    13,  2287,   325, 28743, 28731,   272, 13453,\n",
      "            13,  2287,   325, 28757, 28731,  9585,    13,  2287,   325, 28749,\n",
      "         28731,  3878,  3356,    13,  2287, 26307, 28747,   733, 28748, 16289,\n",
      "         28793,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
      "           298,   576,   298,   970,   272,   905,   654, 28725,   304,   272,\n",
      "          1080,  3917,  1633,   354,   713,   298,  1300,   264, 15987,   302,\n",
      "           905,   682,   347,   297,  1852,  6432,  5020, 28723,     2],\n",
      "        [    1,   733, 16289, 28793, 22478, 28747,  4157,  1916,  2613,   298,\n",
      "           576,   298,   970,   272,   905,   654, 28723, 28705,  6926,  1659,\n",
      "           400,   576, 28804,    13,  2287, 25405,  1214, 28747,    13,  2287,\n",
      "           325, 28741, 28731,  5941,  3508,    13,  2287,   325, 28760, 28731,\n",
      "          1852,  6432,  5020,    13,  2287,   325, 28743, 28731,   272, 13453,\n",
      "            13,  2287,   325, 28757, 28731,  9585,    13,  2287,   325, 28749,\n",
      "         28731,  3878,  3356,    13,  2287, 26307, 28747,   733, 28748, 16289,\n",
      "         28793,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
      "           298,   576,   298,   970,   272,   905,   654, 28725,   579,   272,\n",
      "          1489,  4782,   682,   347,  1852,  6432,  5020, 28723,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    1,   733, 16289, 28793, 22478, 28747,  4157,  1916,  2613,   298,\n",
      "           576,   298,   970,   272,   905,   654, 28723, 28705,  6926,  1659,\n",
      "           400,   576, 28804,    13,  2287, 25405,  1214, 28747,    13,  2287,\n",
      "           325, 28741, 28731,  5941,  3508,    13,  2287,   325, 28760, 28731,\n",
      "          1852,  6432,  5020,    13,  2287,   325, 28743, 28731,   272, 13453,\n",
      "            13,  2287,   325, 28757, 28731,  9585,    13,  2287,   325, 28749,\n",
      "         28731,  3878,  3356,    13,  2287, 26307, 28747,   733, 28748, 16289,\n",
      "         28793,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
      "           298,   576,   298,   970,   272,   905,   654, 28725,   579,   272,\n",
      "          1489,  4782,   682,   347,   325, 28760, 28731,  1852,  6432,  5020,\n",
      "         28723,     2,     2,     2,     2,     2,     2,     2,     2]],\n",
      "       device='cuda:0')\n",
      "Forward pass duration:  0.4530041217803955\n",
      "Answer Mask shape:  torch.Size([3, 109])\n",
      "Answer Mask:  tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
      "       device='cuda:0')\n",
      "Masked logprobs shape:  torch.Size([3, 108])\n",
      "tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -5.4728e+00, -1.6213e+00, -1.8084e-01, -1.1185e+00, -1.3405e-01,\n",
      "         -9.0221e-01, -2.9251e+00, -5.5199e-03, -9.2033e-01, -1.7015e-02,\n",
      "         -9.7381e-02, -2.5143e-01, -1.6338e-01, -5.1914e-02, -1.5602e-02,\n",
      "         -3.3452e-02, -2.0387e+00, -1.2873e+00, -1.3322e+00, -3.0203e+00,\n",
      "         -1.8557e+00, -1.8372e-01, -1.3123e+00, -1.5729e+00, -6.9466e-02,\n",
      "         -4.9632e-01, -2.8537e+00, -6.0782e+00, -5.1736e-03, -7.6902e-02,\n",
      "         -9.2022e-01, -4.8787e-02, -5.3059e-01, -1.7076e+00, -1.1508e-03,\n",
      "         -1.3585e-02, -2.3183e-01, -4.1461e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -5.4728e+00, -1.6213e+00, -1.8084e-01, -1.1185e+00, -1.3405e-01,\n",
      "         -9.0221e-01, -2.9251e+00, -5.5199e-03, -9.2033e-01, -1.7015e-02,\n",
      "         -9.7381e-02, -2.5143e-01, -1.6338e-01, -5.1914e-02, -1.5602e-02,\n",
      "         -3.3452e-02, -2.0387e+00, -9.7480e-01, -1.9192e+00, -1.7657e+00,\n",
      "         -1.0493e+00, -1.7170e+00, -2.8353e-02, -1.9419e+00, -7.0154e-04,\n",
      "         -1.6166e-02, -1.3882e-01, -4.1233e+00, -5.8846e+00, -6.0276e+00,\n",
      "         -5.4454e+00, -5.3057e+00, -4.8668e+00, -4.5931e+00, -4.6649e+00,\n",
      "         -4.8398e+00, -5.0575e+00, -5.3827e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -5.4728e+00, -1.6213e+00, -1.8084e-01, -1.1185e+00, -1.3405e-01,\n",
      "         -9.0221e-01, -2.9251e+00, -5.5199e-03, -9.2033e-01, -1.7015e-02,\n",
      "         -9.7381e-02, -2.5143e-01, -1.6338e-01, -5.1914e-02, -1.5602e-02,\n",
      "         -3.3452e-02, -2.0387e+00, -9.7480e-01, -1.9192e+00, -1.7657e+00,\n",
      "         -1.0493e+00, -1.7170e+00, -2.8353e-02, -1.8794e+00, -3.8721e-02,\n",
      "         -4.5785e-01, -2.5230e-01, -6.0981e-04, -1.6495e-02, -8.1329e-02,\n",
      "         -3.7309e+00, -5.8599e+00, -6.0558e+00, -5.4424e+00, -5.3842e+00,\n",
      "         -4.9678e+00, -4.7196e+00, -4.8214e+00]], device='cuda:0')\n",
      "Nonzero elements count per row (a_i length):  tensor([38, 38, 38], device='cuda:0')\n",
      "Batch summed logprobs:  torch.Size([3]) \n",
      " tensor([-43.6933, -81.6929, -67.1124], device='cuda:0')\n",
      "summed_probs:  torch.Size([3])\n",
      "[ (733): -0.0\n",
      "INST (16289): -0.0\n",
      "] (28793): -0.0\n",
      "Question (22478): -0.0\n",
      ": (28747): -0.0\n",
      "Sam (4157): -0.0\n",
      "my (1916): -0.0\n",
      "wanted (2613): -0.0\n",
      "to (298): -0.0\n",
      "go (576): -0.0\n",
      "to (298): -0.0\n",
      "where (970): -0.0\n",
      "the (272): -0.0\n",
      "people (905): -0.0\n",
      "were (654): -0.0\n",
      ". (28723): -0.0\n",
      " (28705): -0.0\n",
      "Where (6926): -0.0\n",
      "might (1659): -0.0\n",
      "he (400): -0.0\n",
      "go (576): -0.0\n",
      "? (28804): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "Cho (25405): -0.0\n",
      "ices (1214): -0.0\n",
      ": (28747): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "A (28741): -0.0\n",
      ") (28731): -0.0\n",
      "race (5941): -0.0\n",
      "track (3508): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "B (28760): -0.0\n",
      ") (28731): -0.0\n",
      "pop (1852): -0.0\n",
      "ulated (6432): -0.0\n",
      "areas (5020): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "C (28743): -0.0\n",
      ") (28731): -0.0\n",
      "the (272): -0.0\n",
      "desert (13453): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "D (28757): -0.0\n",
      ") (28731): -0.0\n",
      "apartment (9585): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "E (28749): -0.0\n",
      ") (28731): -0.0\n",
      "road (3878): -0.0\n",
      "block (3356): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "Answer (26307): -0.0\n",
      ": (28747): -0.0\n",
      "[ (733): -0.0\n",
      "/ (28748): -0.0\n",
      "INST (16289): -0.0\n",
      "] (28793): -0.0\n",
      "The (415): -5.47276496887207\n",
      "answer (4372): -1.6212756633758545\n",
      "is (349): -0.18083862960338593\n",
      "( (325): -1.1185141801834106\n",
      "B (28760): -0.13404898345470428\n",
      "). (609): -0.9022071957588196\n",
      "Sam (4157): -2.925072193145752\n",
      "my (1916): -0.0055198632180690765\n",
      "wanted (2613): -0.920333981513977\n",
      "to (298): -0.017014604061841965\n",
      "go (576): -0.09738121181726456\n",
      "to (298): -0.25142863392829895\n",
      "where (970): -0.16337856650352478\n",
      "the (272): -0.051913969218730927\n",
      "people (905): -0.01560242474079132\n",
      "were (654): -0.03345245122909546\n",
      ", (28725): -2.038684606552124\n",
      "and (304): -1.2872986793518066\n",
      "the (272): -1.332227349281311\n",
      "most (1080): -3.020294427871704\n",
      "likely (3917): -1.8556941747665405\n",
      "place (1633): -0.18371786177158356\n",
      "for (354): -1.3122528791427612\n",
      "him (713): -1.572869062423706\n",
      "to (298): -0.0694664791226387\n",
      "find (1300): -0.4963184893131256\n",
      "a (264): -2.8536577224731445\n",
      "concentration (15987): -6.0781755447387695\n",
      "of (302): -0.005173628218472004\n",
      "people (905): -0.07690169662237167\n",
      "would (682): -0.920218825340271\n",
      "be (347): -0.048786938190460205\n",
      "in (297): -0.5305923223495483\n",
      "pop (1852): -1.70762300491333\n",
      "ulated (6432): -0.0011507801245898008\n",
      "areas (5020): -0.013585373759269714\n",
      ". (28723): -0.23183493316173553\n",
      "[ (733): -0.0\n",
      "INST (16289): -0.0\n",
      "] (28793): -0.0\n",
      "Question (22478): -0.0\n",
      ": (28747): -0.0\n",
      "Sam (4157): -0.0\n",
      "my (1916): -0.0\n",
      "wanted (2613): -0.0\n",
      "to (298): -0.0\n",
      "go (576): -0.0\n",
      "to (298): -0.0\n",
      "where (970): -0.0\n",
      "the (272): -0.0\n",
      "people (905): -0.0\n",
      "were (654): -0.0\n",
      ". (28723): -0.0\n",
      " (28705): -0.0\n",
      "Where (6926): -0.0\n",
      "might (1659): -0.0\n",
      "he (400): -0.0\n",
      "go (576): -0.0\n",
      "? (28804): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "Cho (25405): -0.0\n",
      "ices (1214): -0.0\n",
      ": (28747): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "A (28741): -0.0\n",
      ") (28731): -0.0\n",
      "race (5941): -0.0\n",
      "track (3508): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "B (28760): -0.0\n",
      ") (28731): -0.0\n",
      "pop (1852): -0.0\n",
      "ulated (6432): -0.0\n",
      "areas (5020): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "C (28743): -0.0\n",
      ") (28731): -0.0\n",
      "the (272): -0.0\n",
      "desert (13453): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "D (28757): -0.0\n",
      ") (28731): -0.0\n",
      "apartment (9585): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "E (28749): -0.0\n",
      ") (28731): -0.0\n",
      "road (3878): -0.0\n",
      "block (3356): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "Answer (26307): -0.0\n",
      ": (28747): -0.0\n",
      "[ (733): -0.0\n",
      "/ (28748): -0.0\n",
      "INST (16289): -0.0\n",
      "] (28793): -0.0\n",
      "The (415): -5.47276496887207\n",
      "answer (4372): -1.6212756633758545\n",
      "is (349): -0.18083862960338593\n",
      "( (325): -1.1185141801834106\n",
      "B (28760): -0.13404898345470428\n",
      "). (609): -0.9022071957588196\n",
      "Sam (4157): -2.925072193145752\n",
      "my (1916): -0.0055198632180690765\n",
      "wanted (2613): -0.920333981513977\n",
      "to (298): -0.017014604061841965\n",
      "go (576): -0.09738121181726456\n",
      "to (298): -0.25142863392829895\n",
      "where (970): -0.16337856650352478\n",
      "the (272): -0.051913969218730927\n",
      "people (905): -0.01560242474079132\n",
      "were (654): -0.03345245122909546\n",
      ", (28725): -2.038684606552124\n",
      "so (579): -0.9747986793518066\n",
      "the (272): -1.919182538986206\n",
      "best (1489): -1.765682339668274\n",
      "choice (4782): -1.0493028163909912\n",
      "would (682): -1.7170119285583496\n",
      "be (347): -0.02835310809314251\n",
      "pop (1852): -1.9418944120407104\n",
      "ulated (6432): -0.0007015389273874462\n",
      "areas (5020): -0.016166316345334053\n",
      ". (28723): -0.13881775736808777\n",
      "[ (733): -0.0\n",
      "INST (16289): -0.0\n",
      "] (28793): -0.0\n",
      "Question (22478): -0.0\n",
      ": (28747): -0.0\n",
      "Sam (4157): -0.0\n",
      "my (1916): -0.0\n",
      "wanted (2613): -0.0\n",
      "to (298): -0.0\n",
      "go (576): -0.0\n",
      "to (298): -0.0\n",
      "where (970): -0.0\n",
      "the (272): -0.0\n",
      "people (905): -0.0\n",
      "were (654): -0.0\n",
      ". (28723): -0.0\n",
      " (28705): -0.0\n",
      "Where (6926): -0.0\n",
      "might (1659): -0.0\n",
      "he (400): -0.0\n",
      "go (576): -0.0\n",
      "? (28804): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "Cho (25405): -0.0\n",
      "ices (1214): -0.0\n",
      ": (28747): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "A (28741): -0.0\n",
      ") (28731): -0.0\n",
      "race (5941): -0.0\n",
      "track (3508): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "B (28760): -0.0\n",
      ") (28731): -0.0\n",
      "pop (1852): -0.0\n",
      "ulated (6432): -0.0\n",
      "areas (5020): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "C (28743): -0.0\n",
      ") (28731): -0.0\n",
      "the (272): -0.0\n",
      "desert (13453): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "D (28757): -0.0\n",
      ") (28731): -0.0\n",
      "apartment (9585): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "( (325): -0.0\n",
      "E (28749): -0.0\n",
      ") (28731): -0.0\n",
      "road (3878): -0.0\n",
      "block (3356): -0.0\n",
      "\n",
      " (13): -0.0\n",
      "   (2287): -0.0\n",
      "Answer (26307): -0.0\n",
      ": (28747): -0.0\n",
      "[ (733): -0.0\n",
      "/ (28748): -0.0\n",
      "INST (16289): -0.0\n",
      "] (28793): -0.0\n",
      "The (415): -5.47276496887207\n",
      "answer (4372): -1.6212756633758545\n",
      "is (349): -0.18083862960338593\n",
      "( (325): -1.1185141801834106\n",
      "B (28760): -0.13404898345470428\n",
      "). (609): -0.9022071957588196\n",
      "Sam (4157): -2.925072193145752\n",
      "my (1916): -0.0055198632180690765\n",
      "wanted (2613): -0.920333981513977\n",
      "to (298): -0.017014604061841965\n",
      "go (576): -0.09738121181726456\n",
      "to (298): -0.25142863392829895\n",
      "where (970): -0.16337856650352478\n",
      "the (272): -0.051913969218730927\n",
      "people (905): -0.01560242474079132\n",
      "were (654): -0.03345245122909546\n",
      ", (28725): -2.038684606552124\n",
      "so (579): -0.9747986793518066\n",
      "the (272): -1.919182538986206\n",
      "best (1489): -1.765682339668274\n",
      "choice (4782): -1.0493028163909912\n",
      "would (682): -1.7170119285583496\n",
      "be (347): -0.02835310809314251\n",
      "( (325): -1.8793944120407104\n",
      "B (28760): -0.03872136399149895\n",
      ") (28731): -0.45784956216812134\n",
      "pop (1852): -0.2523024082183838\n",
      "ulated (6432): -0.0006098079611547291\n",
      "areas (5020): -0.016494929790496826\n",
      ". (28723): -0.08132926374673843\n"
     ]
    }
   ],
   "source": [
    "total_summed_logprobs, norm_total_summed_logprobs = sum_answer_logprobs(base_model, base_tokenizer, full_answer, answer_mask, batch_size=3, print_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-43.6933, -81.6929, -67.1124], device='cuda:0'),\n",
       " tensor([-1.1498, -2.1498, -1.7661], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_summed_logprobs, norm_total_summed_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file_path = f\"logs/columbus_base_logprobs_{q_num}.csv\"\n",
    "file_exists = os.path.isfile(csv_file_path)\n",
    "\n",
    "with open(csv_file_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    if not file_exists:\n",
    "        writer.writerow([\"question_number\", \"answer_letter\", \"total_summed_logprobs\", \"norm_total_summed_logprobs\"])\n",
    "    for total_logprob, norm_logprob in zip(total_summed_logprobs.tolist(), norm_total_summed_logprobs.tolist()):\n",
    "        writer.writerow([q_num, answer_letter, total_logprob, norm_logprob])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reason",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
