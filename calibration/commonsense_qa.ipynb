{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/vvajipey/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/vvajipey/miniconda3/envs/reason/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18dfb4e6fe6d4319a66fa2bcfa38478e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/vvajipey/miniconda3/envs/reason/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import get_close_matches\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "gsm_df = pd.read_csv('../distribution/data/gsm8kTest.csv')\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_concept</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>075e483d21c29a511267ef62bedc0461</td>\n",
       "      <td>The sanctions against the school were a punish...</td>\n",
       "      <td>punishing</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61fe6e879ff18686d7552425a36344c8</td>\n",
       "      <td>Sammy wanted to go to where the people were.  ...</td>\n",
       "      <td>people</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c1cb0e95b99f72d55c068ba0255c54d</td>\n",
       "      <td>To locate a choker not located in a jewelry bo...</td>\n",
       "      <td>choker</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02e821a3e53cb320790950aab4489e85</td>\n",
       "      <td>Google Maps and other highway and street GPS s...</td>\n",
       "      <td>highway</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23505889b94e880c3e89cff4ba119860</td>\n",
       "      <td>The fox walked from the city into the forest, ...</td>\n",
       "      <td>fox</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>f1b2a30a1facff543e055231c5f90dd0</td>\n",
       "      <td>What would someone need to do if he or she wan...</td>\n",
       "      <td>going public</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>a63b4d0c0b34d6e5f5ce7b2c2c08b825</td>\n",
       "      <td>Where might you find a chair at an office?</td>\n",
       "      <td>chair</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>22d0eea15e10be56024fd00bb0e4f72f</td>\n",
       "      <td>Where would you buy jeans in a place with a la...</td>\n",
       "      <td>jeans</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>7c55160a4630de9690eb328b57a18dc2</td>\n",
       "      <td>John fell down the well.  he couldn't believe ...</td>\n",
       "      <td>well</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>dd640927f9920930501fb8dc3efc196b</td>\n",
       "      <td>I forgot to pay the electricity bill, now what...</td>\n",
       "      <td>electricity</td>\n",
       "      <td>{'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9741 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0     075e483d21c29a511267ef62bedc0461   \n",
       "1     61fe6e879ff18686d7552425a36344c8   \n",
       "2     4c1cb0e95b99f72d55c068ba0255c54d   \n",
       "3     02e821a3e53cb320790950aab4489e85   \n",
       "4     23505889b94e880c3e89cff4ba119860   \n",
       "...                                ...   \n",
       "9736  f1b2a30a1facff543e055231c5f90dd0   \n",
       "9737  a63b4d0c0b34d6e5f5ce7b2c2c08b825   \n",
       "9738  22d0eea15e10be56024fd00bb0e4f72f   \n",
       "9739  7c55160a4630de9690eb328b57a18dc2   \n",
       "9740  dd640927f9920930501fb8dc3efc196b   \n",
       "\n",
       "                                               question question_concept  \\\n",
       "0     The sanctions against the school were a punish...        punishing   \n",
       "1     Sammy wanted to go to where the people were.  ...           people   \n",
       "2     To locate a choker not located in a jewelry bo...           choker   \n",
       "3     Google Maps and other highway and street GPS s...          highway   \n",
       "4     The fox walked from the city into the forest, ...              fox   \n",
       "...                                                 ...              ...   \n",
       "9736  What would someone need to do if he or she wan...     going public   \n",
       "9737         Where might you find a chair at an office?            chair   \n",
       "9738  Where would you buy jeans in a place with a la...            jeans   \n",
       "9739  John fell down the well.  he couldn't believe ...             well   \n",
       "9740  I forgot to pay the electricity bill, now what...      electricity   \n",
       "\n",
       "                                                choices answerKey  \n",
       "0     {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         A  \n",
       "1     {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         B  \n",
       "2     {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         A  \n",
       "3     {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         D  \n",
       "4     {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         C  \n",
       "...                                                 ...       ...  \n",
       "9736  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         E  \n",
       "9737  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         D  \n",
       "9738  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         A  \n",
       "9739  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         A  \n",
       "9740  {'label': ['A', 'B', 'C', 'D', 'E'], 'text': [...         C  \n",
       "\n",
       "[9741 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "train_df = pd.read_csv(\"data/commonsense_qa_train.csv\")\n",
    "train_df['choices'] = train_df['choices'].apply(ast.literal_eval)\n",
    "train_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_completions_from_model(question, choices, tokenizer, n_samples=1):\n",
    "    # https://arxiv.org/html/2402.17302v1\n",
    "    \n",
    "    prompt = f\"\"\"Question: {question}\n",
    "    Choices:\n",
    "    A. {choices[0]}\n",
    "    B. {choices[1]}\n",
    "    C. {choices[2]}\n",
    "    D. {choices[3]}\n",
    "    E. {choices[4]}\n",
    "    Answer:\"\"\"\n",
    "    chat = [{\"role\":\"user\", \"content\":prompt}]\n",
    "\n",
    "    input_tensor = tokenizer.apply_chat_template(chat)\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        gen_outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        ).sequences\n",
    "    return new_paths, path_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race track', 'populated areas', 'the desert', 'apartment', 'roadblock']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['choices'][1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer:'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"Question: {train_df['question'][1]}\n",
    "    Choices:\n",
    "    (A) {train_df['choices'][1]['text'][0]}\n",
    "    (B) {train_df['choices'][1]['text'][1]}\n",
    "    (C) {train_df['choices'][1]['text'][2]}\n",
    "    (D) {train_df['choices'][1]['text'][3]}\n",
    "    (E) {train_df['choices'][1]['text'][4]}\n",
    "    Answer:\"\"\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer: [/INST]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [{\"role\":\"user\", \"content\":prompt}]\n",
    "\n",
    "input_tensor = tokenizer.apply_chat_template(chat, return_tensors='pt')\n",
    "tokenizer.apply_chat_template(chat, tokenize=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> [INST] Question: Sammy wanted to go to where the people were.  Where '\n",
      " 'might he go?\\n'\n",
      " '    Choices:\\n'\n",
      " '    (A) race track\\n'\n",
      " '    (B) populated areas\\n'\n",
      " '    (C) the desert\\n'\n",
      " '    (D) apartment\\n'\n",
      " '    (E) roadblock\\n'\n",
      " '    Answer: [/INST] Sammy might go to where the people are, so the correct '\n",
      " 'choice would be (B) populated areas.</s>']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(gen_outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2758e-01,  0.0000e+00, -3.8946e-01, -1.9602e-02, -5.8768e-05,\n",
       "         -3.8944e+00, -1.0339e-01, -2.3374e-04, -1.1094e-01, -9.0137e-02,\n",
       "         -3.1514e-01, -1.1663e-01, -1.7940e+00, -1.5629e-01, -2.5322e-01,\n",
       "         -1.7047e-05, -3.9002e-03, -1.9322e-03, -1.1921e-07, -5.1259e-05,\n",
       "          0.0000e+00,  0.0000e+00, -1.9518e-03, -2.8916e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores = model.compute_transition_scores(\n",
    "    gen_outputs.sequences, gen_outputs.scores, normalize_logits=True\n",
    ")\n",
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length = input_tensor.shape[1]\n",
    "input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  4157 | Sam      | -0.1276 | 88.02%\n",
      "|  1916 | my       | 0.0000 | 100.00%\n",
      "|  1659 | might    | -0.3895 | 67.74%\n",
      "|   576 | go       | -0.0196 | 98.06%\n",
      "|   298 | to       | -0.0001 | 99.99%\n",
      "|   970 | where    | -3.8944 | 2.04%\n",
      "|   272 | the      | -0.1034 | 90.18%\n",
      "|   905 | people   | -0.0002 | 99.98%\n",
      "|   460 | are      | -0.1109 | 89.50%\n",
      "| 28725 | ,        | -0.0901 | 91.38%\n",
      "|   579 | so       | -0.3151 | 72.97%\n",
      "|   272 | the      | -0.1166 | 88.99%\n",
      "|  4714 | correct  | -1.7940 | 16.63%\n",
      "|  4782 | choice   | -0.1563 | 85.53%\n",
      "|   682 | would    | -0.2532 | 77.63%\n",
      "|   347 | be       | -0.0000 | 100.00%\n",
      "|   325 | (        | -0.0039 | 99.61%\n",
      "| 28760 | B        | -0.0019 | 99.81%\n",
      "| 28731 | )        | -0.0000 | 100.00%\n",
      "|  1852 | pop      | -0.0001 | 99.99%\n",
      "|  6432 | ulated   | 0.0000 | 100.00%\n",
      "|  5020 | areas    | 0.0000 | 100.00%\n",
      "| 28723 | .        | -0.0020 | 99.81%\n",
      "|     2 | </s>     | -0.0003 | 99.97%\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = gen_outputs.sequences[:, input_length:]\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.4f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 = '<s>'\n",
      "325 = '('\n",
      "28741 = 'A'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28760 = 'B'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28743 = 'C'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28757 = 'D'\n",
      "28731 = ')'\n",
      "10653 = '_('\n",
      "28749 = 'E'\n",
      "28731 = ')'\n",
      "13 = '\n",
      "'\n",
      "330 = 'A'\n",
      "330 = 'A'\n",
      "13 = '\n",
      "'\n",
      "1094 = 'An'\n",
      "26307 = 'Answer'\n",
      "264 = 'a'\n",
      "365 = 'B'\n",
      "334 = 'C'\n",
      "384 = 'D'\n",
      "413 = 'E'\n"
     ]
    }
   ],
   "source": [
    "letters = tokenizer.encode(\"(A)_(B)_(C)_(D)_(E)\\n A A\\n An Answer a B C D E\")\n",
    "for tok in letters:\n",
    "    print(f\"{tok} = '{tokenizer.decode(tok)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0, 28760,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0, 28760,     0,\n",
       "             0,     0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = gen_outputs.sequences\n",
    "\n",
    "answer_to_tok = {'A':[28741, 330], 'B':[28760, 365], 'C':[28743, 334], 'D':[28757, 384], 'E':[28749, 413]}\n",
    "\n",
    "masked_output_tensor = torch.where(\n",
    "    torch.isin(output_tensor, torch.tensor(answer_to_tok['B'], device=output_tensor.device)),\n",
    "    output_tensor,\n",
    "    torch.tensor(0, device=output_tensor.device)\n",
    ")\n",
    "masked_output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     0 | <unk>    | -0.1276 | 88.02%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.3895 | 67.74%\n",
      "|     0 | <unk>    | -0.0196 | 98.06%\n",
      "|     0 | <unk>    | -0.0001 | 99.99%\n",
      "|     0 | <unk>    | -3.8944 | 2.04%\n",
      "|     0 | <unk>    | -0.1034 | 90.18%\n",
      "|     0 | <unk>    | -0.0002 | 99.98%\n",
      "|     0 | <unk>    | -0.1109 | 89.50%\n",
      "|     0 | <unk>    | -0.0901 | 91.38%\n",
      "|     0 | <unk>    | -0.3151 | 72.97%\n",
      "|     0 | <unk>    | -0.1166 | 88.99%\n",
      "|     0 | <unk>    | -1.7940 | 16.63%\n",
      "|     0 | <unk>    | -0.1563 | 85.53%\n",
      "|     0 | <unk>    | -0.2532 | 77.63%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0039 | 99.61%\n",
      "| 28760 | B        | -0.0019 | 99.81%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0001 | 99.99%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0020 | 99.81%\n",
      "|     0 | <unk>    | -0.0003 | 99.97%\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = masked_output_tensor[:, input_length:]\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.4f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28760, 28760], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zero_values = masked_output_tensor[masked_output_tensor != 0]\n",
    "non_zero_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 28760,     0,     0,\n",
       "             0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_mask = torch.isin(generated_tokens, torch.tensor(answer_to_tok['B'], device=output_tensor.device)).int()\n",
    "masked_generated_tokens = generated_tokens * answer_mask\n",
    "masked_generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "| 28760 | B        | -0.0019 | 99.81%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | 0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n",
      "|     0 | <unk>    | -0.0000 | 100.00%\n"
     ]
    }
   ],
   "source": [
    "masked_log_probs = transition_scores * answer_mask\n",
    "for tok, score in zip(masked_generated_tokens[0], masked_log_probs[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.4f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[28741, 330]\n",
      "['A', 'A']\n",
      "B\n",
      "[28760, 365]\n",
      "['B', 'B']\n",
      "C\n",
      "[28743, 334]\n",
      "['C', 'C']\n",
      "D\n",
      "[28757, 384]\n",
      "['D', 'D']\n",
      "E\n",
      "[28749, 413]\n",
      "['E', 'E']\n"
     ]
    }
   ],
   "source": [
    "for c in \"ABCDE\":\n",
    "    print(c)\n",
    "    print(answer_to_tok[c])\n",
    "    print(tokenizer.batch_decode(answer_to_tok[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_tensor = tokenizer.batch_decode(tokenizer.encode(tokenizer.apply_chat_template(chat, tokenize=False) + \" The answer is (B).\", return_tensors='pt', add_special_tokens=False))\n",
    "b_tensor = tokenizer.encode(tokenizer.apply_chat_template(chat, tokenize=False) + \" The answer is (B).\", return_tensors='pt', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_outputs = model.generate(\n",
    "            b_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> [INST] Question: Sammy wanted to go to where the people were.  Where might he go?\\n    Choices:\\n    (A) race track\\n    (B) populated areas\\n    (C) the desert\\n    (D) apartment\\n    (E) roadblock\\n    Answer: [/INST] The answer is (B). Sammy wanted to go to where the people were, and the best option among the given choices is the populated areas, where he is likely to find a large number of people.</s>']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(b_outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_output_tensor = b_outputs.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sammy wanted to go to where the people were, and the best option among the given choices is the populated areas, where he is likely to find a large number of people.</s>']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(b_output_tensor[:, b_tensor.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Question: {train_df['question'][1]}\n",
    "    Choices:\n",
    "    (A) {train_df['choices'][1]['text'][0]}\n",
    "    (B) {train_df['choices'][1]['text'][1]}\n",
    "    (C) {train_df['choices'][1]['text'][2]}\n",
    "    (D) {train_df['choices'][1]['text'][3]}\n",
    "    (E) {train_df['choices'][1]['text'][4]}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "answer_prefix = tokenizer.encode(tokenizer.apply_chat_template(chat, tokenize=False) + \" The answer is (B).\", return_tensors='pt', add_special_tokens=False)\n",
    "\n",
    "outputs = model.generate(\n",
    "            answer_prefix.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793, 22478, 28747,  4157,  1916,  2613,   298,\n",
       "           576,   298,   970,   272,   905,   654, 28723, 28705,  6926,  1659,\n",
       "           400,   576, 28804,    13,  2287, 25405,  1214, 28747,    13,  2287,\n",
       "           325, 28741, 28731,  5941,  3508,    13,  2287,   325, 28760, 28731,\n",
       "          1852,  6432,  5020,    13,  2287,   325, 28743, 28731,   272, 13453,\n",
       "            13,  2287,   325, 28757, 28731,  9585,    13,  2287,   325, 28749,\n",
       "         28731,  3878,  3356,    13,  2287, 26307, 28747,   733, 28748, 16289,\n",
       "         28793,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
       "           298,   576,   298,   970,   272,   905,   654, 28725,   579,   272,\n",
       "          1489,  4782,   682,   347,   298,   576,   298,  1852,  6432,  5020,\n",
       "         28723,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_answer = outputs.sequences\n",
    "full_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INST'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INST is 16289\n",
    "tokenizer.decode(16289)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([71])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_last_occurrence(tensor, target, offset=2):\n",
    "    mask = (tensor == target)\n",
    "    last_occurrence_indices = torch.full((tensor.shape[0],), -1, dtype=torch.long)\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        indices = torch.nonzero(mask[i], as_tuple=False).squeeze()\n",
    "        if indices.numel() > 0:\n",
    "            last_occurrence_indices[i] = indices[-1].item() + offset\n",
    "    \n",
    "    return last_occurrence_indices\n",
    "\n",
    "find_last_occurrence(full_answer, 16289)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_before_last_occurrence(tensor, target, offset=2):\n",
    "    # Find the last occurrence indices\n",
    "    last_occurrence_indices = find_last_occurrence(tensor, target, offset)\n",
    "    \n",
    "    # Create a mask tensor filled with ones\n",
    "    mask = torch.ones_like(tensor, dtype=torch.bool)\n",
    "    \n",
    "    # Initialize a list to store the count of masked tokens for each row\n",
    "    masked_tokens_count = []\n",
    "    \n",
    "    # Apply the mask for each row based on the last occurrence index\n",
    "    for i in range(tensor.shape[0]):\n",
    "        if last_occurrence_indices[i] != -1:  # Check if the target is present in the row\n",
    "            mask[i, :last_occurrence_indices[i]] = 0  # Mask out all values before the last occurrence\n",
    "            masked_count = last_occurrence_indices[i]  # Count of masked tokens\n",
    "        else:\n",
    "            masked_count = 0  # No tokens masked if the target is not found in the row\n",
    "        masked_tokens_count.append(masked_count)\n",
    "    \n",
    "    # Apply the mask to the original tensor\n",
    "    masked_tensor = tensor * mask\n",
    "    \n",
    "    return masked_tensor, masked_tokens_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,   415,  4372,   349,   325, 28760,   609,  4157,  1916,  2613,\n",
       "            298,   576,   298,   970,   272,   905,   654, 28725,   579,   272,\n",
       "           1489,  4782,   682,   347,   298,   576,   298,  1852,  6432,  5020,\n",
       "          28723,     2]], device='cuda:0'),\n",
       " ['The answer is (B). Sammy wanted to go to where the people were, so the best choice would be to go to populated areas.</s>'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tensor, prefix_lengths = mask_before_last_occurrence(full_answer, 16289)\n",
    "answer_tensor, tokenizer.batch_decode(answer_tensor[:, prefix_lengths[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_answer.shape[1] - answer_prefix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0739e-03,  0.0000e+00, -1.0460e-01, -8.3446e-07, -2.1458e-06,\n",
       "         -3.2990e-03, -5.6334e-01, -3.6283e-03, -3.0994e-06, -3.0994e-06,\n",
       "         -3.0695e-03, -2.8968e-01, -2.5802e-01, -1.0032e+00, -3.4426e-01,\n",
       "         -1.0220e-01, -4.0292e-05, -2.3906e+00, -2.9752e-01, -1.7881e-06,\n",
       "         -6.6615e-02,  0.0000e+00,  0.0000e+00, -3.0647e-02, -5.6161e-02]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_log_probs = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "instruct_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reason",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
