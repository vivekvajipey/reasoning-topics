{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekvajipey/miniconda3/envs/nightly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.47s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# model_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "NUM_COT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_and_scores(model, tokenizer, question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question + \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"}\n",
    "    ]\n",
    "    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_tensor.to(model.device), max_new_tokens=1000, return_dict_in_generate=True, output_scores=True)\n",
    "    transition_scores = model.compute_transition_scores(\n",
    "        outputs.sequences, outputs.scores, normalize_logits=True\n",
    "    )\n",
    "    print(outputs.sequences)\n",
    "\n",
    "    input_length = input_tensor.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    print(generated_tokens)\n",
    "    for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "        # | token | token string | log probability | probability\n",
    "        print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy()} | {np.exp(score.cpu().numpy()):.2%}\")\n",
    "\n",
    "    return outputs.sequences, generated_tokens[0], transition_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens_and_logprobs(model, tokenizer, input_ids):\n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "    # Adjust indices to ignore the first token's log prob as it corresponds to the second token\n",
    "    probs = probs[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    batch = []\n",
    "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
    "        text_sequence = []\n",
    "        for token, p in zip(input_sentence, input_probs):\n",
    "            if token not in tokenizer.all_special_ids:\n",
    "                text_sequence.append((tokenizer.decode(token), p.item()))\n",
    "        batch.append(text_sequence)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   733, 16289, 28793,  1824,   349, 28705, 28774,   648, 28705,\n",
      "         28740, 28734,   398, 28705, 28750, 28804,    13, 12069,  2611,  3707,\n",
      "           486,  3707, 28725,   304,  1658,   574,  1480,  4372,  2373,   414,\n",
      "          2858,   286, 28751,  2051,   733, 28748, 16289, 28793,  1791, 12049,\n",
      "           272,  5782, 28705, 28774,   648, 28705, 28740, 28734,   398, 28705,\n",
      "         28750, 28725,   478,   927,   298,  1372,   272,  1745,   302,  6933,\n",
      "         28725,   690,   349,  2608, 10216,   486,   272,  1183,  1689,  1082,\n",
      "         21025,  4915,  2109, 28747, 18712,  2053,   274, 28725,  1529,  6445,\n",
      "         28725, 18317,  2459,   304,  8618,   325,  3211,  1749,   298,  1103,\n",
      "           557,   304,  3301,   685,   304,  5078,   434,  1774,   325,  3211,\n",
      "          1749,   298,  1103,   609,    13,    13,   657,   456,  1222, 28725,\n",
      "           736,   460,   708,  2564,  2053,   274,   442,   439,  6445, 28725,\n",
      "           579,   478,  2318,   356,   298,   272,  1679,  3707, 28723,    13,\n",
      "            13,  9977, 28705, 28740, 28747,  2744,   674,   272,  6079,  2459,\n",
      "            13, 28740, 28734,   398, 28705, 28750,   327, 28705, 28750, 28734,\n",
      "            13,    13,  9977, 28705, 28750, 28747,  2744,   674,   272,  4518,\n",
      "            13, 28774,   648, 28705, 28750, 28734,   327, 28705, 28750, 28774,\n",
      "            13,    13,  5142, 28725,   272,  1480,  4372,   349,   414,  2858,\n",
      "           286, 28751, 28750, 28774,  2051,     2]], device='mps:0')\n",
      "tensor([[ 1791, 12049,   272,  5782, 28705, 28774,   648, 28705, 28740, 28734,\n",
      "           398, 28705, 28750, 28725,   478,   927,   298,  1372,   272,  1745,\n",
      "           302,  6933, 28725,   690,   349,  2608, 10216,   486,   272,  1183,\n",
      "          1689,  1082, 21025,  4915,  2109, 28747, 18712,  2053,   274, 28725,\n",
      "          1529,  6445, 28725, 18317,  2459,   304,  8618,   325,  3211,  1749,\n",
      "           298,  1103,   557,   304,  3301,   685,   304,  5078,   434,  1774,\n",
      "           325,  3211,  1749,   298,  1103,   609,    13,    13,   657,   456,\n",
      "          1222, 28725,   736,   460,   708,  2564,  2053,   274,   442,   439,\n",
      "          6445, 28725,   579,   478,  2318,   356,   298,   272,  1679,  3707,\n",
      "         28723,    13,    13,  9977, 28705, 28740, 28747,  2744,   674,   272,\n",
      "          6079,  2459,    13, 28740, 28734,   398, 28705, 28750,   327, 28705,\n",
      "         28750, 28734,    13,    13,  9977, 28705, 28750, 28747,  2744,   674,\n",
      "           272,  4518,    13, 28774,   648, 28705, 28750, 28734,   327, 28705,\n",
      "         28750, 28774,    13,    13,  5142, 28725,   272,  1480,  4372,   349,\n",
      "           414,  2858,   286, 28751, 28750, 28774,  2051,     2]],\n",
      "       device='mps:0')\n",
      "|  1791 | To       | -0.13854679465293884 | 87.06%\n",
      "| 12049 | solve    | -0.02403736300766468 | 97.62%\n",
      "|   272 | the      | -0.16032463312149048 | 85.19%\n",
      "|  5782 | expression | -0.0005321278586052358 | 99.95%\n",
      "| 28705 |          | -0.006042072083801031 | 99.40%\n",
      "| 28774 | 9        | -0.00033087024348787963 | 99.97%\n",
      "|   648 | +        | -3.0636318115284666e-05 | 100.00%\n",
      "| 28705 |          | -2.3841855067985307e-07 | 100.00%\n",
      "| 28740 | 1        | -1.1920928244535389e-07 | 100.00%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|   398 | *        | -4.1960789531003684e-05 | 100.00%\n",
      "| 28705 |          | 0.0 | 100.00%\n",
      "| 28750 | 2        | -5.960462772236497e-07 | 100.00%\n",
      "| 28725 | ,        | -0.0024573388509452343 | 99.75%\n",
      "|   478 | we       | -0.019182177260518074 | 98.10%\n",
      "|   927 | need     | -0.0333944633603096 | 96.72%\n",
      "|   298 | to       | -1.4305104514278355e-06 | 100.00%\n",
      "|  1372 | follow   | -0.008089871145784855 | 99.19%\n",
      "|   272 | the      | -1.3351351299206726e-05 | 100.00%\n",
      "|  1745 | order    | -0.004840560257434845 | 99.52%\n",
      "|   302 | of       | -3.576278118089249e-07 | 100.00%\n",
      "|  6933 | operations | -0.000809818331617862 | 99.92%\n",
      "| 28725 | ,        | -0.05047057941555977 | 95.08%\n",
      "|   690 | which    | -0.3874920606613159 | 67.88%\n",
      "|   349 | is       | -0.00029583368450403214 | 99.97%\n",
      "|  2608 | often    | -0.5905787348747253 | 55.40%\n",
      "| 10216 | remembered | -0.031841397285461426 | 96.87%\n",
      "|   486 | by       | -0.014357476495206356 | 98.57%\n",
      "|   272 | the      | -3.957670196541585e-05 | 100.00%\n",
      "|  1183 | ac       | -0.0006562701310031116 | 99.93%\n",
      "|  1689 | ron      | -5.245195097813848e-06 | 100.00%\n",
      "|  1082 | ym       | -0.00020168177434243262 | 99.98%\n",
      "| 21025 | PE       | -6.0437283536884934e-05 | 99.99%\n",
      "|  4915 | MD       | -0.0001408954558428377 | 99.99%\n",
      "|  2109 | AS       | -5.483612312673358e-06 | 100.00%\n",
      "| 28747 | :        | -0.016775256022810936 | 98.34%\n",
      "| 18712 | Parent   | -0.0033274304587394 | 99.67%\n",
      "|  2053 | hes      | -8.594620157964528e-05 | 99.99%\n",
      "|   274 | es       | -1.1920928244535389e-07 | 100.00%\n",
      "| 28725 | ,        | -0.0005404680850915611 | 99.95%\n",
      "|  1529 | Ex       | -0.0003573255962692201 | 99.96%\n",
      "|  6445 | ponents  | -0.00024029705673456192 | 99.98%\n",
      "| 28725 | ,        | -6.711257447022945e-05 | 99.99%\n",
      "| 18317 | Multi    | -0.0013487775577232242 | 99.87%\n",
      "|  2459 | plication | -5.5549986427649856e-05 | 99.99%\n",
      "|   304 | and      | -0.00012981049076188356 | 99.99%\n",
      "|  8618 | Division | -0.0016381428577005863 | 99.84%\n",
      "|   325 | (        | -0.0004058252670802176 | 99.96%\n",
      "|  3211 | from     | -0.002120153047144413 | 99.79%\n",
      "|  1749 | left     | -0.0003163314249832183 | 99.97%\n",
      "|   298 | to       | -4.291525328881107e-06 | 100.00%\n",
      "|  1103 | right    | -1.3351351299206726e-05 | 100.00%\n",
      "|   557 | ),       | -5.364403477869928e-06 | 100.00%\n",
      "|   304 | and      | -0.5764424204826355 | 56.19%\n",
      "|  3301 | Add      | -0.004477238282561302 | 99.55%\n",
      "|   685 | ition    | -9.417489309271332e-06 | 100.00%\n",
      "|   304 | and      | -8.344646857949556e-07 | 100.00%\n",
      "|  5078 | Sub      | -9.238292841473594e-05 | 99.99%\n",
      "|   434 | tr       | -6.97350042173639e-05 | 99.99%\n",
      "|  1774 | action   | -1.4066597032069694e-05 | 100.00%\n",
      "|   325 | (        | -1.7523612768854946e-05 | 100.00%\n",
      "|  3211 | from     | -0.00010966652916977182 | 99.99%\n",
      "|  1749 | left     | -5.757642793469131e-05 | 99.99%\n",
      "|   298 | to       | -2.145764938177308e-06 | 100.00%\n",
      "|  1103 | right    | -0.00022075122979003936 | 99.98%\n",
      "|   609 | ).       | -8.713819261174649e-05 | 99.99%\n",
      "|    13 | \n",
      "        | -0.2187599241733551 | 80.35%\n",
      "|    13 | \n",
      "        | -1.0132738680113107e-05 | 100.00%\n",
      "|   657 | In       | -0.4523194134235382 | 63.62%\n",
      "|   456 | this     | -0.023332329466938972 | 97.69%\n",
      "|  1222 | case     | -0.4360274374485016 | 64.66%\n",
      "| 28725 | ,        | -0.00017677174764685333 | 99.98%\n",
      "|   736 | there    | -0.3525223135948181 | 70.29%\n",
      "|   460 | are      | -0.0004538459761533886 | 99.95%\n",
      "|   708 | no       | -0.003657558234408498 | 99.63%\n",
      "|  2564 | parent   | -0.0011724510695785284 | 99.88%\n",
      "|  2053 | hes      | -4.768370445162873e-07 | 100.00%\n",
      "|   274 | es       | -2.3841855067985307e-07 | 100.00%\n",
      "|   442 | or       | -0.14292693138122559 | 86.68%\n",
      "|   439 | ex       | -0.00019667598826345056 | 99.98%\n",
      "|  6445 | ponents  | -1.490105023549404e-05 | 100.00%\n",
      "| 28725 | ,        | -0.026885611936450005 | 97.35%\n",
      "|   579 | so       | -0.2400268316268921 | 78.66%\n",
      "|   478 | we       | -0.004767004866153002 | 99.52%\n",
      "|  2318 | move     | -0.43828341364860535 | 64.51%\n",
      "|   356 | on       | -0.016059335321187973 | 98.41%\n",
      "|   298 | to       | -2.3245540432981215e-05 | 100.00%\n",
      "|   272 | the      | -0.30865201354026794 | 73.44%\n",
      "|  1679 | next     | -0.02343294583261013 | 97.68%\n",
      "|  3707 | step     | -0.29568472504615784 | 74.40%\n",
      "| 28723 | .        | -0.4083835482597351 | 66.47%\n",
      "|    13 | \n",
      "        | -1.0216823816299438 | 36.00%\n",
      "|    13 | \n",
      "        | -0.00015054999676067382 | 99.98%\n",
      "|  9977 | Step     | -1.4552913904190063 | 23.33%\n",
      "| 28705 |          | -2.145764938177308e-06 | 100.00%\n",
      "| 28740 | 1        | -3.576278118089249e-07 | 100.00%\n",
      "| 28747 | :        | -0.0006897454732097685 | 99.93%\n",
      "|  2744 | Per      | -0.5875695943832397 | 55.57%\n",
      "|   674 | form     | -3.2186455882765586e-06 | 100.00%\n",
      "|   272 | the      | -0.6084904670715332 | 54.42%\n",
      "|  6079 | multi    | -0.0008191090892069042 | 99.92%\n",
      "|  2459 | plication | -1.6689286894688848e-06 | 100.00%\n",
      "|    13 | \n",
      "        | -1.1190662384033203 | 32.66%\n",
      "| 28740 | 1        | -0.01619153469800949 | 98.39%\n",
      "| 28734 | 0        | -1.1920928244535389e-07 | 100.00%\n",
      "|   398 | *        | -1.4305104514278355e-06 | 100.00%\n",
      "| 28705 |          | -1.1920928244535389e-07 | 100.00%\n",
      "| 28750 | 2        | 0.0 | 100.00%\n",
      "|   327 | =        | -2.8132995794294402e-05 | 100.00%\n",
      "| 28705 |          | -0.08925306797027588 | 91.46%\n",
      "| 28750 | 2        | -0.0010765953920781612 | 99.89%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|    13 | \n",
      "        | -0.00010716341057559475 | 99.99%\n",
      "|    13 | \n",
      "        | -0.001634096377529204 | 99.84%\n",
      "|  9977 | Step     | -0.016278797760605812 | 98.39%\n",
      "| 28705 |          | 0.0 | 100.00%\n",
      "| 28750 | 2        | -1.1920922133867862e-06 | 100.00%\n",
      "| 28747 | :        | -1.2516897186287679e-05 | 100.00%\n",
      "|  2744 | Per      | -0.11129377782344818 | 89.47%\n",
      "|   674 | form     | -3.099436753473128e-06 | 100.00%\n",
      "|   272 | the      | -0.0020919600501656532 | 99.79%\n",
      "|  4518 | addition | -0.00014745102089364082 | 99.99%\n",
      "|    13 | \n",
      "        | -0.0026129886973649263 | 99.74%\n",
      "| 28774 | 9        | -5.07818695041351e-05 | 99.99%\n",
      "|   648 | +        | -1.2874520507466514e-05 | 100.00%\n",
      "| 28705 |          | -7.152555099310121e-07 | 100.00%\n",
      "| 28750 | 2        | -2.3841855067985307e-07 | 100.00%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|   327 | =        | -0.2814255356788635 | 75.47%\n",
      "| 28705 |          | -0.00013290952483657748 | 99.99%\n",
      "| 28750 | 2        | -0.00012468514614738524 | 99.99%\n",
      "| 28774 | 9        | -2.0265558760002023e-06 | 100.00%\n",
      "|    13 | \n",
      "        | -0.00028034092974849045 | 99.97%\n",
      "|    13 | \n",
      "        | -1.823885577323381e-05 | 100.00%\n",
      "|  5142 | So       | -0.0939418226480484 | 91.03%\n",
      "| 28725 | ,        | -0.576753556728363 | 56.17%\n",
      "|   272 | the      | -0.005017783492803574 | 99.50%\n",
      "|  1480 | final    | -0.45765984058380127 | 63.28%\n",
      "|  4372 | answer   | -0.00012611546844709665 | 99.99%\n",
      "|   349 | is       | -0.0141821363940835 | 98.59%\n",
      "|   414 | \\        | -0.05882381275296211 | 94.29%\n",
      "|  2858 | box      | -1.5735502529423684e-05 | 100.00%\n",
      "|   286 | ed       | -7.152555099310121e-07 | 100.00%\n",
      "| 28751 | {        | -5.960446742392378e-06 | 100.00%\n",
      "| 28750 | 2        | -9.179073458653875e-06 | 100.00%\n",
      "| 28774 | 9        | -1.1920928244535389e-07 | 100.00%\n",
      "|  2051 | }.       | -0.00015507926582358778 | 99.98%\n",
      "|     2 | </s>     | -9.095254790736362e-05 | 99.99%\n",
      "Generated Text Log Probabilities:\n",
      "Token: 1791, Generated Log Prob: -0.13854679465293884\n",
      "Token: 12049, Generated Log Prob: -0.02403736300766468\n",
      "Token: 272, Generated Log Prob: -0.16032463312149048\n",
      "Token: 5782, Generated Log Prob: -0.0005321278586052358\n",
      "Token: 28705, Generated Log Prob: -0.006042072083801031\n",
      "Token: 28774, Generated Log Prob: -0.00033087024348787963\n",
      "Token: 648, Generated Log Prob: -3.0636318115284666e-05\n",
      "Token: 28705, Generated Log Prob: -2.3841855067985307e-07\n",
      "Token: 28740, Generated Log Prob: -1.1920928244535389e-07\n",
      "Token: 28734, Generated Log Prob: 0.0\n",
      "Token: 398, Generated Log Prob: -4.1960789531003684e-05\n",
      "Token: 28705, Generated Log Prob: 0.0\n",
      "Token: 28750, Generated Log Prob: -5.960462772236497e-07\n",
      "Token: 28725, Generated Log Prob: -0.0024573388509452343\n",
      "Token: 478, Generated Log Prob: -0.019182177260518074\n",
      "Token: 927, Generated Log Prob: -0.0333944633603096\n",
      "Token: 298, Generated Log Prob: -1.4305104514278355e-06\n",
      "Token: 1372, Generated Log Prob: -0.008089871145784855\n",
      "Token: 272, Generated Log Prob: -1.3351351299206726e-05\n",
      "Token: 1745, Generated Log Prob: -0.004840560257434845\n",
      "Token: 302, Generated Log Prob: -3.576278118089249e-07\n",
      "Token: 6933, Generated Log Prob: -0.000809818331617862\n",
      "Token: 28725, Generated Log Prob: -0.05047057941555977\n",
      "Token: 690, Generated Log Prob: -0.3874920606613159\n",
      "Token: 349, Generated Log Prob: -0.00029583368450403214\n",
      "Token: 2608, Generated Log Prob: -0.5905787348747253\n",
      "Token: 10216, Generated Log Prob: -0.031841397285461426\n",
      "Token: 486, Generated Log Prob: -0.014357476495206356\n",
      "Token: 272, Generated Log Prob: -3.957670196541585e-05\n",
      "Token: 1183, Generated Log Prob: -0.0006562701310031116\n",
      "Token: 1689, Generated Log Prob: -5.245195097813848e-06\n",
      "Token: 1082, Generated Log Prob: -0.00020168177434243262\n",
      "Token: 21025, Generated Log Prob: -6.0437283536884934e-05\n",
      "Token: 4915, Generated Log Prob: -0.0001408954558428377\n",
      "Token: 2109, Generated Log Prob: -5.483612312673358e-06\n",
      "Token: 28747, Generated Log Prob: -0.016775256022810936\n",
      "Token: 18712, Generated Log Prob: -0.0033274304587394\n",
      "Token: 2053, Generated Log Prob: -8.594620157964528e-05\n",
      "Token: 274, Generated Log Prob: -1.1920928244535389e-07\n",
      "Token: 28725, Generated Log Prob: -0.0005404680850915611\n",
      "Token: 1529, Generated Log Prob: -0.0003573255962692201\n",
      "Token: 6445, Generated Log Prob: -0.00024029705673456192\n",
      "Token: 28725, Generated Log Prob: -6.711257447022945e-05\n",
      "Token: 18317, Generated Log Prob: -0.0013487775577232242\n",
      "Token: 2459, Generated Log Prob: -5.5549986427649856e-05\n",
      "Token: 304, Generated Log Prob: -0.00012981049076188356\n",
      "Token: 8618, Generated Log Prob: -0.0016381428577005863\n",
      "Token: 325, Generated Log Prob: -0.0004058252670802176\n",
      "Token: 3211, Generated Log Prob: -0.002120153047144413\n",
      "Token: 1749, Generated Log Prob: -0.0003163314249832183\n",
      "Token: 298, Generated Log Prob: -4.291525328881107e-06\n",
      "Token: 1103, Generated Log Prob: -1.3351351299206726e-05\n",
      "Token: 557, Generated Log Prob: -5.364403477869928e-06\n",
      "Token: 304, Generated Log Prob: -0.5764424204826355\n",
      "Token: 3301, Generated Log Prob: -0.004477238282561302\n",
      "Token: 685, Generated Log Prob: -9.417489309271332e-06\n",
      "Token: 304, Generated Log Prob: -8.344646857949556e-07\n",
      "Token: 5078, Generated Log Prob: -9.238292841473594e-05\n",
      "Token: 434, Generated Log Prob: -6.97350042173639e-05\n",
      "Token: 1774, Generated Log Prob: -1.4066597032069694e-05\n",
      "Token: 325, Generated Log Prob: -1.7523612768854946e-05\n",
      "Token: 3211, Generated Log Prob: -0.00010966652916977182\n",
      "Token: 1749, Generated Log Prob: -5.757642793469131e-05\n",
      "Token: 298, Generated Log Prob: -2.145764938177308e-06\n",
      "Token: 1103, Generated Log Prob: -0.00022075122979003936\n",
      "Token: 609, Generated Log Prob: -8.713819261174649e-05\n",
      "Token: 13, Generated Log Prob: -0.2187599241733551\n",
      "Token: 13, Generated Log Prob: -1.0132738680113107e-05\n",
      "Token: 657, Generated Log Prob: -0.4523194134235382\n",
      "Token: 456, Generated Log Prob: -0.023332329466938972\n",
      "Token: 1222, Generated Log Prob: -0.4360274374485016\n",
      "Token: 28725, Generated Log Prob: -0.00017677174764685333\n",
      "Token: 736, Generated Log Prob: -0.3525223135948181\n",
      "Token: 460, Generated Log Prob: -0.0004538459761533886\n",
      "Token: 708, Generated Log Prob: -0.003657558234408498\n",
      "Token: 2564, Generated Log Prob: -0.0011724510695785284\n",
      "Token: 2053, Generated Log Prob: -4.768370445162873e-07\n",
      "Token: 274, Generated Log Prob: -2.3841855067985307e-07\n",
      "Token: 442, Generated Log Prob: -0.14292693138122559\n",
      "Token: 439, Generated Log Prob: -0.00019667598826345056\n",
      "Token: 6445, Generated Log Prob: -1.490105023549404e-05\n",
      "Token: 28725, Generated Log Prob: -0.026885611936450005\n",
      "Token: 579, Generated Log Prob: -0.2400268316268921\n",
      "Token: 478, Generated Log Prob: -0.004767004866153002\n",
      "Token: 2318, Generated Log Prob: -0.43828341364860535\n",
      "Token: 356, Generated Log Prob: -0.016059335321187973\n",
      "Token: 298, Generated Log Prob: -2.3245540432981215e-05\n",
      "Token: 272, Generated Log Prob: -0.30865201354026794\n",
      "Token: 1679, Generated Log Prob: -0.02343294583261013\n",
      "Token: 3707, Generated Log Prob: -0.29568472504615784\n",
      "Token: 28723, Generated Log Prob: -0.4083835482597351\n",
      "Token: 13, Generated Log Prob: -1.0216823816299438\n",
      "Token: 13, Generated Log Prob: -0.00015054999676067382\n",
      "Token: 9977, Generated Log Prob: -1.4552913904190063\n",
      "Token: 28705, Generated Log Prob: -2.145764938177308e-06\n",
      "Token: 28740, Generated Log Prob: -3.576278118089249e-07\n",
      "Token: 28747, Generated Log Prob: -0.0006897454732097685\n",
      "Token: 2744, Generated Log Prob: -0.5875695943832397\n",
      "Token: 674, Generated Log Prob: -3.2186455882765586e-06\n",
      "Token: 272, Generated Log Prob: -0.6084904670715332\n",
      "Token: 6079, Generated Log Prob: -0.0008191090892069042\n",
      "Token: 2459, Generated Log Prob: -1.6689286894688848e-06\n",
      "Token: 13, Generated Log Prob: -1.1190662384033203\n",
      "Token: 28740, Generated Log Prob: -0.01619153469800949\n",
      "Token: 28734, Generated Log Prob: -1.1920928244535389e-07\n",
      "Token: 398, Generated Log Prob: -1.4305104514278355e-06\n",
      "Token: 28705, Generated Log Prob: -1.1920928244535389e-07\n",
      "Token: 28750, Generated Log Prob: 0.0\n",
      "Token: 327, Generated Log Prob: -2.8132995794294402e-05\n",
      "Token: 28705, Generated Log Prob: -0.08925306797027588\n",
      "Token: 28750, Generated Log Prob: -0.0010765953920781612\n",
      "Token: 28734, Generated Log Prob: 0.0\n",
      "Token: 13, Generated Log Prob: -0.00010716341057559475\n",
      "Token: 13, Generated Log Prob: -0.001634096377529204\n",
      "Token: 9977, Generated Log Prob: -0.016278797760605812\n",
      "Token: 28705, Generated Log Prob: 0.0\n",
      "Token: 28750, Generated Log Prob: -1.1920922133867862e-06\n",
      "Token: 28747, Generated Log Prob: -1.2516897186287679e-05\n",
      "Token: 2744, Generated Log Prob: -0.11129377782344818\n",
      "Token: 674, Generated Log Prob: -3.099436753473128e-06\n",
      "Token: 272, Generated Log Prob: -0.0020919600501656532\n",
      "Token: 4518, Generated Log Prob: -0.00014745102089364082\n",
      "Token: 13, Generated Log Prob: -0.0026129886973649263\n",
      "Token: 28774, Generated Log Prob: -5.07818695041351e-05\n",
      "Token: 648, Generated Log Prob: -1.2874520507466514e-05\n",
      "Token: 28705, Generated Log Prob: -7.152555099310121e-07\n",
      "Token: 28750, Generated Log Prob: -2.3841855067985307e-07\n",
      "Token: 28734, Generated Log Prob: 0.0\n",
      "Token: 327, Generated Log Prob: -0.2814255356788635\n",
      "Token: 28705, Generated Log Prob: -0.00013290952483657748\n",
      "Token: 28750, Generated Log Prob: -0.00012468514614738524\n",
      "Token: 28774, Generated Log Prob: -2.0265558760002023e-06\n",
      "Token: 13, Generated Log Prob: -0.00028034092974849045\n",
      "Token: 13, Generated Log Prob: -1.823885577323381e-05\n",
      "Token: 5142, Generated Log Prob: -0.0939418226480484\n",
      "Token: 28725, Generated Log Prob: -0.576753556728363\n",
      "Token: 272, Generated Log Prob: -0.005017783492803574\n",
      "Token: 1480, Generated Log Prob: -0.45765984058380127\n",
      "Token: 4372, Generated Log Prob: -0.00012611546844709665\n",
      "Token: 349, Generated Log Prob: -0.0141821363940835\n",
      "Token: 414, Generated Log Prob: -0.05882381275296211\n",
      "Token: 2858, Generated Log Prob: -1.5735502529423684e-05\n",
      "Token: 286, Generated Log Prob: -7.152555099310121e-07\n",
      "Token: 28751, Generated Log Prob: -5.960446742392378e-06\n",
      "Token: 28750, Generated Log Prob: -9.179073458653875e-06\n",
      "Token: 28774, Generated Log Prob: -1.1920928244535389e-07\n",
      "Token: 2051, Generated Log Prob: -0.00015507926582358778\n",
      "Token: 2, Generated Log Prob: -9.095254790736362e-05\n",
      "\n",
      "Calculated Log Probabilities from Concatenated Text:\n",
      "Token: [, Calculated Log Prob: -9.379661560058594\n",
      "Token: INST, Calculated Log Prob: -11.709663391113281\n",
      "Token: ], Calculated Log Prob: -12.90582275390625\n",
      "Token: What, Calculated Log Prob: -13.26082992553711\n",
      "Token: is, Calculated Log Prob: -0.6441332697868347\n",
      "Token: , Calculated Log Prob: -7.122815132141113\n",
      "Token: 9, Calculated Log Prob: -3.511679172515869\n",
      "Token: +, Calculated Log Prob: -5.391874313354492\n",
      "Token: , Calculated Log Prob: -0.23111653327941895\n",
      "Token: 1, Calculated Log Prob: -1.1535981893539429\n",
      "Token: 0, Calculated Log Prob: -1.4491122961044312\n",
      "Token: *, Calculated Log Prob: -1.6907577514648438\n",
      "Token: , Calculated Log Prob: -0.11482894420623779\n",
      "Token: 2, Calculated Log Prob: -0.3742263615131378\n",
      "Token: ?, Calculated Log Prob: -0.12354228645563126\n",
      "Token: \n",
      ", Calculated Log Prob: -0.2805356979370117\n",
      "Token: Please, Calculated Log Prob: -9.997407913208008\n",
      "Token: reason, Calculated Log Prob: -12.95058536529541\n",
      "Token: step, Calculated Log Prob: -5.26135778427124\n",
      "Token: by, Calculated Log Prob: -0.062126148492097855\n",
      "Token: step, Calculated Log Prob: -7.497983460780233e-05\n",
      "Token: ,, Calculated Log Prob: -2.7327542304992676\n",
      "Token: and, Calculated Log Prob: -3.515874147415161\n",
      "Token: put, Calculated Log Prob: -8.407917022705078\n",
      "Token: your, Calculated Log Prob: -4.023624897003174\n",
      "Token: final, Calculated Log Prob: -4.041463851928711\n",
      "Token: answer, Calculated Log Prob: -0.032512418925762177\n",
      "Token: within, Calculated Log Prob: -7.389629364013672\n",
      "Token: \\, Calculated Log Prob: -12.62710952758789\n",
      "Token: box, Calculated Log Prob: -4.384269714355469\n",
      "Token: ed, Calculated Log Prob: -0.00013183678674977273\n",
      "Token: {, Calculated Log Prob: -0.07179071754217148\n",
      "Token: }., Calculated Log Prob: -0.8477483987808228\n",
      "Token: [, Calculated Log Prob: -13.785758972167969\n",
      "Token: /, Calculated Log Prob: -15.247478485107422\n",
      "Token: INST, Calculated Log Prob: -10.707427978515625\n",
      "Token: ], Calculated Log Prob: -11.843785285949707\n",
      "Token: To, Calculated Log Prob: -0.13824182748794556\n",
      "Token: solve, Calculated Log Prob: -0.022136036306619644\n",
      "Token: the, Calculated Log Prob: -0.14277978241443634\n",
      "Token: expression, Calculated Log Prob: -0.0005411829333752394\n",
      "Token: , Calculated Log Prob: -0.0082096504047513\n",
      "Token: 9, Calculated Log Prob: -0.00034397884155623615\n",
      "Token: +, Calculated Log Prob: -3.0636318115284666e-05\n",
      "Token: , Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: 1, Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: 0, Calculated Log Prob: 0.0\n",
      "Token: *, Calculated Log Prob: -4.0411134250462055e-05\n",
      "Token: , Calculated Log Prob: 0.0\n",
      "Token: 2, Calculated Log Prob: -7.152555099310121e-07\n",
      "Token: ,, Calculated Log Prob: -0.0025719678960740566\n",
      "Token: we, Calculated Log Prob: -0.018918199464678764\n",
      "Token: need, Calculated Log Prob: -0.03363516926765442\n",
      "Token: to, Calculated Log Prob: -1.5497195136049413e-06\n",
      "Token: follow, Calculated Log Prob: -0.007955886423587799\n",
      "Token: the, Calculated Log Prob: -1.3112935448589269e-05\n",
      "Token: order, Calculated Log Prob: -0.004836170934140682\n",
      "Token: of, Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: operations, Calculated Log Prob: -0.0007257211836986244\n",
      "Token: ,, Calculated Log Prob: -0.05748266354203224\n",
      "Token: which, Calculated Log Prob: -0.4392339885234833\n",
      "Token: is, Calculated Log Prob: -0.0003169272968079895\n",
      "Token: often, Calculated Log Prob: -0.5662762522697449\n",
      "Token: remembered, Calculated Log Prob: -0.03176853060722351\n",
      "Token: by, Calculated Log Prob: -0.014357946813106537\n",
      "Token: the, Calculated Log Prob: -5.066266385256313e-05\n",
      "Token: ac, Calculated Log Prob: -0.0005915798828937113\n",
      "Token: ron, Calculated Log Prob: -5.364403477869928e-06\n",
      "Token: ym, Calculated Log Prob: -0.00021848674805369228\n",
      "Token: PE, Calculated Log Prob: -6.09140915912576e-05\n",
      "Token: MD, Calculated Log Prob: -0.00015627116954419762\n",
      "Token: AS, Calculated Log Prob: -4.887569048150908e-06\n",
      "Token: :, Calculated Log Prob: -0.01670972630381584\n",
      "Token: Parent, Calculated Log Prob: -0.0031417079735547304\n",
      "Token: hes, Calculated Log Prob: -8.594620157964528e-05\n",
      "Token: es, Calculated Log Prob: -1.1920928244535389e-07\n",
      "Token: ,, Calculated Log Prob: -0.0006093314150348306\n",
      "Token: Ex, Calculated Log Prob: -0.00035780228790827096\n",
      "Token: ponents, Calculated Log Prob: -0.00021419614495243877\n",
      "Token: ,, Calculated Log Prob: -5.924526340095326e-05\n",
      "Token: Multi, Calculated Log Prob: -0.0014040146488696337\n",
      "Token: plication, Calculated Log Prob: -5.4238757002167404e-05\n",
      "Token: and, Calculated Log Prob: -0.00013004888023715466\n",
      "Token: Division, Calculated Log Prob: -0.0016461167251691222\n",
      "Token: (, Calculated Log Prob: -0.000516877043992281\n",
      "Token: from, Calculated Log Prob: -0.002379921032115817\n",
      "Token: left, Calculated Log Prob: -0.00035649145138449967\n",
      "Token: to, Calculated Log Prob: -4.0531076592742465e-06\n",
      "Token: right, Calculated Log Prob: -1.5139465176616795e-05\n",
      "Token: ),, Calculated Log Prob: -5.245195097813848e-06\n",
      "Token: and, Calculated Log Prob: -0.5764377117156982\n",
      "Token: Add, Calculated Log Prob: -0.00453562568873167\n",
      "Token: ition, Calculated Log Prob: -8.821448318485636e-06\n",
      "Token: and, Calculated Log Prob: -9.536738616588991e-07\n",
      "Token: Sub, Calculated Log Prob: -9.238292841473594e-05\n",
      "Token: tr, Calculated Log Prob: -6.997340824455023e-05\n",
      "Token: action, Calculated Log Prob: -1.3470558769768104e-05\n",
      "Token: (, Calculated Log Prob: -1.6212332411669195e-05\n",
      "Token: from, Calculated Log Prob: -9.679325739853084e-05\n",
      "Token: left, Calculated Log Prob: -5.817244164063595e-05\n",
      "Token: to, Calculated Log Prob: -2.145764938177308e-06\n",
      "Token: right, Calculated Log Prob: -0.00025090406415984035\n",
      "Token: )., Calculated Log Prob: -9.238292841473594e-05\n",
      "Token: \n",
      ", Calculated Log Prob: -0.2181280106306076\n",
      "Token: \n",
      ", Calculated Log Prob: -9.059865078597795e-06\n",
      "Token: In, Calculated Log Prob: -0.41002434492111206\n",
      "Token: this, Calculated Log Prob: -0.021391388028860092\n",
      "Token: case, Calculated Log Prob: -0.43602871894836426\n",
      "Token: ,, Calculated Log Prob: -0.00016735582903493196\n",
      "Token: there, Calculated Log Prob: -0.353019654750824\n",
      "Token: are, Calculated Log Prob: -0.0004558716027531773\n",
      "Token: no, Calculated Log Prob: -0.004136103205382824\n",
      "Token: parent, Calculated Log Prob: -0.001262939884327352\n",
      "Token: hes, Calculated Log Prob: -3.576278118089249e-07\n",
      "Token: es, Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: or, Calculated Log Prob: -0.16047176718711853\n",
      "Token: ex, Calculated Log Prob: -0.00019870213873218745\n",
      "Token: ponents, Calculated Log Prob: -1.5020257706055418e-05\n",
      "Token: ,, Calculated Log Prob: -0.028614375740289688\n",
      "Token: so, Calculated Log Prob: -0.2400253266096115\n",
      "Token: we, Calculated Log Prob: -0.004226086661219597\n",
      "Token: move, Calculated Log Prob: -0.39644327759742737\n",
      "Token: on, Calculated Log Prob: -0.017069686204195023\n",
      "Token: to, Calculated Log Prob: -2.2649508537142538e-05\n",
      "Token: the, Calculated Log Prob: -0.31193122267723083\n",
      "Token: next, Calculated Log Prob: -0.026239020749926567\n",
      "Token: step, Calculated Log Prob: -0.3001135289669037\n",
      "Token: ., Calculated Log Prob: -0.40889522433280945\n",
      "Token: \n",
      ", Calculated Log Prob: -1.0425580739974976\n",
      "Token: \n",
      ", Calculated Log Prob: -0.00014280252798926085\n",
      "Token: Step, Calculated Log Prob: -1.4867056608200073\n",
      "Token: , Calculated Log Prob: -2.145764938177308e-06\n",
      "Token: 1, Calculated Log Prob: -3.576278118089249e-07\n",
      "Token: :, Calculated Log Prob: -0.0006206493126228452\n",
      "Token: Per, Calculated Log Prob: -0.6451556086540222\n",
      "Token: form, Calculated Log Prob: -3.099436753473128e-06\n",
      "Token: the, Calculated Log Prob: -0.66339510679245\n",
      "Token: multi, Calculated Log Prob: -0.0008206575294025242\n",
      "Token: plication, Calculated Log Prob: -1.7881377516459906e-06\n",
      "Token: \n",
      ", Calculated Log Prob: -1.1323245763778687\n",
      "Token: 1, Calculated Log Prob: -0.018819116055965424\n",
      "Token: 0, Calculated Log Prob: -1.1920928244535389e-07\n",
      "Token: *, Calculated Log Prob: -1.5497195136049413e-06\n",
      "Token: , Calculated Log Prob: -1.1920928244535389e-07\n",
      "Token: 2, Calculated Log Prob: 0.0\n",
      "Token: =, Calculated Log Prob: -2.8609820219571702e-05\n",
      "Token: , Calculated Log Prob: -0.07917316257953644\n",
      "Token: 2, Calculated Log Prob: -0.0008068405441008508\n",
      "Token: 0, Calculated Log Prob: 0.0\n",
      "Token: \n",
      ", Calculated Log Prob: -0.00010513706365600228\n",
      "Token: \n",
      ", Calculated Log Prob: -0.0017215684056282043\n",
      "Token: Step, Calculated Log Prob: -0.016283255070447922\n",
      "Token: , Calculated Log Prob: 0.0\n",
      "Token: 2, Calculated Log Prob: -1.1920922133867862e-06\n",
      "Token: :, Calculated Log Prob: -1.0967194612021558e-05\n",
      "Token: Per, Calculated Log Prob: -0.09585320949554443\n",
      "Token: form, Calculated Log Prob: -2.9802276912960224e-06\n",
      "Token: the, Calculated Log Prob: -0.0017521518748253584\n",
      "Token: addition, Calculated Log Prob: -0.0001573438785271719\n",
      "Token: \n",
      ", Calculated Log Prob: -0.0023045192938297987\n",
      "Token: 9, Calculated Log Prob: -5.0424259825376794e-05\n",
      "Token: +, Calculated Log Prob: -1.5616295058862306e-05\n",
      "Token: , Calculated Log Prob: -7.152555099310121e-07\n",
      "Token: 2, Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: 0, Calculated Log Prob: 0.0\n",
      "Token: =, Calculated Log Prob: -0.252165287733078\n",
      "Token: , Calculated Log Prob: -0.0001284993631998077\n",
      "Token: 2, Calculated Log Prob: -0.00012730741582345217\n",
      "Token: 9, Calculated Log Prob: -2.0265558760002023e-06\n",
      "Token: \n",
      ", Calculated Log Prob: -0.00029666791670024395\n",
      "Token: \n",
      ", Calculated Log Prob: -1.7523612768854946e-05\n",
      "Token: So, Calculated Log Prob: -0.10031203180551529\n",
      "Token: ,, Calculated Log Prob: -0.5768049955368042\n",
      "Token: the, Calculated Log Prob: -0.004491004627197981\n",
      "Token: final, Calculated Log Prob: -0.5007284879684448\n",
      "Token: answer, Calculated Log Prob: -0.00014161060971673578\n",
      "Token: is, Calculated Log Prob: -0.014703132212162018\n",
      "Token: \\, Calculated Log Prob: -0.05873129889369011\n",
      "Token: box, Calculated Log Prob: -1.5020257706055418e-05\n",
      "Token: ed, Calculated Log Prob: -7.152555099310121e-07\n",
      "Token: {, Calculated Log Prob: -6.556489552167477e-06\n",
      "Token: 2, Calculated Log Prob: -9.65590606938349e-06\n",
      "Token: 9, Calculated Log Prob: 0.0\n",
      "Token: }., Calculated Log Prob: -0.00017033556650858372\n"
     ]
    }
   ],
   "source": [
    "# Load your model and tokenizer\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# Generate text and scores, and calculate log probabilities\n",
    "question = \"What is 9 + 10 * 2?\"\n",
    "total_tokens, generated_tokens, generated_logprobs = generate_text_and_scores(model, tokenizer, question)\n",
    "print(\"Generated Text Log Probabilities:\")\n",
    "for i, (token, score) in enumerate(zip(generated_tokens, generated_logprobs)):\n",
    "    print(f\"Token: {token}, Generated Log Prob: {score.item()}\")\n",
    "    \n",
    "log_probs = to_tokens_and_logprobs(model, tokenizer, total_tokens) \n",
    "\n",
    "print(\"\\nCalculated Log Probabilities from Concatenated Text:\")\n",
    "for sequence in log_probs:\n",
    "    for token, log_prob in sequence:\n",
    "        print(f\"Token: {token}, Calculated Log Prob: {log_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Prompt Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   733, 16289, 28793,  1824,   349, 28705, 28774,   648, 28705,\n",
      "         28740, 28734,   398, 28705, 28750, 28804,    13, 12069,  2611,  3707,\n",
      "           486,  3707, 28725,   304,  1658,   574,  1480,  4372,  2373,   414,\n",
      "          2858,   286, 28751,  2051,   733, 28748, 16289, 28793,  1791, 12049,\n",
      "           272,  5782, 28705, 28774,   648, 28705, 28740, 28734,   398, 28705,\n",
      "         28750, 28725,   478,   927,   298,  1372,   272,  1745,   302,  6933,\n",
      "         28725,   690,   349,  2608, 10216,   486,   272,  1183,  1689,  1082,\n",
      "         21025,  4915,  2109, 28747, 18712,  2053,   274, 28725,  1529,  6445,\n",
      "         28725, 18317,  2459,   304,  8618,   325,  3211,  1749,   298,  1103,\n",
      "           557,   304,  3301,   685,   304,  5078,   434,  1774,   325,  3211,\n",
      "          1749,   298,  1103,   609,    13,    13,   657,   456,  1222, 28725,\n",
      "           736,   460,   708,  2564,  2053,   274,   442,   439,  6445, 28725,\n",
      "           579,   478,  2318,   356,   298,   272,  1679,  3707, 28723,    13,\n",
      "            13,  9977, 28705, 28740, 28747,  2744,   674,   272,  6079,  2459,\n",
      "            13, 28740, 28734,   398, 28705, 28750,   327, 28705, 28750, 28734,\n",
      "            13,    13,  9977, 28705, 28750, 28747,  2744,   674,   272,  4518,\n",
      "            13, 28774,   648, 28705, 28750, 28734,   327, 28705, 28750, 28774,\n",
      "            13,    13,  5142, 28725,   272,  1480,  4372,   349,   414,  2858,\n",
      "           286, 28751, 28750, 28774,  2051,     2]], device='mps:0')\n",
      "tensor([[ 1791, 12049,   272,  5782, 28705, 28774,   648, 28705, 28740, 28734,\n",
      "           398, 28705, 28750, 28725,   478,   927,   298,  1372,   272,  1745,\n",
      "           302,  6933, 28725,   690,   349,  2608, 10216,   486,   272,  1183,\n",
      "          1689,  1082, 21025,  4915,  2109, 28747, 18712,  2053,   274, 28725,\n",
      "          1529,  6445, 28725, 18317,  2459,   304,  8618,   325,  3211,  1749,\n",
      "           298,  1103,   557,   304,  3301,   685,   304,  5078,   434,  1774,\n",
      "           325,  3211,  1749,   298,  1103,   609,    13,    13,   657,   456,\n",
      "          1222, 28725,   736,   460,   708,  2564,  2053,   274,   442,   439,\n",
      "          6445, 28725,   579,   478,  2318,   356,   298,   272,  1679,  3707,\n",
      "         28723,    13,    13,  9977, 28705, 28740, 28747,  2744,   674,   272,\n",
      "          6079,  2459,    13, 28740, 28734,   398, 28705, 28750,   327, 28705,\n",
      "         28750, 28734,    13,    13,  9977, 28705, 28750, 28747,  2744,   674,\n",
      "           272,  4518,    13, 28774,   648, 28705, 28750, 28734,   327, 28705,\n",
      "         28750, 28774,    13,    13,  5142, 28725,   272,  1480,  4372,   349,\n",
      "           414,  2858,   286, 28751, 28750, 28774,  2051,     2]],\n",
      "       device='mps:0')\n",
      "|  1791 | To       | -0.13854679465293884 | 87.06%\n",
      "| 12049 | solve    | -0.02403736300766468 | 97.62%\n",
      "|   272 | the      | -0.16032463312149048 | 85.19%\n",
      "|  5782 | expression | -0.0005321278586052358 | 99.95%\n",
      "| 28705 |          | -0.006042072083801031 | 99.40%\n",
      "| 28774 | 9        | -0.00033087024348787963 | 99.97%\n",
      "|   648 | +        | -3.0636318115284666e-05 | 100.00%\n",
      "| 28705 |          | -2.3841855067985307e-07 | 100.00%\n",
      "| 28740 | 1        | -1.1920928244535389e-07 | 100.00%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|   398 | *        | -4.1960789531003684e-05 | 100.00%\n",
      "| 28705 |          | 0.0 | 100.00%\n",
      "| 28750 | 2        | -5.960462772236497e-07 | 100.00%\n",
      "| 28725 | ,        | -0.0024573388509452343 | 99.75%\n",
      "|   478 | we       | -0.019182177260518074 | 98.10%\n",
      "|   927 | need     | -0.0333944633603096 | 96.72%\n",
      "|   298 | to       | -1.4305104514278355e-06 | 100.00%\n",
      "|  1372 | follow   | -0.008089871145784855 | 99.19%\n",
      "|   272 | the      | -1.3351351299206726e-05 | 100.00%\n",
      "|  1745 | order    | -0.004840560257434845 | 99.52%\n",
      "|   302 | of       | -3.576278118089249e-07 | 100.00%\n",
      "|  6933 | operations | -0.000809818331617862 | 99.92%\n",
      "| 28725 | ,        | -0.05047057941555977 | 95.08%\n",
      "|   690 | which    | -0.3874920606613159 | 67.88%\n",
      "|   349 | is       | -0.00029583368450403214 | 99.97%\n",
      "|  2608 | often    | -0.5905787348747253 | 55.40%\n",
      "| 10216 | remembered | -0.031841397285461426 | 96.87%\n",
      "|   486 | by       | -0.014357476495206356 | 98.57%\n",
      "|   272 | the      | -3.957670196541585e-05 | 100.00%\n",
      "|  1183 | ac       | -0.0006562701310031116 | 99.93%\n",
      "|  1689 | ron      | -5.245195097813848e-06 | 100.00%\n",
      "|  1082 | ym       | -0.00020168177434243262 | 99.98%\n",
      "| 21025 | PE       | -6.0437283536884934e-05 | 99.99%\n",
      "|  4915 | MD       | -0.0001408954558428377 | 99.99%\n",
      "|  2109 | AS       | -5.483612312673358e-06 | 100.00%\n",
      "| 28747 | :        | -0.016775256022810936 | 98.34%\n",
      "| 18712 | Parent   | -0.0033274304587394 | 99.67%\n",
      "|  2053 | hes      | -8.594620157964528e-05 | 99.99%\n",
      "|   274 | es       | -1.1920928244535389e-07 | 100.00%\n",
      "| 28725 | ,        | -0.0005404680850915611 | 99.95%\n",
      "|  1529 | Ex       | -0.0003573255962692201 | 99.96%\n",
      "|  6445 | ponents  | -0.00024029705673456192 | 99.98%\n",
      "| 28725 | ,        | -6.711257447022945e-05 | 99.99%\n",
      "| 18317 | Multi    | -0.0013487775577232242 | 99.87%\n",
      "|  2459 | plication | -5.5549986427649856e-05 | 99.99%\n",
      "|   304 | and      | -0.00012981049076188356 | 99.99%\n",
      "|  8618 | Division | -0.0016381428577005863 | 99.84%\n",
      "|   325 | (        | -0.0004058252670802176 | 99.96%\n",
      "|  3211 | from     | -0.002120153047144413 | 99.79%\n",
      "|  1749 | left     | -0.0003163314249832183 | 99.97%\n",
      "|   298 | to       | -4.291525328881107e-06 | 100.00%\n",
      "|  1103 | right    | -1.3351351299206726e-05 | 100.00%\n",
      "|   557 | ),       | -5.364403477869928e-06 | 100.00%\n",
      "|   304 | and      | -0.5764424204826355 | 56.19%\n",
      "|  3301 | Add      | -0.004477238282561302 | 99.55%\n",
      "|   685 | ition    | -9.417489309271332e-06 | 100.00%\n",
      "|   304 | and      | -8.344646857949556e-07 | 100.00%\n",
      "|  5078 | Sub      | -9.238292841473594e-05 | 99.99%\n",
      "|   434 | tr       | -6.97350042173639e-05 | 99.99%\n",
      "|  1774 | action   | -1.4066597032069694e-05 | 100.00%\n",
      "|   325 | (        | -1.7523612768854946e-05 | 100.00%\n",
      "|  3211 | from     | -0.00010966652916977182 | 99.99%\n",
      "|  1749 | left     | -5.757642793469131e-05 | 99.99%\n",
      "|   298 | to       | -2.145764938177308e-06 | 100.00%\n",
      "|  1103 | right    | -0.00022075122979003936 | 99.98%\n",
      "|   609 | ).       | -8.713819261174649e-05 | 99.99%\n",
      "|    13 | \n",
      "        | -0.2187599241733551 | 80.35%\n",
      "|    13 | \n",
      "        | -1.0132738680113107e-05 | 100.00%\n",
      "|   657 | In       | -0.4523194134235382 | 63.62%\n",
      "|   456 | this     | -0.023332329466938972 | 97.69%\n",
      "|  1222 | case     | -0.4360274374485016 | 64.66%\n",
      "| 28725 | ,        | -0.00017677174764685333 | 99.98%\n",
      "|   736 | there    | -0.3525223135948181 | 70.29%\n",
      "|   460 | are      | -0.0004538459761533886 | 99.95%\n",
      "|   708 | no       | -0.003657558234408498 | 99.63%\n",
      "|  2564 | parent   | -0.0011724510695785284 | 99.88%\n",
      "|  2053 | hes      | -4.768370445162873e-07 | 100.00%\n",
      "|   274 | es       | -2.3841855067985307e-07 | 100.00%\n",
      "|   442 | or       | -0.14292693138122559 | 86.68%\n",
      "|   439 | ex       | -0.00019667598826345056 | 99.98%\n",
      "|  6445 | ponents  | -1.490105023549404e-05 | 100.00%\n",
      "| 28725 | ,        | -0.026885611936450005 | 97.35%\n",
      "|   579 | so       | -0.2400268316268921 | 78.66%\n",
      "|   478 | we       | -0.004767004866153002 | 99.52%\n",
      "|  2318 | move     | -0.43828341364860535 | 64.51%\n",
      "|   356 | on       | -0.016059335321187973 | 98.41%\n",
      "|   298 | to       | -2.3245540432981215e-05 | 100.00%\n",
      "|   272 | the      | -0.30865201354026794 | 73.44%\n",
      "|  1679 | next     | -0.02343294583261013 | 97.68%\n",
      "|  3707 | step     | -0.29568472504615784 | 74.40%\n",
      "| 28723 | .        | -0.4083835482597351 | 66.47%\n",
      "|    13 | \n",
      "        | -1.0216823816299438 | 36.00%\n",
      "|    13 | \n",
      "        | -0.00015054999676067382 | 99.98%\n",
      "|  9977 | Step     | -1.4552913904190063 | 23.33%\n",
      "| 28705 |          | -2.145764938177308e-06 | 100.00%\n",
      "| 28740 | 1        | -3.576278118089249e-07 | 100.00%\n",
      "| 28747 | :        | -0.0006897454732097685 | 99.93%\n",
      "|  2744 | Per      | -0.5875695943832397 | 55.57%\n",
      "|   674 | form     | -3.2186455882765586e-06 | 100.00%\n",
      "|   272 | the      | -0.6084904670715332 | 54.42%\n",
      "|  6079 | multi    | -0.0008191090892069042 | 99.92%\n",
      "|  2459 | plication | -1.6689286894688848e-06 | 100.00%\n",
      "|    13 | \n",
      "        | -1.1190662384033203 | 32.66%\n",
      "| 28740 | 1        | -0.01619153469800949 | 98.39%\n",
      "| 28734 | 0        | -1.1920928244535389e-07 | 100.00%\n",
      "|   398 | *        | -1.4305104514278355e-06 | 100.00%\n",
      "| 28705 |          | -1.1920928244535389e-07 | 100.00%\n",
      "| 28750 | 2        | 0.0 | 100.00%\n",
      "|   327 | =        | -2.8132995794294402e-05 | 100.00%\n",
      "| 28705 |          | -0.08925306797027588 | 91.46%\n",
      "| 28750 | 2        | -0.0010765953920781612 | 99.89%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|    13 | \n",
      "        | -0.00010716341057559475 | 99.99%\n",
      "|    13 | \n",
      "        | -0.001634096377529204 | 99.84%\n",
      "|  9977 | Step     | -0.016278797760605812 | 98.39%\n",
      "| 28705 |          | 0.0 | 100.00%\n",
      "| 28750 | 2        | -1.1920922133867862e-06 | 100.00%\n",
      "| 28747 | :        | -1.2516897186287679e-05 | 100.00%\n",
      "|  2744 | Per      | -0.11129377782344818 | 89.47%\n",
      "|   674 | form     | -3.099436753473128e-06 | 100.00%\n",
      "|   272 | the      | -0.0020919600501656532 | 99.79%\n",
      "|  4518 | addition | -0.00014745102089364082 | 99.99%\n",
      "|    13 | \n",
      "        | -0.0026129886973649263 | 99.74%\n",
      "| 28774 | 9        | -5.07818695041351e-05 | 99.99%\n",
      "|   648 | +        | -1.2874520507466514e-05 | 100.00%\n",
      "| 28705 |          | -7.152555099310121e-07 | 100.00%\n",
      "| 28750 | 2        | -2.3841855067985307e-07 | 100.00%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|   327 | =        | -0.2814255356788635 | 75.47%\n",
      "| 28705 |          | -0.00013290952483657748 | 99.99%\n",
      "| 28750 | 2        | -0.00012468514614738524 | 99.99%\n",
      "| 28774 | 9        | -2.0265558760002023e-06 | 100.00%\n",
      "|    13 | \n",
      "        | -0.00028034092974849045 | 99.97%\n",
      "|    13 | \n",
      "        | -1.823885577323381e-05 | 100.00%\n",
      "|  5142 | So       | -0.0939418226480484 | 91.03%\n",
      "| 28725 | ,        | -0.576753556728363 | 56.17%\n",
      "|   272 | the      | -0.005017783492803574 | 99.50%\n",
      "|  1480 | final    | -0.45765984058380127 | 63.28%\n",
      "|  4372 | answer   | -0.00012611546844709665 | 99.99%\n",
      "|   349 | is       | -0.0141821363940835 | 98.59%\n",
      "|   414 | \\        | -0.05882381275296211 | 94.29%\n",
      "|  2858 | box      | -1.5735502529423684e-05 | 100.00%\n",
      "|   286 | ed       | -7.152555099310121e-07 | 100.00%\n",
      "| 28751 | {        | -5.960446742392378e-06 | 100.00%\n",
      "| 28750 | 2        | -9.179073458653875e-06 | 100.00%\n",
      "| 28774 | 9        | -1.1920928244535389e-07 | 100.00%\n",
      "|  2051 | }.       | -0.00015507926582358778 | 99.98%\n",
      "|     2 | </s>     | -9.095254790736362e-05 | 99.99%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_input_and_scores(question, model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question + \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"}\n",
    "    ]\n",
    "    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_tensor.to(model.device), max_new_tokens=1000, return_dict_in_generate=True, output_scores=True)\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs.sequences[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "    input_text = tokenizer.decode(input_tensor[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Compute transition scores (log probabilities)\n",
    "    transition_scores = model.compute_transition_scores(\n",
    "        outputs.sequences, outputs.scores, normalize_logits=True\n",
    "    )\n",
    "    \n",
    "    # Extract log probabilities for the generated tokens\n",
    "    token_log_probs = transition_scores[0][input_tensor.shape[1]:].tolist()  # Assuming single batch\n",
    "\n",
    "    return input_text, generated_text, token_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tokens_and_logprobs(model, tokenizer, input_text, generated_text):\n",
    "    full_text = input_text + generated_text\n",
    "    input_ids = tokenizer(full_text, padding=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "    # Adjust indices to ignore the first token's log prob as it corresponds to the second token\n",
    "    probs = probs[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    batch = []\n",
    "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
    "        text_sequence = []\n",
    "        for token, p in zip(input_sentence, input_probs):\n",
    "            if token not in tokenizer.all_special_ids:\n",
    "                text_sequence.append((tokenizer.decode(token), p.item()))\n",
    "        batch.append(text_sequence)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text Log Probabilities:\n",
      "Token: ▁To, Generated Log Prob: -19.497920989990234\n",
      "Token: ▁solve, Generated Log Prob: -24.336538314819336\n",
      "Token: ▁the, Generated Log Prob: -26.707199096679688\n",
      "Token: ▁expression, Generated Log Prob: -27.356489181518555\n",
      "Token: ▁, Generated Log Prob: -19.32635498046875\n",
      "Token: 9, Generated Log Prob: -21.62142562866211\n",
      "Token: ▁+, Generated Log Prob: -30.819366455078125\n",
      "Token: ▁, Generated Log Prob: -29.140625\n",
      "Token: 1, Generated Log Prob: -21.78125\n",
      "Token: 0, Generated Log Prob: -21.6875\n",
      "Token: ▁*, Generated Log Prob: -25.496135711669922\n",
      "Token: ▁, Generated Log Prob: -27.359375\n",
      "Token: 2, Generated Log Prob: -27.93359375\n",
      "Token: ,, Generated Log Prob: -22.229019165039062\n",
      "Token: ▁we, Generated Log Prob: -22.026994705200195\n",
      "Token: ▁need, Generated Log Prob: -23.68183135986328\n",
      "Token: ▁to, Generated Log Prob: -30.445314407348633\n",
      "Token: ▁follow, Generated Log Prob: -22.601839065551758\n",
      "Token: ▁the, Generated Log Prob: -23.01563835144043\n",
      "Token: ▁order, Generated Log Prob: -21.551715850830078\n",
      "Token: ▁of, Generated Log Prob: -26.765625\n",
      "Token: ▁operations, Generated Log Prob: -25.664873123168945\n",
      "Token: ,, Generated Log Prob: -12.362970352172852\n",
      "Token: ▁which, Generated Log Prob: -20.59842872619629\n",
      "Token: ▁is, Generated Log Prob: -21.219045639038086\n",
      "Token: ▁often, Generated Log Prob: -17.489015579223633\n",
      "Token: ▁remembered, Generated Log Prob: -24.297466278076172\n",
      "Token: ▁by, Generated Log Prob: -18.733108520507812\n",
      "Token: ▁the, Generated Log Prob: -23.08597755432129\n",
      "Token: ▁ac, Generated Log Prob: -27.016281127929688\n",
      "Token: ron, Generated Log Prob: -25.8125057220459\n",
      "Token: ym, Generated Log Prob: -26.422077178955078\n",
      "Token: ▁PE, Generated Log Prob: -29.32818603515625\n",
      "Token: MD, Generated Log Prob: -32.14076614379883\n",
      "Token: AS, Generated Log Prob: -29.8398494720459\n",
      "Token: :, Generated Log Prob: -23.516775131225586\n",
      "Token: ▁Parent, Generated Log Prob: -23.050203323364258\n",
      "Token: hes, Generated Log Prob: -31.271570205688477\n",
      "Token: es, Generated Log Prob: -32.5\n",
      "Token: ,, Generated Log Prob: -22.766164779663086\n",
      "Token: ▁Ex, Generated Log Prob: -26.461294174194336\n",
      "Token: ponents, Generated Log Prob: -28.562740325927734\n",
      "Token: ,, Generated Log Prob: -26.00006675720215\n",
      "Token: ▁Multi, Generated Log Prob: -22.1732234954834\n",
      "Token: plication, Generated Log Prob: -26.96880531311035\n",
      "Token: ▁and, Generated Log Prob: -25.07825469970703\n",
      "Token: ▁Division, Generated Log Prob: -26.446950912475586\n",
      "Token: ▁(, Generated Log Prob: -22.43790626525879\n",
      "Token: from, Generated Log Prob: -30.627120971679688\n",
      "Token: ▁left, Generated Log Prob: -23.297191619873047\n",
      "Token: ▁to, Generated Log Prob: -20.375003814697266\n",
      "Token: ▁right, Generated Log Prob: -25.10938835144043\n",
      "Token: ),, Generated Log Prob: -26.0859432220459\n",
      "Token: ▁and, Generated Log Prob: -18.74831771850586\n",
      "Token: ▁Add, Generated Log Prob: -23.78572654724121\n",
      "Token: ition, Generated Log Prob: -30.507822036743164\n",
      "Token: ▁and, Generated Log Prob: -30.839599609375\n",
      "Token: ▁Sub, Generated Log Prob: -25.984466552734375\n",
      "Token: tr, Generated Log Prob: -28.168039321899414\n",
      "Token: action, Generated Log Prob: -29.06641960144043\n",
      "Token: ▁(, Generated Log Prob: -23.171892166137695\n",
      "Token: from, Generated Log Prob: -35.1251106262207\n",
      "Token: ▁left, Generated Log Prob: -25.327693939208984\n",
      "Token: ▁to, Generated Log Prob: -29.109376907348633\n",
      "Token: ▁right, Generated Log Prob: -23.847877502441406\n",
      "Token: )., Generated Log Prob: -21.23446273803711\n",
      "Token: <0x0A>, Generated Log Prob: -14.656259536743164\n",
      "Token: <0x0A>, Generated Log Prob: -26.390634536743164\n",
      "Token: In, Generated Log Prob: -18.374195098876953\n",
      "Token: ▁this, Generated Log Prob: -21.288957595825195\n",
      "Token: ▁case, Generated Log Prob: -26.81102752685547\n",
      "Token: ,, Generated Log Prob: -27.98455238342285\n",
      "Token: ▁there, Generated Log Prob: -18.821271896362305\n",
      "Token: ▁are, Generated Log Prob: -23.21920394897461\n",
      "Token: ▁no, Generated Log Prob: -26.253658294677734\n",
      "Token: ▁parent, Generated Log Prob: -22.43086051940918\n",
      "Token: hes, Generated Log Prob: -34.28125\n",
      "Token: es, Generated Log Prob: -33.62890625\n",
      "Token: ▁or, Generated Log Prob: -26.681989669799805\n",
      "Token: ▁ex, Generated Log Prob: -20.37519645690918\n",
      "Token: ponents, Generated Log Prob: -26.421890258789062\n",
      "Token: ,, Generated Log Prob: -23.948760986328125\n",
      "Token: ▁so, Generated Log Prob: -4.240026950836182\n",
      "Token: ▁we, Generated Log Prob: -23.5242977142334\n",
      "Token: ▁move, Generated Log Prob: -16.594533920288086\n",
      "Token: ▁on, Generated Log Prob: -23.65668487548828\n",
      "Token: ▁to, Generated Log Prob: -20.265647888183594\n",
      "Token: ▁the, Generated Log Prob: -12.7149019241333\n",
      "Token: ▁next, Generated Log Prob: -16.210933685302734\n",
      "Token: ▁step, Generated Log Prob: -18.459747314453125\n",
      "Token: ., Generated Log Prob: -6.033383369445801\n",
      "Token: <0x0A>, Generated Log Prob: -13.084182739257812\n",
      "Token: <0x0A>, Generated Log Prob: -21.285306930541992\n",
      "Token: Step, Generated Log Prob: -18.939666748046875\n",
      "Token: ▁, Generated Log Prob: -27.226564407348633\n",
      "Token: 1, Generated Log Prob: -27.63671875\n",
      "Token: :, Generated Log Prob: -24.293659210205078\n",
      "Token: ▁Per, Generated Log Prob: -14.650069236755371\n",
      "Token: form, Generated Log Prob: -19.875003814697266\n",
      "Token: ▁the, Generated Log Prob: -18.202239990234375\n",
      "Token: ▁multi, Generated Log Prob: -22.735193252563477\n",
      "Token: plication, Generated Log Prob: -32.2734375\n",
      "Token: <0x0A>, Generated Log Prob: -13.49406623840332\n",
      "Token: 1, Generated Log Prob: -5.641191482543945\n",
      "Token: 0, Generated Log Prob: -29.1640625\n",
      "Token: ▁*, Generated Log Prob: -25.664064407348633\n",
      "Token: ▁, Generated Log Prob: -27.2890625\n",
      "Token: 2, Generated Log Prob: -28.232421875\n",
      "Token: ▁=, Generated Log Prob: -21.078153610229492\n",
      "Token: ▁, Generated Log Prob: -21.862689971923828\n",
      "Token: 2, Generated Log Prob: -21.794044494628906\n",
      "Token: 0, Generated Log Prob: -28.34375\n",
      "Token: <0x0A>, Generated Log Prob: -23.285263061523438\n",
      "Token: <0x0A>, Generated Log Prob: -22.14225959777832\n",
      "Token: Step, Generated Log Prob: -24.492841720581055\n",
      "Token: ▁, Generated Log Prob: -26.40625\n",
      "Token: 2, Generated Log Prob: -31.179689407348633\n",
      "Token: :, Generated Log Prob: -29.703136444091797\n",
      "Token: ▁Per, Generated Log Prob: -16.59566879272461\n",
      "Token: form, Generated Log Prob: -32.632816314697266\n",
      "Token: ▁the, Generated Log Prob: -22.345842361450195\n",
      "Token: ▁addition, Generated Log Prob: -23.297021865844727\n",
      "Token: <0x0A>, Generated Log Prob: -15.158863067626953\n",
      "Token: 9, Generated Log Prob: -24.515676498413086\n",
      "Token: ▁+, Generated Log Prob: -28.14210319519043\n",
      "Token: ▁, Generated Log Prob: -26.59375\n",
      "Token: 2, Generated Log Prob: -27.5986328125\n",
      "Token: 0, Generated Log Prob: -31.0625\n",
      "Token: ▁=, Generated Log Prob: -1.4064255952835083\n",
      "Token: ▁, Generated Log Prob: -16.062633514404297\n",
      "Token: 2, Generated Log Prob: -19.578248977661133\n",
      "Token: 9, Generated Log Prob: -23.718751907348633\n",
      "Token: <0x0A>, Generated Log Prob: -12.937780380249023\n",
      "Token: <0x0A>, Generated Log Prob: -21.687517166137695\n",
      "Token: So, Generated Log Prob: -26.218942642211914\n",
      "Token: ,, Generated Log Prob: -23.451753616333008\n",
      "Token: ▁the, Generated Log Prob: -0.00501766474917531\n",
      "Token: ▁final, Generated Log Prob: -20.317035675048828\n",
      "Token: ▁answer, Generated Log Prob: -28.339969635009766\n",
      "Token: ▁is, Generated Log Prob: -20.162620544433594\n",
      "Token: ▁\\, Generated Log Prob: -20.61741828918457\n",
      "Token: box, Generated Log Prob: -25.148452758789062\n",
      "Token: ed, Generated Log Prob: -33.6484375\n",
      "Token: {, Generated Log Prob: -26.0625057220459\n",
      "Token: 2, Generated Log Prob: -8.940656698541716e-06\n",
      "Token: 9, Generated Log Prob: -30.7421875\n",
      "Token: }., Generated Log Prob: -26.458162307739258\n",
      "\n",
      "Calculated Log Probabilities from Concatenated Text:\n",
      "Token: [, Calculated Log Prob: -9.379661560058594\n",
      "Token: INST, Calculated Log Prob: -11.709663391113281\n",
      "Token: ], Calculated Log Prob: -12.90582275390625\n",
      "Token: What, Calculated Log Prob: -13.26082992553711\n",
      "Token: is, Calculated Log Prob: -0.6441332697868347\n",
      "Token: , Calculated Log Prob: -7.122815132141113\n",
      "Token: 9, Calculated Log Prob: -3.511679172515869\n",
      "Token: +, Calculated Log Prob: -5.391874313354492\n",
      "Token: , Calculated Log Prob: -0.23111653327941895\n",
      "Token: 1, Calculated Log Prob: -1.1535981893539429\n",
      "Token: 0, Calculated Log Prob: -1.4491122961044312\n",
      "Token: *, Calculated Log Prob: -1.6907577514648438\n",
      "Token: , Calculated Log Prob: -0.11482894420623779\n",
      "Token: 2, Calculated Log Prob: -0.3742263615131378\n",
      "Token: ?, Calculated Log Prob: -0.12354228645563126\n",
      "Token: \n",
      ", Calculated Log Prob: -0.2805356979370117\n",
      "Token: Please, Calculated Log Prob: -9.997407913208008\n",
      "Token: reason, Calculated Log Prob: -12.95058536529541\n",
      "Token: step, Calculated Log Prob: -5.26135778427124\n",
      "Token: by, Calculated Log Prob: -0.062126148492097855\n",
      "Token: step, Calculated Log Prob: -7.497983460780233e-05\n",
      "Token: ,, Calculated Log Prob: -2.7327542304992676\n",
      "Token: and, Calculated Log Prob: -3.515874147415161\n",
      "Token: put, Calculated Log Prob: -8.407917022705078\n",
      "Token: your, Calculated Log Prob: -4.023624897003174\n",
      "Token: final, Calculated Log Prob: -4.041463851928711\n",
      "Token: answer, Calculated Log Prob: -0.032512418925762177\n",
      "Token: within, Calculated Log Prob: -7.389629364013672\n",
      "Token: \\, Calculated Log Prob: -12.62710952758789\n",
      "Token: box, Calculated Log Prob: -4.384269714355469\n",
      "Token: ed, Calculated Log Prob: -0.00013183678674977273\n",
      "Token: {, Calculated Log Prob: -0.07179071754217148\n",
      "Token: }., Calculated Log Prob: -0.8477483987808228\n",
      "Token: [, Calculated Log Prob: -13.785758972167969\n",
      "Token: /, Calculated Log Prob: -15.247478485107422\n",
      "Token: INST, Calculated Log Prob: -10.707427978515625\n",
      "Token: ], Calculated Log Prob: -11.843785285949707\n",
      "Token: To, Calculated Log Prob: -7.325741767883301\n",
      "Token: solve, Calculated Log Prob: -0.03880668058991432\n",
      "Token: the, Calculated Log Prob: -0.2520483732223511\n",
      "Token: expression, Calculated Log Prob: -0.0005380851798690856\n",
      "Token: , Calculated Log Prob: -0.0032298800069838762\n",
      "Token: 9, Calculated Log Prob: -0.000324673397699371\n",
      "Token: +, Calculated Log Prob: -2.4199192921514623e-05\n",
      "Token: , Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: 1, Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: 0, Calculated Log Prob: 0.0\n",
      "Token: *, Calculated Log Prob: -3.635817120084539e-05\n",
      "Token: , Calculated Log Prob: 0.0\n",
      "Token: 2, Calculated Log Prob: -5.960462772236497e-07\n",
      "Token: ,, Calculated Log Prob: -0.002832807367667556\n",
      "Token: we, Calculated Log Prob: -0.0161511842161417\n",
      "Token: need, Calculated Log Prob: -0.04779325798153877\n",
      "Token: to, Calculated Log Prob: -3.4570634852570947e-06\n",
      "Token: follow, Calculated Log Prob: -0.01019272394478321\n",
      "Token: the, Calculated Log Prob: -1.3828182090946939e-05\n",
      "Token: order, Calculated Log Prob: -0.003366281511262059\n",
      "Token: of, Calculated Log Prob: -4.768370445162873e-07\n",
      "Token: operations, Calculated Log Prob: -0.0008238735608756542\n",
      "Token: ,, Calculated Log Prob: -0.07052040845155716\n",
      "Token: which, Calculated Log Prob: -0.3726021945476532\n",
      "Token: is, Calculated Log Prob: -0.000287729810224846\n",
      "Token: often, Calculated Log Prob: -0.6218234896659851\n",
      "Token: remembered, Calculated Log Prob: -0.03447694703936577\n",
      "Token: by, Calculated Log Prob: -0.014452654868364334\n",
      "Token: the, Calculated Log Prob: -4.017272294731811e-05\n",
      "Token: ac, Calculated Log Prob: -0.0005884823040105402\n",
      "Token: ron, Calculated Log Prob: -5.960446742392378e-06\n",
      "Token: ym, Calculated Log Prob: -0.00020251607929822057\n",
      "Token: PE, Calculated Log Prob: -6.007967749610543e-05\n",
      "Token: MD, Calculated Log Prob: -0.0001565095444675535\n",
      "Token: AS, Calculated Log Prob: -5.602820692729438e-06\n",
      "Token: :, Calculated Log Prob: -0.019266139715909958\n",
      "Token: Parent, Calculated Log Prob: -0.003140163142234087\n",
      "Token: hes, Calculated Log Prob: -9.738924563862383e-05\n",
      "Token: es, Calculated Log Prob: -1.1920928244535389e-07\n",
      "Token: ,, Calculated Log Prob: -0.000612071540672332\n",
      "Token: Ex, Calculated Log Prob: -0.00035649145138449967\n",
      "Token: ponents, Calculated Log Prob: -0.00023374208831228316\n",
      "Token: ,, Calculated Log Prob: -5.924526340095326e-05\n",
      "Token: Multi, Calculated Log Prob: -0.0013487775577232242\n",
      "Token: plication, Calculated Log Prob: -5.411955135059543e-05\n",
      "Token: and, Calculated Log Prob: -0.00014697425649501383\n",
      "Token: Division, Calculated Log Prob: -0.001856866991147399\n",
      "Token: (, Calculated Log Prob: -0.0004857790481764823\n",
      "Token: from, Calculated Log Prob: -0.002141327131539583\n",
      "Token: left, Calculated Log Prob: -0.0003987947420682758\n",
      "Token: to, Calculated Log Prob: -5.125986263010418e-06\n",
      "Token: right, Calculated Log Prob: -1.5020257706055418e-05\n",
      "Token: ),, Calculated Log Prob: -5.8412379075889476e-06\n",
      "Token: and, Calculated Log Prob: -0.6937279105186462\n",
      "Token: Add, Calculated Log Prob: -0.004340869374573231\n",
      "Token: ition, Calculated Log Prob: -1.0609570381348021e-05\n",
      "Token: and, Calculated Log Prob: -1.1920922133867862e-06\n",
      "Token: Sub, Calculated Log Prob: -9.262132516596466e-05\n",
      "Token: tr, Calculated Log Prob: -6.97350042173639e-05\n",
      "Token: action, Calculated Log Prob: -1.537788011773955e-05\n",
      "Token: (, Calculated Log Prob: -2.8729025871143676e-05\n",
      "Token: from, Calculated Log Prob: -0.00012432756193447858\n",
      "Token: left, Calculated Log Prob: -6.639736966462806e-05\n",
      "Token: to, Calculated Log Prob: -2.0265558760002023e-06\n",
      "Token: right, Calculated Log Prob: -0.00020811775175388902\n",
      "Token: )., Calculated Log Prob: -0.00010680581908673048\n",
      "Token: \n",
      ", Calculated Log Prob: -0.6468018889427185\n",
      "Token: \n",
      ", Calculated Log Prob: -2.1457441107486375e-05\n",
      "Token: In, Calculated Log Prob: -0.618139386177063\n",
      "Token: this, Calculated Log Prob: -0.023854751139879227\n",
      "Token: case, Calculated Log Prob: -0.4372518062591553\n",
      "Token: ,, Calculated Log Prob: -0.00036566724884323776\n",
      "Token: there, Calculated Log Prob: -0.39330118894577026\n",
      "Token: are, Calculated Log Prob: -0.000668659748043865\n",
      "Token: no, Calculated Log Prob: -0.0053164055570960045\n",
      "Token: parent, Calculated Log Prob: -0.0011879300000146031\n",
      "Token: hes, Calculated Log Prob: -4.768370445162873e-07\n",
      "Token: es, Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: or, Calculated Log Prob: -0.160736545920372\n",
      "Token: ex, Calculated Log Prob: -0.00021324267436284572\n",
      "Token: ponents, Calculated Log Prob: -1.6927575416048057e-05\n",
      "Token: ,, Calculated Log Prob: -0.03564948961138725\n",
      "Token: so, Calculated Log Prob: -0.270198255777359\n",
      "Token: we, Calculated Log Prob: -0.006973688956350088\n",
      "Token: move, Calculated Log Prob: -0.475134015083313\n",
      "Token: on, Calculated Log Prob: -0.019302973523736\n",
      "Token: to, Calculated Log Prob: -4.255681051290594e-05\n",
      "Token: the, Calculated Log Prob: -0.28788992762565613\n",
      "Token: next, Calculated Log Prob: -0.02397824265062809\n",
      "Token: step, Calculated Log Prob: -0.37029916048049927\n",
      "Token: ., Calculated Log Prob: -0.5532752871513367\n",
      "Token: \n",
      ", Calculated Log Prob: -0.8229117393493652\n",
      "Token: \n",
      ", Calculated Log Prob: -0.0002873722987715155\n",
      "Token: Step, Calculated Log Prob: -1.7682939767837524\n",
      "Token: , Calculated Log Prob: -3.814689989667386e-06\n",
      "Token: 1, Calculated Log Prob: -4.768370445162873e-07\n",
      "Token: :, Calculated Log Prob: -0.0011062461417168379\n",
      "Token: Per, Calculated Log Prob: -0.7080928087234497\n",
      "Token: form, Calculated Log Prob: -3.2186455882765586e-06\n",
      "Token: the, Calculated Log Prob: -0.6116694808006287\n",
      "Token: multi, Calculated Log Prob: -0.001033129869028926\n",
      "Token: plication, Calculated Log Prob: -1.9073468138230965e-06\n",
      "Token: \n",
      ", Calculated Log Prob: -1.135597825050354\n",
      "Token: 1, Calculated Log Prob: -0.03407423570752144\n",
      "Token: 0, Calculated Log Prob: -2.3841855067985307e-07\n",
      "Token: *, Calculated Log Prob: -2.264974000354414e-06\n",
      "Token: , Calculated Log Prob: -1.1920928244535389e-07\n",
      "Token: 2, Calculated Log Prob: 0.0\n",
      "Token: =, Calculated Log Prob: -5.185469490243122e-05\n",
      "Token: , Calculated Log Prob: -0.1006975769996643\n",
      "Token: 2, Calculated Log Prob: -0.0011789998970925808\n",
      "Token: 0, Calculated Log Prob: 0.0\n",
      "Token: \n",
      ", Calculated Log Prob: -0.0002661589242052287\n",
      "Token: \n",
      ", Calculated Log Prob: -0.004430953413248062\n",
      "Token: Step, Calculated Log Prob: -0.024352001026272774\n",
      "Token: , Calculated Log Prob: -1.1920928244535389e-07\n",
      "Token: 2, Calculated Log Prob: -1.5497195136049413e-06\n",
      "Token: :, Calculated Log Prob: -1.5258672647178173e-05\n",
      "Token: Per, Calculated Log Prob: -0.10829969495534897\n",
      "Token: form, Calculated Log Prob: -3.4570634852570947e-06\n",
      "Token: the, Calculated Log Prob: -0.002840296132490039\n",
      "Token: addition, Calculated Log Prob: -0.00017438798386137933\n",
      "Token: \n",
      ", Calculated Log Prob: -0.002940618433058262\n",
      "Token: 9, Calculated Log Prob: -0.00011920218094019219\n",
      "Token: +, Calculated Log Prob: -2.3603161025675945e-05\n",
      "Token: , Calculated Log Prob: -1.0728830375228426e-06\n",
      "Token: 2, Calculated Log Prob: -3.576278118089249e-07\n",
      "Token: 0, Calculated Log Prob: 0.0\n",
      "Token: =, Calculated Log Prob: -0.3138502538204193\n",
      "Token: , Calculated Log Prob: -0.00019274283840786666\n",
      "Token: 2, Calculated Log Prob: -0.00012981049076188356\n",
      "Token: 9, Calculated Log Prob: -2.145764938177308e-06\n",
      "Token: \n",
      ", Calculated Log Prob: -0.0004970983718521893\n",
      "Token: \n",
      ", Calculated Log Prob: -6.723177648382261e-05\n",
      "Token: So, Calculated Log Prob: -0.10272379964590073\n",
      "Token: ,, Calculated Log Prob: -0.5242929458618164\n",
      "Token: the, Calculated Log Prob: -0.005475286394357681\n",
      "Token: final, Calculated Log Prob: -0.5134934186935425\n",
      "Token: answer, Calculated Log Prob: -0.00016115796461235732\n",
      "Token: is, Calculated Log Prob: -0.022505005821585655\n",
      "Token: \\, Calculated Log Prob: -0.07454096525907516\n",
      "Token: box, Calculated Log Prob: -1.7165990357170813e-05\n",
      "Token: ed, Calculated Log Prob: -1.1920922133867862e-06\n",
      "Token: {, Calculated Log Prob: -8.940656698541716e-06\n",
      "Token: 2, Calculated Log Prob: -1.9907753085135482e-05\n",
      "Token: 9, Calculated Log Prob: 0.0\n",
      "Token: }., Calculated Log Prob: -0.00030989613151177764\n"
     ]
    }
   ],
   "source": [
    "# Load your model and tokenizer\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "# Generate text and scores, and calculate log probabilities\n",
    "question = \"What is 9 + 10 * 2?\"\n",
    "input_text, generated_text, generated_scores = generate_text_with_input_and_scores(question, model, tokenizer)\n",
    "log_probs = to_tokens_and_logprobs(model, tokenizer, input_text, generated_text)\n",
    "\n",
    "# Print the results and compare\n",
    "print(\"Generated Text Log Probabilities:\")\n",
    "for i, (token, score) in enumerate(zip(tokenizer.tokenize(generated_text), generated_scores)):\n",
    "    print(f\"Token: {token}, Generated Log Prob: {score.item()}\")\n",
    "\n",
    "print(\"\\nCalculated Log Probabilities from Concatenated Text:\")\n",
    "for sequence in log_probs:\n",
    "    for token, log_prob in sequence:\n",
    "        print(f\"Token: {token}, Calculated Log Prob: {log_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Text Token IDs: tensor([[    1,   733, 16289, 28793,  1824,   349, 28705, 28774,   648, 28705,\n",
      "         28740, 28734,   398, 28705, 28750, 28804,    13, 12069,  2611,  3707,\n",
      "           486,  3707, 28725,   304,  1658,   574,  1480,  4372,  2373,   414,\n",
      "          2858,   286, 28751,  2051,   733, 28748, 16289, 28793,  1551, 12049,\n",
      "           272,  5782, 28705, 28774,   648, 28705, 28740, 28734,   398, 28705,\n",
      "         28750, 28725,   478,   927,   298,  1372,   272,  1745,   302,  6933,\n",
      "         28725,   690,   349,  2608, 10216,   486,   272,  1183,  1689,  1082,\n",
      "         21025,  4915,  2109, 28747, 18712,  2053,   274, 28725,  1529,  6445,\n",
      "         28725, 18317,  2459,   304,  8618,   325,  3211,  1749,   298,  1103,\n",
      "           557,   304,  3301,   685,   304,  5078,   434,  1774,   325,  3211,\n",
      "          1749,   298,  1103,   609,    13,    13,   657,   456,  1222, 28725,\n",
      "           736,   460,   708,  2564,  2053,   274,   442,   439,  6445, 28725,\n",
      "           579,   478,  2318,   356,   298,   272,  1679,  3707, 28723,    13,\n",
      "            13,  9977, 28705, 28740, 28747,  2744,   674,   272,  6079,  2459,\n",
      "            13, 28740, 28734,   398, 28705, 28750,   327, 28705, 28750, 28734,\n",
      "            13,    13,  9977, 28705, 28750, 28747,  2744,   674,   272,  4518,\n",
      "            13, 28774,   648, 28705, 28750, 28734,   327, 28705, 28750, 28774,\n",
      "            13,    13,  5142, 28725,   272,  1480,  4372,   349,   414,  2858,\n",
      "           286, 28751, 28750, 28774,  2051]], device='mps:0')\n",
      "Token: [, ID: 733, Log Prob: -9.379661560058594\n",
      "Token: INST, ID: 16289, Log Prob: -11.709663391113281\n",
      "Token: ], ID: 28793, Log Prob: -12.90582275390625\n",
      "Token: What, ID: 1824, Log Prob: -13.26082992553711\n",
      "Token: is, ID: 349, Log Prob: -0.6441332697868347\n",
      "Token: , ID: 28705, Log Prob: -7.122815132141113\n",
      "Token: 9, ID: 28774, Log Prob: -3.511679172515869\n",
      "Token: +, ID: 648, Log Prob: -5.391874313354492\n",
      "Token: , ID: 28705, Log Prob: -0.23111653327941895\n",
      "Token: 1, ID: 28740, Log Prob: -1.1535981893539429\n",
      "Token: 0, ID: 28734, Log Prob: -1.4491122961044312\n",
      "Token: *, ID: 398, Log Prob: -1.6907577514648438\n",
      "Token: , ID: 28705, Log Prob: -0.11482894420623779\n",
      "Token: 2, ID: 28750, Log Prob: -0.3742263615131378\n",
      "Token: ?, ID: 28804, Log Prob: -0.12354228645563126\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.2805356979370117\n",
      "Token: Please, ID: 12069, Log Prob: -9.997407913208008\n",
      "Token: reason, ID: 2611, Log Prob: -12.95058536529541\n",
      "Token: step, ID: 3707, Log Prob: -5.26135778427124\n",
      "Token: by, ID: 486, Log Prob: -0.062126148492097855\n",
      "Token: step, ID: 3707, Log Prob: -7.497983460780233e-05\n",
      "Token: ,, ID: 28725, Log Prob: -2.7327542304992676\n",
      "Token: and, ID: 304, Log Prob: -3.515874147415161\n",
      "Token: put, ID: 1658, Log Prob: -8.407917022705078\n",
      "Token: your, ID: 574, Log Prob: -4.023624897003174\n",
      "Token: final, ID: 1480, Log Prob: -4.041463851928711\n",
      "Token: answer, ID: 4372, Log Prob: -0.032512418925762177\n",
      "Token: within, ID: 2373, Log Prob: -7.389629364013672\n",
      "Token: \\, ID: 414, Log Prob: -12.62710952758789\n",
      "Token: box, ID: 2858, Log Prob: -4.384269714355469\n",
      "Token: ed, ID: 286, Log Prob: -0.00013183678674977273\n",
      "Token: {, ID: 28751, Log Prob: -0.07179071754217148\n",
      "Token: }., ID: 2051, Log Prob: -0.8477483987808228\n",
      "Token: [, ID: 733, Log Prob: -13.785758972167969\n",
      "Token: /, ID: 28748, Log Prob: -15.247478485107422\n",
      "Token: INST, ID: 16289, Log Prob: -10.707427978515625\n",
      "Token: ], ID: 28793, Log Prob: -11.843785285949707\n",
      "Token: To, ID: 1551, Log Prob: -7.325741767883301\n",
      "Token: solve, ID: 12049, Log Prob: -0.03880668058991432\n",
      "Token: the, ID: 272, Log Prob: -0.2520483732223511\n",
      "Token: expression, ID: 5782, Log Prob: -0.0005380851798690856\n",
      "Token: , ID: 28705, Log Prob: -0.0032298800069838762\n",
      "Token: 9, ID: 28774, Log Prob: -0.000324673397699371\n",
      "Token: +, ID: 648, Log Prob: -2.4199192921514623e-05\n",
      "Token: , ID: 28705, Log Prob: -2.3841855067985307e-07\n",
      "Token: 1, ID: 28740, Log Prob: -2.3841855067985307e-07\n",
      "Token: 0, ID: 28734, Log Prob: 0.0\n",
      "Token: *, ID: 398, Log Prob: -3.635817120084539e-05\n",
      "Token: , ID: 28705, Log Prob: 0.0\n",
      "Token: 2, ID: 28750, Log Prob: -5.960462772236497e-07\n",
      "Token: ,, ID: 28725, Log Prob: -0.002832807367667556\n",
      "Token: we, ID: 478, Log Prob: -0.0161511842161417\n",
      "Token: need, ID: 927, Log Prob: -0.04779325798153877\n",
      "Token: to, ID: 298, Log Prob: -3.4570634852570947e-06\n",
      "Token: follow, ID: 1372, Log Prob: -0.01019272394478321\n",
      "Token: the, ID: 272, Log Prob: -1.3828182090946939e-05\n",
      "Token: order, ID: 1745, Log Prob: -0.003366281511262059\n",
      "Token: of, ID: 302, Log Prob: -4.768370445162873e-07\n",
      "Token: operations, ID: 6933, Log Prob: -0.0008238735608756542\n",
      "Token: ,, ID: 28725, Log Prob: -0.07052040845155716\n",
      "Token: which, ID: 690, Log Prob: -0.3726021945476532\n",
      "Token: is, ID: 349, Log Prob: -0.000287729810224846\n",
      "Token: often, ID: 2608, Log Prob: -0.6218234896659851\n",
      "Token: remembered, ID: 10216, Log Prob: -0.03447694703936577\n",
      "Token: by, ID: 486, Log Prob: -0.014452654868364334\n",
      "Token: the, ID: 272, Log Prob: -4.017272294731811e-05\n",
      "Token: ac, ID: 1183, Log Prob: -0.0005884823040105402\n",
      "Token: ron, ID: 1689, Log Prob: -5.960446742392378e-06\n",
      "Token: ym, ID: 1082, Log Prob: -0.00020251607929822057\n",
      "Token: PE, ID: 21025, Log Prob: -6.007967749610543e-05\n",
      "Token: MD, ID: 4915, Log Prob: -0.0001565095444675535\n",
      "Token: AS, ID: 2109, Log Prob: -5.602820692729438e-06\n",
      "Token: :, ID: 28747, Log Prob: -0.019266139715909958\n",
      "Token: Parent, ID: 18712, Log Prob: -0.003140163142234087\n",
      "Token: hes, ID: 2053, Log Prob: -9.738924563862383e-05\n",
      "Token: es, ID: 274, Log Prob: -1.1920928244535389e-07\n",
      "Token: ,, ID: 28725, Log Prob: -0.000612071540672332\n",
      "Token: Ex, ID: 1529, Log Prob: -0.00035649145138449967\n",
      "Token: ponents, ID: 6445, Log Prob: -0.00023374208831228316\n",
      "Token: ,, ID: 28725, Log Prob: -5.924526340095326e-05\n",
      "Token: Multi, ID: 18317, Log Prob: -0.0013487775577232242\n",
      "Token: plication, ID: 2459, Log Prob: -5.411955135059543e-05\n",
      "Token: and, ID: 304, Log Prob: -0.00014697425649501383\n",
      "Token: Division, ID: 8618, Log Prob: -0.001856866991147399\n",
      "Token: (, ID: 325, Log Prob: -0.0004857790481764823\n",
      "Token: from, ID: 3211, Log Prob: -0.002141327131539583\n",
      "Token: left, ID: 1749, Log Prob: -0.0003987947420682758\n",
      "Token: to, ID: 298, Log Prob: -5.125986263010418e-06\n",
      "Token: right, ID: 1103, Log Prob: -1.5020257706055418e-05\n",
      "Token: ),, ID: 557, Log Prob: -5.8412379075889476e-06\n",
      "Token: and, ID: 304, Log Prob: -0.6937279105186462\n",
      "Token: Add, ID: 3301, Log Prob: -0.004340869374573231\n",
      "Token: ition, ID: 685, Log Prob: -1.0609570381348021e-05\n",
      "Token: and, ID: 304, Log Prob: -1.1920922133867862e-06\n",
      "Token: Sub, ID: 5078, Log Prob: -9.262132516596466e-05\n",
      "Token: tr, ID: 434, Log Prob: -6.97350042173639e-05\n",
      "Token: action, ID: 1774, Log Prob: -1.537788011773955e-05\n",
      "Token: (, ID: 325, Log Prob: -2.8729025871143676e-05\n",
      "Token: from, ID: 3211, Log Prob: -0.00012432756193447858\n",
      "Token: left, ID: 1749, Log Prob: -6.639736966462806e-05\n",
      "Token: to, ID: 298, Log Prob: -2.0265558760002023e-06\n",
      "Token: right, ID: 1103, Log Prob: -0.00020811775175388902\n",
      "Token: )., ID: 609, Log Prob: -0.00010680581908673048\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.6468018889427185\n",
      "Token: \n",
      ", ID: 13, Log Prob: -2.1457441107486375e-05\n",
      "Token: In, ID: 657, Log Prob: -0.618139386177063\n",
      "Token: this, ID: 456, Log Prob: -0.023854751139879227\n",
      "Token: case, ID: 1222, Log Prob: -0.4372518062591553\n",
      "Token: ,, ID: 28725, Log Prob: -0.00036566724884323776\n",
      "Token: there, ID: 736, Log Prob: -0.39330118894577026\n",
      "Token: are, ID: 460, Log Prob: -0.000668659748043865\n",
      "Token: no, ID: 708, Log Prob: -0.0053164055570960045\n",
      "Token: parent, ID: 2564, Log Prob: -0.0011879300000146031\n",
      "Token: hes, ID: 2053, Log Prob: -4.768370445162873e-07\n",
      "Token: es, ID: 274, Log Prob: -2.3841855067985307e-07\n",
      "Token: or, ID: 442, Log Prob: -0.160736545920372\n",
      "Token: ex, ID: 439, Log Prob: -0.00021324267436284572\n",
      "Token: ponents, ID: 6445, Log Prob: -1.6927575416048057e-05\n",
      "Token: ,, ID: 28725, Log Prob: -0.03564948961138725\n",
      "Token: so, ID: 579, Log Prob: -0.270198255777359\n",
      "Token: we, ID: 478, Log Prob: -0.006973688956350088\n",
      "Token: move, ID: 2318, Log Prob: -0.475134015083313\n",
      "Token: on, ID: 356, Log Prob: -0.019302973523736\n",
      "Token: to, ID: 298, Log Prob: -4.255681051290594e-05\n",
      "Token: the, ID: 272, Log Prob: -0.28788992762565613\n",
      "Token: next, ID: 1679, Log Prob: -0.02397824265062809\n",
      "Token: step, ID: 3707, Log Prob: -0.37029916048049927\n",
      "Token: ., ID: 28723, Log Prob: -0.5532752871513367\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.8229117393493652\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.0002873722987715155\n",
      "Token: Step, ID: 9977, Log Prob: -1.7682939767837524\n",
      "Token: , ID: 28705, Log Prob: -3.814689989667386e-06\n",
      "Token: 1, ID: 28740, Log Prob: -4.768370445162873e-07\n",
      "Token: :, ID: 28747, Log Prob: -0.0011062461417168379\n",
      "Token: Per, ID: 2744, Log Prob: -0.7080928087234497\n",
      "Token: form, ID: 674, Log Prob: -3.2186455882765586e-06\n",
      "Token: the, ID: 272, Log Prob: -0.6116694808006287\n",
      "Token: multi, ID: 6079, Log Prob: -0.001033129869028926\n",
      "Token: plication, ID: 2459, Log Prob: -1.9073468138230965e-06\n",
      "Token: \n",
      ", ID: 13, Log Prob: -1.135597825050354\n",
      "Token: 1, ID: 28740, Log Prob: -0.03407423570752144\n",
      "Token: 0, ID: 28734, Log Prob: -2.3841855067985307e-07\n",
      "Token: *, ID: 398, Log Prob: -2.264974000354414e-06\n",
      "Token: , ID: 28705, Log Prob: -1.1920928244535389e-07\n",
      "Token: 2, ID: 28750, Log Prob: 0.0\n",
      "Token: =, ID: 327, Log Prob: -5.185469490243122e-05\n",
      "Token: , ID: 28705, Log Prob: -0.1006975769996643\n",
      "Token: 2, ID: 28750, Log Prob: -0.0011789998970925808\n",
      "Token: 0, ID: 28734, Log Prob: 0.0\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.0002661589242052287\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.004430953413248062\n",
      "Token: Step, ID: 9977, Log Prob: -0.024352001026272774\n",
      "Token: , ID: 28705, Log Prob: -1.1920928244535389e-07\n",
      "Token: 2, ID: 28750, Log Prob: -1.5497195136049413e-06\n",
      "Token: :, ID: 28747, Log Prob: -1.5258672647178173e-05\n",
      "Token: Per, ID: 2744, Log Prob: -0.10829969495534897\n",
      "Token: form, ID: 674, Log Prob: -3.4570634852570947e-06\n",
      "Token: the, ID: 272, Log Prob: -0.002840296132490039\n",
      "Token: addition, ID: 4518, Log Prob: -0.00017438798386137933\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.002940618433058262\n",
      "Token: 9, ID: 28774, Log Prob: -0.00011920218094019219\n",
      "Token: +, ID: 648, Log Prob: -2.3603161025675945e-05\n",
      "Token: , ID: 28705, Log Prob: -1.0728830375228426e-06\n",
      "Token: 2, ID: 28750, Log Prob: -3.576278118089249e-07\n",
      "Token: 0, ID: 28734, Log Prob: 0.0\n",
      "Token: =, ID: 327, Log Prob: -0.3138502538204193\n",
      "Token: , ID: 28705, Log Prob: -0.00019274283840786666\n",
      "Token: 2, ID: 28750, Log Prob: -0.00012981049076188356\n",
      "Token: 9, ID: 28774, Log Prob: -2.145764938177308e-06\n",
      "Token: \n",
      ", ID: 13, Log Prob: -0.0004970983718521893\n",
      "Token: \n",
      ", ID: 13, Log Prob: -6.723177648382261e-05\n",
      "Token: So, ID: 5142, Log Prob: -0.10272379964590073\n",
      "Token: ,, ID: 28725, Log Prob: -0.5242929458618164\n",
      "Token: the, ID: 272, Log Prob: -0.005475286394357681\n",
      "Token: final, ID: 1480, Log Prob: -0.5134934186935425\n",
      "Token: answer, ID: 4372, Log Prob: -0.00016115796461235732\n",
      "Token: is, ID: 349, Log Prob: -0.022505005821585655\n",
      "Token: \\, ID: 414, Log Prob: -0.07454096525907516\n",
      "Token: box, ID: 2858, Log Prob: -1.7165990357170813e-05\n",
      "Token: ed, ID: 286, Log Prob: -1.1920922133867862e-06\n",
      "Token: {, ID: 28751, Log Prob: -8.940656698541716e-06\n",
      "Token: 2, ID: 28750, Log Prob: -1.9907753085135482e-05\n",
      "Token: 9, ID: 28774, Log Prob: 0.0\n",
      "Token: }., ID: 2051, Log Prob: -0.00030989613151177764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('[', -9.379661560058594),\n",
       "  ('INST', -11.709663391113281),\n",
       "  (']', -12.90582275390625),\n",
       "  ('What', -13.26082992553711),\n",
       "  ('is', -0.6441332697868347),\n",
       "  ('', -7.122815132141113),\n",
       "  ('9', -3.511679172515869),\n",
       "  ('+', -5.391874313354492),\n",
       "  ('', -0.23111653327941895),\n",
       "  ('1', -1.1535981893539429),\n",
       "  ('0', -1.4491122961044312),\n",
       "  ('*', -1.6907577514648438),\n",
       "  ('', -0.11482894420623779),\n",
       "  ('2', -0.3742263615131378),\n",
       "  ('?', -0.12354228645563126),\n",
       "  ('\\n', -0.2805356979370117),\n",
       "  ('Please', -9.997407913208008),\n",
       "  ('reason', -12.95058536529541),\n",
       "  ('step', -5.26135778427124),\n",
       "  ('by', -0.062126148492097855),\n",
       "  ('step', -7.497983460780233e-05),\n",
       "  (',', -2.7327542304992676),\n",
       "  ('and', -3.515874147415161),\n",
       "  ('put', -8.407917022705078),\n",
       "  ('your', -4.023624897003174),\n",
       "  ('final', -4.041463851928711),\n",
       "  ('answer', -0.032512418925762177),\n",
       "  ('within', -7.389629364013672),\n",
       "  ('\\\\', -12.62710952758789),\n",
       "  ('box', -4.384269714355469),\n",
       "  ('ed', -0.00013183678674977273),\n",
       "  ('{', -0.07179071754217148),\n",
       "  ('}.', -0.8477483987808228),\n",
       "  ('[', -13.785758972167969),\n",
       "  ('/', -15.247478485107422),\n",
       "  ('INST', -10.707427978515625),\n",
       "  (']', -11.843785285949707),\n",
       "  ('To', -7.325741767883301),\n",
       "  ('solve', -0.03880668058991432),\n",
       "  ('the', -0.2520483732223511),\n",
       "  ('expression', -0.0005380851798690856),\n",
       "  ('', -0.0032298800069838762),\n",
       "  ('9', -0.000324673397699371),\n",
       "  ('+', -2.4199192921514623e-05),\n",
       "  ('', -2.3841855067985307e-07),\n",
       "  ('1', -2.3841855067985307e-07),\n",
       "  ('0', 0.0),\n",
       "  ('*', -3.635817120084539e-05),\n",
       "  ('', 0.0),\n",
       "  ('2', -5.960462772236497e-07),\n",
       "  (',', -0.002832807367667556),\n",
       "  ('we', -0.0161511842161417),\n",
       "  ('need', -0.04779325798153877),\n",
       "  ('to', -3.4570634852570947e-06),\n",
       "  ('follow', -0.01019272394478321),\n",
       "  ('the', -1.3828182090946939e-05),\n",
       "  ('order', -0.003366281511262059),\n",
       "  ('of', -4.768370445162873e-07),\n",
       "  ('operations', -0.0008238735608756542),\n",
       "  (',', -0.07052040845155716),\n",
       "  ('which', -0.3726021945476532),\n",
       "  ('is', -0.000287729810224846),\n",
       "  ('often', -0.6218234896659851),\n",
       "  ('remembered', -0.03447694703936577),\n",
       "  ('by', -0.014452654868364334),\n",
       "  ('the', -4.017272294731811e-05),\n",
       "  ('ac', -0.0005884823040105402),\n",
       "  ('ron', -5.960446742392378e-06),\n",
       "  ('ym', -0.00020251607929822057),\n",
       "  ('PE', -6.007967749610543e-05),\n",
       "  ('MD', -0.0001565095444675535),\n",
       "  ('AS', -5.602820692729438e-06),\n",
       "  (':', -0.019266139715909958),\n",
       "  ('Parent', -0.003140163142234087),\n",
       "  ('hes', -9.738924563862383e-05),\n",
       "  ('es', -1.1920928244535389e-07),\n",
       "  (',', -0.000612071540672332),\n",
       "  ('Ex', -0.00035649145138449967),\n",
       "  ('ponents', -0.00023374208831228316),\n",
       "  (',', -5.924526340095326e-05),\n",
       "  ('Multi', -0.0013487775577232242),\n",
       "  ('plication', -5.411955135059543e-05),\n",
       "  ('and', -0.00014697425649501383),\n",
       "  ('Division', -0.001856866991147399),\n",
       "  ('(', -0.0004857790481764823),\n",
       "  ('from', -0.002141327131539583),\n",
       "  ('left', -0.0003987947420682758),\n",
       "  ('to', -5.125986263010418e-06),\n",
       "  ('right', -1.5020257706055418e-05),\n",
       "  ('),', -5.8412379075889476e-06),\n",
       "  ('and', -0.6937279105186462),\n",
       "  ('Add', -0.004340869374573231),\n",
       "  ('ition', -1.0609570381348021e-05),\n",
       "  ('and', -1.1920922133867862e-06),\n",
       "  ('Sub', -9.262132516596466e-05),\n",
       "  ('tr', -6.97350042173639e-05),\n",
       "  ('action', -1.537788011773955e-05),\n",
       "  ('(', -2.8729025871143676e-05),\n",
       "  ('from', -0.00012432756193447858),\n",
       "  ('left', -6.639736966462806e-05),\n",
       "  ('to', -2.0265558760002023e-06),\n",
       "  ('right', -0.00020811775175388902),\n",
       "  (').', -0.00010680581908673048),\n",
       "  ('\\n', -0.6468018889427185),\n",
       "  ('\\n', -2.1457441107486375e-05),\n",
       "  ('In', -0.618139386177063),\n",
       "  ('this', -0.023854751139879227),\n",
       "  ('case', -0.4372518062591553),\n",
       "  (',', -0.00036566724884323776),\n",
       "  ('there', -0.39330118894577026),\n",
       "  ('are', -0.000668659748043865),\n",
       "  ('no', -0.0053164055570960045),\n",
       "  ('parent', -0.0011879300000146031),\n",
       "  ('hes', -4.768370445162873e-07),\n",
       "  ('es', -2.3841855067985307e-07),\n",
       "  ('or', -0.160736545920372),\n",
       "  ('ex', -0.00021324267436284572),\n",
       "  ('ponents', -1.6927575416048057e-05),\n",
       "  (',', -0.03564948961138725),\n",
       "  ('so', -0.270198255777359),\n",
       "  ('we', -0.006973688956350088),\n",
       "  ('move', -0.475134015083313),\n",
       "  ('on', -0.019302973523736),\n",
       "  ('to', -4.255681051290594e-05),\n",
       "  ('the', -0.28788992762565613),\n",
       "  ('next', -0.02397824265062809),\n",
       "  ('step', -0.37029916048049927),\n",
       "  ('.', -0.5532752871513367),\n",
       "  ('\\n', -0.8229117393493652),\n",
       "  ('\\n', -0.0002873722987715155),\n",
       "  ('Step', -1.7682939767837524),\n",
       "  ('', -3.814689989667386e-06),\n",
       "  ('1', -4.768370445162873e-07),\n",
       "  (':', -0.0011062461417168379),\n",
       "  ('Per', -0.7080928087234497),\n",
       "  ('form', -3.2186455882765586e-06),\n",
       "  ('the', -0.6116694808006287),\n",
       "  ('multi', -0.001033129869028926),\n",
       "  ('plication', -1.9073468138230965e-06),\n",
       "  ('\\n', -1.135597825050354),\n",
       "  ('1', -0.03407423570752144),\n",
       "  ('0', -2.3841855067985307e-07),\n",
       "  ('*', -2.264974000354414e-06),\n",
       "  ('', -1.1920928244535389e-07),\n",
       "  ('2', 0.0),\n",
       "  ('=', -5.185469490243122e-05),\n",
       "  ('', -0.1006975769996643),\n",
       "  ('2', -0.0011789998970925808),\n",
       "  ('0', 0.0),\n",
       "  ('\\n', -0.0002661589242052287),\n",
       "  ('\\n', -0.004430953413248062),\n",
       "  ('Step', -0.024352001026272774),\n",
       "  ('', -1.1920928244535389e-07),\n",
       "  ('2', -1.5497195136049413e-06),\n",
       "  (':', -1.5258672647178173e-05),\n",
       "  ('Per', -0.10829969495534897),\n",
       "  ('form', -3.4570634852570947e-06),\n",
       "  ('the', -0.002840296132490039),\n",
       "  ('addition', -0.00017438798386137933),\n",
       "  ('\\n', -0.002940618433058262),\n",
       "  ('9', -0.00011920218094019219),\n",
       "  ('+', -2.3603161025675945e-05),\n",
       "  ('', -1.0728830375228426e-06),\n",
       "  ('2', -3.576278118089249e-07),\n",
       "  ('0', 0.0),\n",
       "  ('=', -0.3138502538204193),\n",
       "  ('', -0.00019274283840786666),\n",
       "  ('2', -0.00012981049076188356),\n",
       "  ('9', -2.145764938177308e-06),\n",
       "  ('\\n', -0.0004970983718521893),\n",
       "  ('\\n', -6.723177648382261e-05),\n",
       "  ('So', -0.10272379964590073),\n",
       "  (',', -0.5242929458618164),\n",
       "  ('the', -0.005475286394357681),\n",
       "  ('final', -0.5134934186935425),\n",
       "  ('answer', -0.00016115796461235732),\n",
       "  ('is', -0.022505005821585655),\n",
       "  ('\\\\', -0.07454096525907516),\n",
       "  ('box', -1.7165990357170813e-05),\n",
       "  ('ed', -1.1920922133867862e-06),\n",
       "  ('{', -8.940656698541716e-06),\n",
       "  ('2', -1.9907753085135482e-05),\n",
       "  ('9', 0.0),\n",
       "  ('}.', -0.00030989613151177764)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def debug_tokens_and_logprobs(model, tokenizer, input_text, generated_text):\n",
    "    full_text = input_text + generated_text\n",
    "    input_ids = tokenizer(full_text, padding=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    print(\"Full Text Token IDs:\", input_ids)\n",
    "    \n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "    # Adjust indices to ignore the first token's log prob as it corresponds to the second token\n",
    "    probs = probs[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    batch = []\n",
    "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
    "        text_sequence = []\n",
    "        for token, p in zip(input_sentence, input_probs):\n",
    "            decoded_token = tokenizer.decode([token])\n",
    "            print(f\"Token: {decoded_token}, ID: {token}, Log Prob: {p.item()}\")\n",
    "            if token not in tokenizer.all_special_ids:\n",
    "                text_sequence.append((decoded_token, p.item()))\n",
    "        batch.append(text_sequence)\n",
    "    return batch\n",
    "\n",
    "# Use this function to debug\n",
    "input_text, generated_text, _ = generate_text_with_input_and_scores(question, model, tokenizer)\n",
    "debug_tokens_and_logprobs(model, tokenizer, input_text, generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_logprobs(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question + \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"}\n",
    "    ]\n",
    "    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_tensor.to(model.device), max_new_tokens=1000, return_dict_in_generate=True, output_scores=True) # greedy sampling, should use do_sample for sampling\n",
    "    transition_scores = model.compute_transition_scores(\n",
    "        outputs.sequences, outputs.scores, normalize_logits=True\n",
    "    )\n",
    "\n",
    "    input_length = input_tensor.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    print(generated_tokens)\n",
    "    for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "        # | token | token string | log probability | probability\n",
    "        print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy()} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b \n",
      "tensor([[ 1791, 12049,   272,  5782, 28705, 28774,   648, 28705, 28740, 28734,\n",
      "           398, 28705, 28750, 28725,   478,   927,   298,  1372,   272,  1745,\n",
      "           302,  6933, 28725,   690,   349,  2608, 10216,   486,   272,  1183,\n",
      "          1689,  1082, 21025,  4915,  2109, 28747, 18712,  2053,   274, 28725,\n",
      "          1529,  6445, 28725, 18317,  2459,   304,  8618,   325,  3211,  1749,\n",
      "           298,  1103,   557,   304,  3301,   685,   304,  5078,   434,  1774,\n",
      "           325,  3211,  1749,   298,  1103,   609,    13,    13,   657,   456,\n",
      "          1222, 28725,   736,   460,   708,  2564,  2053,   274,   442,   439,\n",
      "          6445, 28725,   579,   478,  2318,   356,   298,   272,  1679,  3707,\n",
      "         28723,    13,    13,  9977, 28705, 28740, 28747,  2744,   674,   272,\n",
      "          6079,  2459,    13, 28740, 28734,   398, 28705, 28750,   327, 28705,\n",
      "         28750, 28734,    13,    13,  9977, 28705, 28750, 28747,  2744,   674,\n",
      "           272,  4518,    13, 28774,   648, 28705, 28750, 28734,   327, 28705,\n",
      "         28750, 28774,    13,    13,  5142, 28725,   272,  1480,  4372,   349,\n",
      "           414,  2858,   286, 28751, 28750, 28774,  2051,     2]],\n",
      "       device='mps:0')\n",
      "|  1791 | To       | -0.13854679465293884 | 87.06%\n",
      "| 12049 | solve    | -0.02403736300766468 | 97.62%\n",
      "|   272 | the      | -0.16032463312149048 | 85.19%\n",
      "|  5782 | expression | -0.0005321278586052358 | 99.95%\n",
      "| 28705 |          | -0.006042072083801031 | 99.40%\n",
      "| 28774 | 9        | -0.00033087024348787963 | 99.97%\n",
      "|   648 | +        | -3.0636318115284666e-05 | 100.00%\n",
      "| 28705 |          | -2.3841855067985307e-07 | 100.00%\n",
      "| 28740 | 1        | -1.1920928244535389e-07 | 100.00%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|   398 | *        | -4.1960789531003684e-05 | 100.00%\n",
      "| 28705 |          | 0.0 | 100.00%\n",
      "| 28750 | 2        | -5.960462772236497e-07 | 100.00%\n",
      "| 28725 | ,        | -0.0024573388509452343 | 99.75%\n",
      "|   478 | we       | -0.019182177260518074 | 98.10%\n",
      "|   927 | need     | -0.0333944633603096 | 96.72%\n",
      "|   298 | to       | -1.4305104514278355e-06 | 100.00%\n",
      "|  1372 | follow   | -0.008089871145784855 | 99.19%\n",
      "|   272 | the      | -1.3351351299206726e-05 | 100.00%\n",
      "|  1745 | order    | -0.004840560257434845 | 99.52%\n",
      "|   302 | of       | -3.576278118089249e-07 | 100.00%\n",
      "|  6933 | operations | -0.000809818331617862 | 99.92%\n",
      "| 28725 | ,        | -0.05047057941555977 | 95.08%\n",
      "|   690 | which    | -0.3874920606613159 | 67.88%\n",
      "|   349 | is       | -0.00029583368450403214 | 99.97%\n",
      "|  2608 | often    | -0.5905787348747253 | 55.40%\n",
      "| 10216 | remembered | -0.031841397285461426 | 96.87%\n",
      "|   486 | by       | -0.014357476495206356 | 98.57%\n",
      "|   272 | the      | -3.957670196541585e-05 | 100.00%\n",
      "|  1183 | ac       | -0.0006562701310031116 | 99.93%\n",
      "|  1689 | ron      | -5.245195097813848e-06 | 100.00%\n",
      "|  1082 | ym       | -0.00020168177434243262 | 99.98%\n",
      "| 21025 | PE       | -6.0437283536884934e-05 | 99.99%\n",
      "|  4915 | MD       | -0.0001408954558428377 | 99.99%\n",
      "|  2109 | AS       | -5.483612312673358e-06 | 100.00%\n",
      "| 28747 | :        | -0.016775256022810936 | 98.34%\n",
      "| 18712 | Parent   | -0.0033274304587394 | 99.67%\n",
      "|  2053 | hes      | -8.594620157964528e-05 | 99.99%\n",
      "|   274 | es       | -1.1920928244535389e-07 | 100.00%\n",
      "| 28725 | ,        | -0.0005404680850915611 | 99.95%\n",
      "|  1529 | Ex       | -0.0003573255962692201 | 99.96%\n",
      "|  6445 | ponents  | -0.00024029705673456192 | 99.98%\n",
      "| 28725 | ,        | -6.711257447022945e-05 | 99.99%\n",
      "| 18317 | Multi    | -0.0013487775577232242 | 99.87%\n",
      "|  2459 | plication | -5.5549986427649856e-05 | 99.99%\n",
      "|   304 | and      | -0.00012981049076188356 | 99.99%\n",
      "|  8618 | Division | -0.0016381428577005863 | 99.84%\n",
      "|   325 | (        | -0.0004058252670802176 | 99.96%\n",
      "|  3211 | from     | -0.002120153047144413 | 99.79%\n",
      "|  1749 | left     | -0.0003163314249832183 | 99.97%\n",
      "|   298 | to       | -4.291525328881107e-06 | 100.00%\n",
      "|  1103 | right    | -1.3351351299206726e-05 | 100.00%\n",
      "|   557 | ),       | -5.364403477869928e-06 | 100.00%\n",
      "|   304 | and      | -0.5764424204826355 | 56.19%\n",
      "|  3301 | Add      | -0.004477238282561302 | 99.55%\n",
      "|   685 | ition    | -9.417489309271332e-06 | 100.00%\n",
      "|   304 | and      | -8.344646857949556e-07 | 100.00%\n",
      "|  5078 | Sub      | -9.238292841473594e-05 | 99.99%\n",
      "|   434 | tr       | -6.97350042173639e-05 | 99.99%\n",
      "|  1774 | action   | -1.4066597032069694e-05 | 100.00%\n",
      "|   325 | (        | -1.7523612768854946e-05 | 100.00%\n",
      "|  3211 | from     | -0.00010966652916977182 | 99.99%\n",
      "|  1749 | left     | -5.757642793469131e-05 | 99.99%\n",
      "|   298 | to       | -2.145764938177308e-06 | 100.00%\n",
      "|  1103 | right    | -0.00022075122979003936 | 99.98%\n",
      "|   609 | ).       | -8.713819261174649e-05 | 99.99%\n",
      "|    13 | \n",
      "        | -0.2187599241733551 | 80.35%\n",
      "|    13 | \n",
      "        | -1.0132738680113107e-05 | 100.00%\n",
      "|   657 | In       | -0.4523194134235382 | 63.62%\n",
      "|   456 | this     | -0.023332329466938972 | 97.69%\n",
      "|  1222 | case     | -0.4360274374485016 | 64.66%\n",
      "| 28725 | ,        | -0.00017677174764685333 | 99.98%\n",
      "|   736 | there    | -0.3525223135948181 | 70.29%\n",
      "|   460 | are      | -0.0004538459761533886 | 99.95%\n",
      "|   708 | no       | -0.003657558234408498 | 99.63%\n",
      "|  2564 | parent   | -0.0011724510695785284 | 99.88%\n",
      "|  2053 | hes      | -4.768370445162873e-07 | 100.00%\n",
      "|   274 | es       | -2.3841855067985307e-07 | 100.00%\n",
      "|   442 | or       | -0.14292693138122559 | 86.68%\n",
      "|   439 | ex       | -0.00019667598826345056 | 99.98%\n",
      "|  6445 | ponents  | -1.490105023549404e-05 | 100.00%\n",
      "| 28725 | ,        | -0.026885611936450005 | 97.35%\n",
      "|   579 | so       | -0.2400268316268921 | 78.66%\n",
      "|   478 | we       | -0.004767004866153002 | 99.52%\n",
      "|  2318 | move     | -0.43828341364860535 | 64.51%\n",
      "|   356 | on       | -0.016059335321187973 | 98.41%\n",
      "|   298 | to       | -2.3245540432981215e-05 | 100.00%\n",
      "|   272 | the      | -0.30865201354026794 | 73.44%\n",
      "|  1679 | next     | -0.02343294583261013 | 97.68%\n",
      "|  3707 | step     | -0.29568472504615784 | 74.40%\n",
      "| 28723 | .        | -0.4083835482597351 | 66.47%\n",
      "|    13 | \n",
      "        | -1.0216823816299438 | 36.00%\n",
      "|    13 | \n",
      "        | -0.00015054999676067382 | 99.98%\n",
      "|  9977 | Step     | -1.4552913904190063 | 23.33%\n",
      "| 28705 |          | -2.145764938177308e-06 | 100.00%\n",
      "| 28740 | 1        | -3.576278118089249e-07 | 100.00%\n",
      "| 28747 | :        | -0.0006897454732097685 | 99.93%\n",
      "|  2744 | Per      | -0.5875695943832397 | 55.57%\n",
      "|   674 | form     | -3.2186455882765586e-06 | 100.00%\n",
      "|   272 | the      | -0.6084904670715332 | 54.42%\n",
      "|  6079 | multi    | -0.0008191090892069042 | 99.92%\n",
      "|  2459 | plication | -1.6689286894688848e-06 | 100.00%\n",
      "|    13 | \n",
      "        | -1.1190662384033203 | 32.66%\n",
      "| 28740 | 1        | -0.01619153469800949 | 98.39%\n",
      "| 28734 | 0        | -1.1920928244535389e-07 | 100.00%\n",
      "|   398 | *        | -1.4305104514278355e-06 | 100.00%\n",
      "| 28705 |          | -1.1920928244535389e-07 | 100.00%\n",
      "| 28750 | 2        | 0.0 | 100.00%\n",
      "|   327 | =        | -2.8132995794294402e-05 | 100.00%\n",
      "| 28705 |          | -0.08925306797027588 | 91.46%\n",
      "| 28750 | 2        | -0.0010765953920781612 | 99.89%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|    13 | \n",
      "        | -0.00010716341057559475 | 99.99%\n",
      "|    13 | \n",
      "        | -0.001634096377529204 | 99.84%\n",
      "|  9977 | Step     | -0.016278797760605812 | 98.39%\n",
      "| 28705 |          | 0.0 | 100.00%\n",
      "| 28750 | 2        | -1.1920922133867862e-06 | 100.00%\n",
      "| 28747 | :        | -1.2516897186287679e-05 | 100.00%\n",
      "|  2744 | Per      | -0.11129377782344818 | 89.47%\n",
      "|   674 | form     | -3.099436753473128e-06 | 100.00%\n",
      "|   272 | the      | -0.0020919600501656532 | 99.79%\n",
      "|  4518 | addition | -0.00014745102089364082 | 99.99%\n",
      "|    13 | \n",
      "        | -0.0026129886973649263 | 99.74%\n",
      "| 28774 | 9        | -5.07818695041351e-05 | 99.99%\n",
      "|   648 | +        | -1.2874520507466514e-05 | 100.00%\n",
      "| 28705 |          | -7.152555099310121e-07 | 100.00%\n",
      "| 28750 | 2        | -2.3841855067985307e-07 | 100.00%\n",
      "| 28734 | 0        | 0.0 | 100.00%\n",
      "|   327 | =        | -0.2814255356788635 | 75.47%\n",
      "| 28705 |          | -0.00013290952483657748 | 99.99%\n",
      "| 28750 | 2        | -0.00012468514614738524 | 99.99%\n",
      "| 28774 | 9        | -2.0265558760002023e-06 | 100.00%\n",
      "|    13 | \n",
      "        | -0.00028034092974849045 | 99.97%\n",
      "|    13 | \n",
      "        | -1.823885577323381e-05 | 100.00%\n",
      "|  5142 | So       | -0.0939418226480484 | 91.03%\n",
      "| 28725 | ,        | -0.576753556728363 | 56.17%\n",
      "|   272 | the      | -0.005017783492803574 | 99.50%\n",
      "|  1480 | final    | -0.45765984058380127 | 63.28%\n",
      "|  4372 | answer   | -0.00012611546844709665 | 99.99%\n",
      "|   349 | is       | -0.0141821363940835 | 98.59%\n",
      "|   414 | \\        | -0.05882381275296211 | 94.29%\n",
      "|  2858 | box      | -1.5735502529423684e-05 | 100.00%\n",
      "|   286 | ed       | -7.152555099310121e-07 | 100.00%\n",
      "| 28751 | {        | -5.960446742392378e-06 | 100.00%\n",
      "| 28750 | 2        | -9.179073458653875e-06 | 100.00%\n",
      "| 28774 | 9        | -1.1920928244535389e-07 | 100.00%\n",
      "|  2051 | }.       | -0.00015507926582358778 | 99.98%\n",
      "|     2 | </s>     | -9.095254790736362e-05 | 99.99%\n"
     ]
    }
   ],
   "source": [
    "query_logprobs(\"What is 9 + 10 * 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def to_tokens_and_logprobs(model, tokenizer, input_texts):\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "\n",
    "    # Generate outputs from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Calculate log probabilities using log softmax\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Extract log probabilities for the actual tokens\n",
    "    shifted_input_ids = input_ids[:, 1:]  # Shift input ids to the right to align with logits\n",
    "    shifted_log_probs = log_probs[:, :-1]  # Align logits with shifted input ids\n",
    "\n",
    "    # Gather the log probabilities for each token in the input\n",
    "    gathered_log_probs = torch.gather(shifted_log_probs, 2, shifted_input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Prepare output\n",
    "    batch_results = []\n",
    "    for i, sentence_log_probs in enumerate(gathered_log_probs):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(shifted_input_ids[i])\n",
    "        token_log_probs = [(token, log_prob.item()) for token, log_prob in zip(tokens, sentence_log_probs)]\n",
    "        batch_results.append(token_log_probs)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "# Example usage\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "input_text = \"Example text to process.\"\n",
    "results = to_tokens_and_logprobs(model, tokenizer, [input_text])\n",
    "for token, log_prob in results[0]:\n",
    "    print(f\"Token: {token}, Log Prob: {log_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_token_log_probs(input_text: str, model, tokenizer) -> torch.Tensor:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    # Move input ids to the same device as the model\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    # Generate logits from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Calculate log probabilities using log softmax\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Gather the log probabilities for each token in the input\n",
    "    # We use torch.gather here to pick out the log probabilities of the actual tokens\n",
    "    token_log_probs = torch.gather(log_probs, 2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return token_log_probs\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "input_text = \"Example text to process.\"\n",
    "log_probs = get_token_log_probs(input_text, model, tokenizer)\n",
    "\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "for token, log_prob in zip(tokens, log_probs[0]):\n",
    "    print(f\"Token: {token}, Log Prob: {log_prob.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, StoppingCriteriaList\n",
    "\n",
    "def generate_text_with_log_probs(input_text: str, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "\n",
    "    # Set up generation arguments\n",
    "    logits_processor = LogitsProcessorList()\n",
    "    stopping_criteria = StoppingCriteriaList()\n",
    "\n",
    "    # Generate text with log probabilities\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        logits_processor=logits_processor,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    # Extract log probabilities from the output\n",
    "    scores = output_sequences.scores\n",
    "    log_probs = [torch.nn.functional.log_softmax(score, dim=-1) for score in scores]\n",
    "    token_log_probs = [torch.gather(log_prob, 1, output_sequences.sequences[:, i+1].unsqueeze(-1)).squeeze(-1) for i, log_prob in enumerate(log_probs)]\n",
    "\n",
    "    return token_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "input_text = \"Example text to process.\"\n",
    "\n",
    "# Get log probabilities using the defined function\n",
    "calculated_log_probs = get_token_log_probs(input_text, model, tokenizer)\n",
    "\n",
    "# Generate text and get log probabilities during generation\n",
    "generated_log_probs = generate_text_with_log_probs(input_text, model, tokenizer)\n",
    "\n",
    "# Compare the results\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "for token, calc_log_prob, gen_log_prob in zip(tokens, calculated_log_probs[0], generated_log_probs):\n",
    "    print(f\"Token: {token}, Calculated Log Prob: {calc_log_prob.item()}, Generated Log Prob: {gen_log_prob.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsuccessful tinkering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def tokenize_sample_answers(tokenizer, prefix_text: str, suffixes: List[str], padding_value: int = 0):\n",
    "    \"\"\"Create tensor of tokenized suffixes\n",
    "\n",
    "    Args:\n",
    "        tokenizer: tokenizer.\n",
    "        prefix_text: text kept constant that precedes the suffixes\n",
    "        suffixes: List of answers to consider for given prefix_text\n",
    "        padding_value: padding token to make suffix input ids equal length\n",
    "    \"\"\"\n",
    "    combined_texts = [prefix_text + suffix for suffix in suffixes]\n",
    "    full_tokenized = tokenizer(combined_texts, add_special_tokens=True, padding=True, return_tensors=\"pt\")\n",
    "    print(\"full shape: \", full_tokenized[\"input_ids\"].shape)\n",
    "    \n",
    "    prefix_input_ids = tokenizer(prefix_text, add_special_tokens=True, return_tensors='pt')['input_ids']\n",
    "    print(\"prefix shape: \", prefix_input_ids.shape)\n",
    "\n",
    "    suffix_start_idx = prefix_input_ids.shape[1]\n",
    "    suffix_input_ids = full_tokenized[\"input_ids\"][:, suffix_start_idx:]\n",
    "\n",
    "    print(\"suffix shape: \", suffix_input_ids.shape)\n",
    "    print(\"suffix input ids: \", suffix_input_ids)\n",
    "\n",
    "    repeated_prefix_input_ids = prefix_input_ids.repeat(full_tokenized[\"input_ids\"].shape[0], 1)\n",
    "    print(\"repeated prefix shape: \", repeated_prefix_input_ids.shape)\n",
    "\n",
    "    # Check if the first 'prefix_length' tokens of each entry in full_tokenized are the same as prefix_input_ids\n",
    "    is_prefix_equal = torch.all(full_tokenized[\"input_ids\"][:, :suffix_start_idx] == repeated_prefix_input_ids, dim=1)\n",
    "    print(\"Is prefix equal for all entries: \", is_prefix_equal)\n",
    "    \n",
    "    return full_tokenized, suffix_input_ids, suffix_start_idx \n",
    "\n",
    "# sample_answers = [' The answer is $12.', ' The answer is 10.', ' The answer is 3.']\n",
    "# full_tokenized, suffix_input_ids, suffix_start_idx = tokenize_sample_answers(tokenizer, question, sample_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_logps(\n",
    "        logits: torch.FloatTensor,\n",
    "        token_mask: torch.LongTensor,\n",
    "        average_log_prob: bool = False,\n",
    "        label_pad_token_id: int = -100,\n",
    "        is_encoder_decoder: bool = False,\n",
    "    ) -> torch.FloatTensor:\n",
    "    \"\"\"Compute the log probabilities of the given labels under the given logits.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n",
    "        labels: Labels for which to compute the log probabilities. Label tokens with a value of label_pad_token_id are ignored. Shape: (batch_size, sequence_length)\n",
    "        average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.\n",
    "        label_pad_token_id: The label pad token id.\n",
    "        is_encoder_decoder: Whether the model is an encoder-decoder model.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.\n",
    "    \"\"\"\n",
    "    if logits.shape[:-1] != token_mask.shape:\n",
    "        raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n",
    "\n",
    "    # dummy token; we'll ignore the losses on these tokens later\n",
    "    token_mask[token_mask == label_pad_token_id] = 0\n",
    "\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=token_mask.unsqueeze(2)).squeeze(2)\n",
    "    print(per_token_logps.shape)\n",
    "    print(token_mask)\n",
    "    print(per_token_logps * token_mask)\n",
    "\n",
    "    tokens = [tokenizer.decode(ids) for ids in token_mask]\n",
    "\n",
    "    for i, token_sequence in enumerate(tokens):\n",
    "        print(f\"Tokens for sequence {i}: {token_sequence}\")\n",
    "        for j, token in enumerate(token_sequence.split()):\n",
    "            token_log_prob = per_token_logps[i, j]#.max().item()  # Get the max log prob for this token\n",
    "            print(f\"Token: {token}, Log Prob: {token_log_prob}\")\n",
    "\n",
    "    if average_log_prob:\n",
    "        return (per_token_logps * token_mask).sum(-1) / token_mask.sum(-1)\n",
    "    else:\n",
    "        return (per_token_logps * token_mask).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobs_per_token(tokenizer, prefix, suffixes):\n",
    "    full_tokenized, suffix_input_ids, suffix_start_idx = tokenize_sample_answers(tokenizer, prefix, suffixes)\n",
    "    all_logits = model(full_tokenized['input_ids'].to(model.device)).logits\n",
    "    get_batch_logps(all_logits.to(\"cpu\"), full_tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_prefix = \"\"\"[INST] Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "A: [/INST] In April, Natalia sold clips to 48 friends. In May, she sold half as many as in April, so she sold 48 / 2 = 24 clips. In total, she sold 48 + 24 = 72 clips. The answer is 72.\n",
    "\n",
    "Or, we could use the following equation:\n",
    "Total Clips Sold = Clips Sold in April + Clips Sold in May\n",
    "Total Clips Sold = 48 + 24\n",
    "Total Clips Sold = 72.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_suffixes = [\"The answer is 72.\", \"The answer is 64.\", \"Therefore, the answer is 72 clips sold altogether in April and May.\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs_per_token(tokenizer, ex_prefix, ex_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT Turbo Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_suffix_mask(full_tokenized, suffix_start_idx):\n",
    "    \"\"\"Create a mask for the suffix tokens.\"\"\"\n",
    "    batch_size, seq_length = full_tokenized['input_ids'].shape\n",
    "    mask = torch.zeros_like(full_tokenized['input_ids'])\n",
    "    mask[:, suffix_start_idx:] = 1\n",
    "    return mask\n",
    "\n",
    "def get_batch_logps(logits, token_mask, tokenizer, suffix_input_ids):\n",
    "    \"\"\"Calculate log probabilities for suffix tokens.\"\"\"\n",
    "    # Apply the mask to logits\n",
    "    masked_logits = logits * token_mask.unsqueeze(-1)  # Extend mask for vocab size\n",
    "\n",
    "    # Calculate log softmax\n",
    "    log_probs = torch.nn.functional.log_softmax(masked_logits, dim=-1)\n",
    "\n",
    "    # Gather log probabilities for actual token indices\n",
    "    gathered_log_probs = torch.gather(log_probs, 2, suffix_input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Decode tokens and print their log probabilities\n",
    "    tokens = [tokenizer.decode(ids) for ids in suffix_input_ids]\n",
    "    for i, token_sequence in enumerate(tokens):\n",
    "        print(f\"Tokens for sequence {i}: {token_sequence}\")\n",
    "        for j, token in enumerate(token_sequence.split()):\n",
    "            token_log_prob = gathered_log_probs[i, j].item()\n",
    "            print(f\"Token: {token}, Log Prob: {token_log_prob}\")\n",
    "\n",
    "    return gathered_log_probs\n",
    "\n",
    "def logprobs_per_token(tokenizer, prefix, suffixes):\n",
    "    full_tokenized, suffix_input_ids, suffix_start_idx = tokenize_sample_answers(tokenizer, prefix, suffixes)\n",
    "    suffix_mask = create_suffix_mask(full_tokenized, suffix_start_idx)\n",
    "    all_logits = model(full_tokenized['input_ids'].to(model.device)).logits\n",
    "    get_batch_logps(all_logits.to(\"cpu\"), suffix_mask, tokenizer, suffix_input_ids)\n",
    "\n",
    "# Example usage\n",
    "logprobs_per_token(tokenizer, ex_prefix, ex_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_logps(logits, token_mask, average_log_prob=False):\n",
    "    \"\"\"Compute the log probabilities of the given tokens under the given logits.\n",
    "\n",
    "    Args:\n",
    "        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n",
    "        token_mask: Indices of the tokens for which to compute the log probabilities. Shape: (batch_size, sequence_length)\n",
    "        average_log_prob: If True, return the average log probability per token. Otherwise, return the sum of the log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of shape (batch_size,) containing the average/sum log probabilities of the given tokens under the given logits.\n",
    "    \"\"\"\n",
    "    # Ensure logits and token_mask have compatible shapes\n",
    "    if logits.shape[:-1] != token_mask.shape:\n",
    "        raise ValueError(\"Logits and token mask must have the same shape.\")\n",
    "\n",
    "    # Compute log softmax over the vocabulary dimension\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Use gather to select the log probabilities of the actual tokens\n",
    "    gathered_log_probs = torch.gather(log_probs, 2, token_mask.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Print each token and its log probability\n",
    "    for i, sequence in enumerate(token_mask):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(sequence)\n",
    "        log_probs_sequence = gathered_log_probs[i]\n",
    "        print(f\"Sequence {i + 1}:\")\n",
    "        for token, log_prob in zip(tokens, log_probs_sequence):\n",
    "            print(f\"Token: {token}, Log Prob: {log_prob.item()}\")\n",
    "\n",
    "    # Compute the sum or average of the log probabilities\n",
    "    if average_log_prob:\n",
    "        token_count = (token_mask != -100).sum(dim=1).float()  # Assuming -100 is used to mask tokens\n",
    "        return (gathered_log_probs * (token_mask != -100)).sum(dim=1) / token_count\n",
    "    else:\n",
    "        return (gathered_log_probs * (token_mask != -100)).sum(dim=1)\n",
    "\n",
    "# Example usage\n",
    "# Assuming `all_logits` and `full_tokenized['input_ids']` are available from your model's output\n",
    "def logprobs_per_token(tokenizer, prefix, suffixes):\n",
    "    full_tokenized, suffix_input_ids, suffix_start_idx = tokenize_sample_answers(tokenizer, prefix, suffixes)\n",
    "    suffix_mask = create_suffix_mask(full_tokenized, suffix_start_idx)\n",
    "    all_logits = model(full_tokenized['input_ids'].to(model.device)).logits\n",
    "    get_batch_logps(all_logits.to(\"cpu\"), suffix_mask)\n",
    "\n",
    "# Example usage\n",
    "logprobs_per_token(tokenizer, ex_prefix, ex_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messing Around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gpt35_df = pd.read_csv('../conditional/data/112_gsm8k_gpt35_cot_onesent_responses.csv')\n",
    "gpt35_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = gpt35_df['Question'].to_list()[:2]\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = questions[1]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokens = tokenizer(question, add_special_tokens=False)\n",
    "question_input_ids = question_tokens.input_ids\n",
    "print(len(question_tokens.input_ids), type(question_tokens), type(question_tokens.input_ids))\n",
    "question_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokens = {f\"question_{k}\": v for k, v in question_tokens.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = gpt35_df['Answer'].to_list()[:2]\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = answers[1]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokenized = tokenizer(question + answer, add_special_tokens=False)\n",
    "print(len(full_tokenized.input_ids), len(question_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_input_ids = full_tokenized[\"input_ids\"][len(question_input_ids) :]\n",
    "answer_attention_mask = full_tokenized[\"attention_mask\"][len(question_input_ids) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenized_answer(tokenizer, prompt, answer):\n",
    "        \"\"\"\n",
    "        Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n",
    "        It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n",
    "        Reference:\n",
    "            https://github.com/EleutherAI/lm-evaluation-harness/pull/531#issuecomment-1595586257\n",
    "        \"\"\"\n",
    "\n",
    "        full_tokenized = tokenizer(prompt + answer, add_special_tokens=False)\n",
    "        prompt_input_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids) :]\n",
    "        answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids) :]\n",
    "\n",
    "        # Concat tokens to form `enc(a) + enc(a + b)[len(enc(a)):]`\n",
    "        full_concat_input_ids = np.concatenate([prompt_input_ids, answer_input_ids])\n",
    "\n",
    "        # Prepare input tokens for token by token comparison\n",
    "        full_input_ids = np.array(full_tokenized[\"input_ids\"])\n",
    "\n",
    "        if len(full_input_ids) != len(full_concat_input_ids):\n",
    "            raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n",
    "\n",
    "        # On some tokenizers, like Llama-2 tokenizer, there are occasions where tokens\n",
    "        # can be merged together when tokenizing prompt+answer. This could result\n",
    "        # on the last token from the prompt being different when tokenized on its own\n",
    "        # vs when done as prompt+answer.\n",
    "        response_token_ids_start_idx = len(prompt_input_ids)\n",
    "\n",
    "        # If tokenized prompt is different than both prompt+answer, then it means the\n",
    "        # last token has changed due to merging.\n",
    "        if prompt_input_ids != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n",
    "            response_token_ids_start_idx -= 1\n",
    "\n",
    "        prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n",
    "        prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n",
    "\n",
    "        if len(prompt_input_ids) != len(prompt_attention_mask):\n",
    "            raise ValueError(\"Prompt input ids and attention mask should have the same length.\")\n",
    "\n",
    "        answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n",
    "        answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n",
    "\n",
    "        return dict(\n",
    "            prompt_input_ids=prompt_input_ids,\n",
    "            prompt_attention_mask=prompt_attention_mask,\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answer = build_tokenized_answer(tokenizer, question, answer)\n",
    "len(tokenized_answer['prompt_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_len_input_ids = len(question_tokens[\"question_input_ids\"])\n",
    "question_len_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_answer), type(tokenized_answer['prompt_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from util import pad_to_length\n",
    "\n",
    "def concatenated_inputs(\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        is_encoder_decoder: bool = False,\n",
    "        label_pad_token_id: int = -100,\n",
    "        padding_value: int = 0,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ) -> Dict[str, torch.LongTensor]:\n",
    "    \"\"\"Concatenate the chosen and rejected inputs into a single tensor.\n",
    "\n",
    "    Args:\n",
    "        batch: A batch of data. Must contain the keys 'chosen_input_ids' and 'rejected_input_ids', which are tensors of shape (batch_size, sequence_length).\n",
    "        is_encoder_decoder: Whether the model is an encoder-decoder model.\n",
    "        label_pad_token_id: The label pad token id.\n",
    "        padding_value: The padding value to use for the concatenated inputs_ids.\n",
    "        device: The device for the concatenated inputs.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the concatenated inputs under the key 'concatenated_input_ids'.\n",
    "    \"\"\"\n",
    "    concatenated_batch = {}\n",
    "\n",
    "    if is_encoder_decoder:\n",
    "        max_length = max(batch[\"chosen_labels\"].shape[1], batch[\"rejected_labels\"].shape[1])\n",
    "    else:\n",
    "        max_length = max(batch[\"chosen_input_ids\"].shape[1], batch[\"rejected_input_ids\"].shape[1])\n",
    "\n",
    "    for k in batch:\n",
    "        if k.startswith(\"chosen\") and isinstance(batch[k], torch.Tensor):\n",
    "            if \"labels\" in k or is_encoder_decoder:\n",
    "                pad_value = label_pad_token_id\n",
    "            elif k.endswith(\"_input_ids\"):\n",
    "                pad_value = padding_value\n",
    "            elif k.endswith(\"_attention_mask\"):\n",
    "                pad_value = 0\n",
    "            concatenated_key = k.replace(\"chosen\", \"concatenated\")\n",
    "            concatenated_batch[concatenated_key] = pad_to_length(batch[k], max_length, pad_value=pad_value)\n",
    "    for k in batch:\n",
    "        if k.startswith(\"rejected\") and isinstance(batch[k], torch.Tensor):\n",
    "            if \"labels\" in k or is_encoder_decoder:\n",
    "                pad_value = label_pad_token_id\n",
    "            elif k.endswith(\"_input_ids\"):\n",
    "                pad_value = padding_value\n",
    "            elif k.endswith(\"_attention_mask\"):\n",
    "                pad_value = 0\n",
    "            concatenated_key = k.replace(\"rejected\", \"concatenated\")\n",
    "            concatenated_batch[concatenated_key] = torch.cat(\n",
    "                (\n",
    "                    concatenated_batch[concatenated_key],\n",
    "                    pad_to_length(batch[k], max_length, pad_value=pad_value),\n",
    "                ),\n",
    "                dim=0,\n",
    "            ).to(device=device)\n",
    "\n",
    "    if is_encoder_decoder:\n",
    "        concatenated_batch[\"concatenated_input_ids\"] = batch[\"prompt_input_ids\"].repeat(2, 1).to(device=device)\n",
    "        concatenated_batch[\"concatenated_attention_mask\"] = (\n",
    "            batch[\"prompt_attention_mask\"].repeat(2, 1).to(device=device)\n",
    "        )\n",
    "\n",
    "    return concatenated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1 = \" The answer is $12.\"\n",
    "answer2 = \" The answer is 10.\"\n",
    "answer3 = \" The answer is 3.\"\n",
    "sample_answers = [answer1, answer2, answer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_labels_mask(full_tokenized, suffix_start_idx):\n",
    "    \"\"\"\n",
    "    Create a binary mask for the suffix tokens in the tokenized batch.\n",
    "\n",
    "    Args:\n",
    "        full_tokenized: The tokenized data containing 'input_ids' and 'attention_mask'.\n",
    "        suffix_start_idx: The start index of the suffix in the tokenized sequences.\n",
    "\n",
    "    Returns:\n",
    "        A binary mask tensor of the same shape as `full_tokenized['input_ids']` where suffix tokens are marked with 1 and others with 0.\n",
    "    \"\"\"\n",
    "    batch_size, seq_length = full_tokenized['input_ids'].shape\n",
    "\n",
    "    labels_mask = torch.zeros((batch_size, seq_length), dtype=torch.long)\n",
    "\n",
    "    labels_mask[:, suffix_start_idx:] = 1\n",
    "\n",
    "    labels_mask *= full_tokenized['attention_mask']\n",
    "\n",
    "    return labels_mask\n",
    "\n",
    "wrong_suffix_mask = create_labels_mask(full_tokenized, suffix_start_idx)\n",
    "print(\"Labels mask shape:\", wrong_suffix_mask.shape)\n",
    "print(\"Labels mask:\", wrong_suffix_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = model(full_tokenized['input_ids'].to(model.device)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_forward(\n",
    "        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "    \"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\n",
    "\n",
    "    We do this to avoid doing two forward passes, because it's faster for FSDP.\n",
    "    \"\"\"\n",
    "    concatenated_batch = self.concatenated_inputs(\n",
    "        batch,\n",
    "        is_encoder_decoder=self.is_encoder_decoder,\n",
    "        label_pad_token_id=self.label_pad_token_id,\n",
    "        padding_value=self.padding_value,\n",
    "        device=self.accelerator.device,\n",
    "    )\n",
    "    len_chosen = batch[\"chosen_labels\"].shape[0]\n",
    "\n",
    "    model_kwargs = (\n",
    "        {\n",
    "            \"labels\": concatenated_batch[\"concatenated_labels\"],\n",
    "            \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n",
    "        }\n",
    "        if self.is_encoder_decoder\n",
    "        else {}\n",
    "    )\n",
    "    all_logits = model(\n",
    "        concatenated_batch[\"concatenated_input_ids\"],\n",
    "        attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "        use_cache=False,\n",
    "        **model_kwargs,\n",
    "    ).logits\n",
    "\n",
    "    all_logps = self.get_batch_logps(\n",
    "        all_logits,\n",
    "        concatenated_batch[\"concatenated_labels\"],\n",
    "        average_log_prob=self.loss_type == \"ipo\",\n",
    "        is_encoder_decoder=self.is_encoder_decoder,\n",
    "        label_pad_token_id=self.label_pad_token_id,\n",
    "    )\n",
    "\n",
    "    chosen_logps = all_logps[:len_chosen]\n",
    "    rejected_logps = all_logps[len_chosen:]\n",
    "\n",
    "    chosen_logits = all_logits[:len_chosen]\n",
    "    rejected_logits = all_logits[len_chosen:]\n",
    "\n",
    "    return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_suffix_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tokenized.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch_logps(all_logits.to(\"cpu\"), full_tokenized.input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
