{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/vvajipey/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dd6347e38344e4a6e31d4d5c4e64a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fc8bf4a0fa43a89446ecc7acf1e02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932f54639f3d4e9cb969d6f628141ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fa73ac43c8403fb80bd042b8a09905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c918efd75cae4b8c9ad77ca3299aeb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889a5fec999140b19215946299dfee5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3902140132b94c538db19ddc886d36d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dd3cccd8a3417c9a8f59502afe5b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1175a4a918043908e90c44a50856c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e508e53b77ed4b12b094d15050572690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e0c055153e42b7b4f689c41a6976d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from difflib import get_close_matches\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "gsm_df = pd.read_csv('../distribution/data/gsm8kTest.csv')\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< constructing get_all_transitions >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_paths(path_counter, n_samples):\n",
    "    \"\"\"\n",
    "    Sample from a path counter\n",
    "    \"\"\"\n",
    "    return random.choices(\n",
    "        list(path_counter.keys()), weights=list(path_counter.values()), k=n_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_left_padded(tensor):\n",
    "    trimmed_sequences = []\n",
    "    for seq in tensor:\n",
    "        # Find indices of the first and last non-pad tokens\n",
    "        non_pad_indices = (seq != tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(non_pad_indices) > 0:\n",
    "            start_index = non_pad_indices[0]\n",
    "            end_index = non_pad_indices[-1] + 1\n",
    "            trimmed_sequences.append(seq[start_index:end_index])\n",
    "        else:\n",
    "            # if entire sequence is padding, use an empty sequence\n",
    "            trimmed_sequences.append(torch.tensor([], dtype=torch.long, device=model.device))\n",
    "\n",
    "    # Determine the maximum length after trimming\n",
    "    max_length = max(len(seq) for seq in trimmed_sequences)\n",
    "    padded_tensor = torch.full((tensor.shape[0], max_length), fill_value=tokenizer.pad_token_id, dtype=torch.long, device=model.device)\n",
    "\n",
    "    # put left padding\n",
    "    for i, seq in enumerate(trimmed_sequences):\n",
    "        padded_tensor[i, -len(seq):] = seq\n",
    "\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSentenceStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_sequences):\n",
    "        # Tokenize each stop sequence and store their token IDs\n",
    "        self.stop_token_ids_list = [tokenizer.encode(seq, add_special_tokens=False)[1:] for seq in stop_sequences]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28750, 28747], [9977, 28705, 28770, 28747], [9977, 28705, 28770, 28747]]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28770, 28747], [9977, 28705, 28781, 28747]]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28781, 28747], [9977, 28705, 28782, 28747]]\n",
    "        # print(\"Stop token IDs:\", self.stop_token_ids_list)  # Debugging to see the token IDs\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Check each stop sequence against the end of the input_ids for each sequence in the batch\n",
    "        for stop_token_ids in self.stop_token_ids_list:\n",
    "            if input_ids.shape[1] >= len(stop_token_ids):\n",
    "                # Extract the last tokens of the same length as the stop sequence\n",
    "                last_tokens = input_ids[:, -len(stop_token_ids):]\n",
    "                # Check if they match the stop sequence tokens\n",
    "                is_match = (last_tokens == torch.tensor(stop_token_ids, device=input_ids.device)).all(dim=1)\n",
    "                # If any sequence in the batch matches, return True to stop generation\n",
    "                if is_match.any():\n",
    "                    print(\"Stopping sequence detected, stopping generation.\")\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_step_splits_numbered(question):\n",
    "    # get_prompt_message(overtime_question, 0)\n",
    "    fewshot_question = f\"\"\"Q: Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?\n",
    "\n",
    "A:\n",
    "Step 1:\n",
    "First, lets calculate how much Tina earns for working a regular 8-hour shift:\n",
    "Regular hours earned = Hourly wage * Hours worked per day * Number of days\n",
    "Regular hours earned = $18.00 * 8 * 5\n",
    "Regular hours earned = $720.00\n",
    "Step 2:\n",
    "Next, lets calculate how many overtime hours she works in total:\n",
    "Overtime hours = Total hours worked - Regular hours\n",
    "Overtime hours = 10 hours * 5 days - 8 hours * 5 days\n",
    "Overtime hours = 10 * 5 - 8 * 5 Overtime hours = 50 hours - 40 hours\n",
    "Overtime hours = 10 hours\n",
    "Step 3:\n",
    "Now, lets calculate the overtime pay for those 10 hours:\n",
    "Overtime pay per hour = Hourly wage + 1/2 hourly wage\n",
    "Overtime pay per hour = $18.00 + $9.00\n",
    "Overtime pay per hour = $27.00\n",
    "Total overtime pay = Overtime pay per hour * Overtime hours\n",
    "Total overtime pay = $27.00 * 10\n",
    "Total overtime pay = $270.00\n",
    "Step 4:\n",
    "Finally, lets calculate her total earnings:\n",
    "Total earnings = Regular hours earned + Total overtime pay\n",
    "Total earnings = $720.00 + $270.00\n",
    "Total earnings = $990.00\n",
    "Step 5:\n",
    "So, Tina makes a total of $990.00 for working 10 hours every day for 5 days\n",
    "\n",
    "Q: Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\\n\n",
    "\n",
    "A:\n",
    "Step 1: \n",
    " First, lets determine how many pages Joy reads in one minute:\n",
    " Pages read per minute = Pages read per 20 minutes / 20 minutes\n",
    " = 8 pages / 20 minutes = 0.4 pages per minute\n",
    "Step 2: \n",
    " Next, well find out how long it takes Joy to read 120 pages:\n",
    " Time = Pages to be read / Pages read per minute\n",
    " = 120 pages / 0.4 pages per minute\n",
    " = 300 minutes\n",
    "Step 3: \n",
    " Finally, well convert 300 minutes into hours:\n",
    " Hours = Minutes / 60\n",
    " = 300 minutes / 60\n",
    " = 5 hours\n",
    "Step 4: \n",
    " So it will take Joy 5 hours to read 120\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "A:\n",
    "Step 1:\n",
    "\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": fewshot_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import get_prompt_message\n",
    "\n",
    "def sample_completions_from_model(model, tokenizer, path, path_to_tensor, n_samples):\n",
    "    input_tensor = path_to_tensor[path]\n",
    "    if input_tensor[0, -1].item() == 2:\n",
    "        new_paths = [path + ('<eos>',)] # no need to repeat n_samples times\n",
    "        return new_paths, path_to_tensor\n",
    "\n",
    "    stop_sequences = [f\"\\nStep {i}:\" for i in range(10)]\n",
    "    step_num_stopping_criteria = StoppingCriteriaList([BatchSentenceStoppingCriteria(tokenizer, stop_sequences)])\n",
    "    \n",
    "    # max_length = 0\n",
    "    new_paths = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        gen_outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "            stopping_criteria=step_num_stopping_criteria\n",
    "        ).sequences\n",
    "\n",
    "        # if gen_outputs.shape[1] > max_length:\n",
    "            # max_length = gen_outputs.shape[1]\n",
    "\n",
    "        completion = tokenizer.batch_decode(gen_outputs[:, input_tensor.shape[1]:])[0]\n",
    "        new_path = path + (completion,)\n",
    "        new_paths.append(new_path)\n",
    "\n",
    "        path_to_tensor[new_path] = gen_outputs\n",
    "\n",
    "    return new_paths, path_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='../.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def remove_trailing_step_info(step):\n",
    "    # Regular expression to match \"Step n:\" at the end of the string, where n is any number\n",
    "    pattern = r\"\\s*Step \\d+:$\"\n",
    "    return re.sub(pattern, \"\", step).strip()\n",
    "\n",
    "def get_bucket_prompt(question, step, buckets=None, examples=None):\n",
    "    if buckets is None:\n",
    "        buckets = []\n",
    "    if examples is None:\n",
    "        examples = []\n",
    "    \n",
    "    bucket_examples_str = \"\"\n",
    "    for i, (bucket, example) in enumerate(zip(buckets, examples), 1):\n",
    "        bucket_examples_str += f\"<BUCKET {i}>{bucket}</BUCKET {i}>\\n<EXAMPLE from BUCKET {i}>{example}</EXAMPLE from BUCKET {i}>\\n\"\n",
    "    \n",
    "    return f\"\"\"I will give you a math problem, and a substep in a solution to the problem. I will also give you a list of natural language label buckets that have been created from previous answers to the same question. Look at the step, identify which bucket it falls under, and return just the name of the bucket. If none of the existing buckets are representative, create a new bucket preceded with the string \"NEW :\". The label name must be descriptive, specific, and concise natural language. Return this new bucket string. \n",
    "\n",
    "Make sure that fundamentally different steps are put in different buckets; if two steps belong in the same bucket, ensure that the types of mathematical operations/fundamental logic are approximately equivalent. Do not generate a new bucket if a step fits into an existing bucket, and do not name buckets based on correct vs. incorrect. Create a separate bucket for the declaration of the final answer.\n",
    "<QUESTION>{question}</QUESTION>\n",
    "{bucket_examples_str}\n",
    "<STEP TO CATEGORIZE>{step}</STEP TO CATEGORIZE>\n",
    "Reminders: DO NOT INCLUDE tags such as <BUCKET> </BUCKET> in your answer. DO NOT NAME NEW BUCKETS BASED ON WHETHER A STEP IS CORRECT/INCORRECT. If you are proposing a new bucket name, start with \"NEW :\" e.g. \"NEW : bucket name\".\"\"\"\n",
    "\n",
    "# f\"\"\"I will give you a math problem, and a substep in a solution to the problem. I will also give you a list of natural language label buckets that have been created from previous answers to the same question. Look at the step, identify which bucket it falls under, and return just the name of the bucket. If none of the existing buckets are representative, create a new bucket preceded with the string \"NEW :\". The label name must be descriptive, specific, and concise natural language. Return this new bucket string. \n",
    "\n",
    "# Make sure that fundamentally different steps are put in different buckets; if two steps belong in the same bucket, ensure that the types of mathematical operations/fundamental logic are approximately equivalent. Do not generate a new bucket if a step fits into an existing bucket, and do not name buckets based on correct vs. incorrect. Create a separate bucket for the declaration of the final answer.\n",
    "# <QUESTION>{question}</QUESTION>\n",
    "# {bucket_examples_str}\n",
    "# <STEP TO CATEGORIZE>{step}</STEP TO CATEGORIZE>\n",
    "# Reminders: DO NOT INCLUDE tags such as <BUCKET> </BUCKET> in your answer. DO NOT NAME NEW BUCKETS BASED ON WHETHER A STEP IS CORRECT/INCORRECT. If you are proposing a new bucket name, start with \"NEW :\" e.g. \"NEW : bucket name\".\"\"\"\n",
    "\n",
    "def categorize_step_with_gpt4(question, step, eq_class_labels, eq_class_examples):\n",
    "    # print(\"BUCKETING PROMPT\", get_bucket_prompt(question, step, eq_class_labels, eq_class_examples))\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": get_bucket_prompt(question, step, eq_class_labels, eq_class_examples)}\n",
    "        ],\n",
    "        max_tokens=50,\n",
    "        temperature=0.0,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_logging = False\n",
    "\n",
    "def get_equivalence_classes(paths, all_equivalence_classes):\n",
    "    equivalence_classes = defaultdict(Counter)\n",
    "    current_eq_class_labels = [key for key in all_equivalence_classes.keys() if not key.startswith(\"question_\")]\n",
    "    \n",
    "    # Loop over the current_eq_class_labels and sample one path for each label    \n",
    "    current_eq_class_examples = []\n",
    "    for key in current_eq_class_labels:\n",
    "        prev_paths = list(all_equivalence_classes[key].keys())\n",
    "        sampled_path = random.choice(prev_paths)\n",
    "        current_eq_class_examples.append(sampled_path[-1])\n",
    "\n",
    "    for path in paths:\n",
    "        last_state = path[-1]\n",
    "        if last_state == \"<eos>\":\n",
    "            continue\n",
    "        # print(\"CURRENT PATH: \", path)\n",
    "        # print(\"LAST STATE (TO CATEGORIZE): \", last_state)\n",
    "        last_state_abstract = categorize_step_with_gpt4(path[0], remove_trailing_step_info(last_state), current_eq_class_labels, current_eq_class_examples)\n",
    "        # print(\"+++++++++++++++++++++++++++++\")\n",
    "        # print(\"gpt4o: \", last_state_abstract)#, \"LAST STATE: \", last_state)\n",
    "        # print(\"+++++++++++++++++++++++++++++\")\n",
    "\n",
    "        if last_state_abstract.startswith(\"NEW : \"):\n",
    "            last_state_abstract = last_state_abstract[len(\"NEW : \"):]\n",
    "            current_eq_class_labels.append(last_state_abstract)\n",
    "            current_eq_class_examples.append(path[-1])\n",
    "            if print_logging:\n",
    "                print(\"CREATING NEW BUCKET: \", last_state_abstract)\n",
    "        else:\n",
    "            # Find the closest string match between the response and current_eq_class_labels\n",
    "            closest_match = get_close_matches(last_state_abstract, current_eq_class_labels, n=1, cutoff=0.75)\n",
    "            if closest_match:\n",
    "                if print_logging:\n",
    "                    print(f\"ADDING '{last_state_abstract}' TO EXISTING BUCKET\", closest_match[0])\n",
    "                last_state_abstract = closest_match[0]\n",
    "            else:\n",
    "                current_eq_class_labels.append(last_state_abstract)\n",
    "                current_eq_class_examples.append(path[-1])\n",
    "                if print_logging:\n",
    "                    print(\"NO CLOSEST MATCH, NEW BUCKET: \", last_state_abstract)\n",
    "        \n",
    "        equivalence_classes[last_state_abstract].update([path])\n",
    "\n",
    "    return equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_equivalence_classes(equivalence_classes, new_classes):\n",
    "    \"\"\"\n",
    "    Update the set of equivalence classes with the newly-discovered ones\n",
    "    \"\"\"\n",
    "    for new_class in new_classes:\n",
    "        if new_class in equivalence_classes:\n",
    "            equivalence_classes[new_class] += new_classes[new_class]\n",
    "        else:\n",
    "            equivalence_classes[new_class] = new_classes[new_class]\n",
    "\n",
    "    return equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_logging = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_transitions(model, question_index, n_samples=2):\n",
    "    \"\"\"\n",
    "    Build a graph of the probability of each state given the previous state.\n",
    "    \"\"\"\n",
    "    question = gsm_df['question'].tolist()[question_index]\n",
    "    path_to_tensor = {(question,) : tokenizer.apply_chat_template(prompt_with_step_splits_numbered(question), add_generation_prompt=True, return_tensors=\"pt\")}\n",
    "\n",
    "    question_abstract = f\"question_{question_index}\"\n",
    "    # keep track of the paths that end in each equivalence class\n",
    "    all_equivalence_classes = {question_abstract : Counter([(question,)])}\n",
    "    # track the equivalence classes that we discovered in the last step\n",
    "    new_equivalence_classes = list(all_equivalence_classes.keys())\n",
    "    # track the different wordings we find that make up each equivalence class\n",
    "    unique_wordings = defaultdict(set)\n",
    "    unique_wordings[question_abstract] = {question}\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        if print_logging:\n",
    "            print(\"CURRENT ALL_EQUIVALENCE_CLASSES: \", all_equivalence_classes)\n",
    "            print(\"MOST RECENT EQUIVALENCE CLASSES: \", new_equivalence_classes)\n",
    "        # reset the new equivalence classes\n",
    "        prev_equivalence_classes = new_equivalence_classes\n",
    "        new_equivalence_classes = []\n",
    "\n",
    "        # sample paths from each existing equivalence class\n",
    "        # (this should be batched if we're using a transformer)\n",
    "        for class_name in prev_equivalence_classes:\n",
    "            if print_logging:\n",
    "                print(\"class name: \", class_name)\n",
    "                print()\n",
    "            \n",
    "            # sample some paths that lead to the new equivalence class\n",
    "            paths = sample_paths(all_equivalence_classes[class_name], n_samples)\n",
    "            if print_logging:\n",
    "                print(\"paths: \", paths)\n",
    "                print()\n",
    "\n",
    "            for path in paths:\n",
    "                # sample completions from the model to get the next step following the path\n",
    "                new_paths, path_to_tensor = sample_completions_from_model(model, tokenizer, path, path_to_tensor, n_samples=n_samples) \n",
    "                if print_logging:\n",
    "                    print(\"new_paths: \", new_paths)\n",
    "\n",
    "                # group the completions into equivalence classes\n",
    "                completion_classes = get_equivalence_classes(new_paths, all_equivalence_classes)\n",
    "                if print_logging:\n",
    "                    print(\"completions_classes: \", completion_classes)\n",
    "\n",
    "                # Update our data structures\n",
    "                # I think it's ok for this kind of thing to be in a for loop,\n",
    "                # as each operation won't take much time\n",
    "                for completion_class in completion_classes:\n",
    "                    if completion_class not in all_equivalence_classes:\n",
    "                        new_equivalence_classes.append(completion_class)\n",
    "                    unique_wordings[completion_class].update(\n",
    "                        [x[-1] for x in completion_classes[completion_class].keys()]\n",
    "                    )\n",
    "                # update the running tracker of all discovered euqivalence classes\n",
    "                all_equivalence_classes = update_equivalence_classes(\n",
    "                    all_equivalence_classes, completion_classes\n",
    "                )\n",
    "        \n",
    "\n",
    "        # break when we stop discovering new equivalence classes\n",
    "        if print_logging:\n",
    "            print(\"LEN(NEW_EQ_CLASSES): \", len(new_equivalence_classes))\n",
    "            print(\"=====================================================\")\n",
    "            print()\n",
    "\n",
    "        print(\"Current state of the tree of solutions after Iteration \", iteration, \":\")\n",
    "        eqc_idx = 0\n",
    "        for eq_class, cntr in all_equivalence_classes.items():\n",
    "            print(f\"Equivalence Class {eqc_idx}: {eq_class}, Number of Paths: {len(cntr)}, Path Lengths: {[len(trace) for trace in list(cntr.keys())]}\")\n",
    "            eqc_idx += 1\n",
    "        iteration += 1\n",
    "        print()\n",
    "\n",
    "        if len(new_equivalence_classes) == 0:\n",
    "            break\n",
    "\n",
    "    return all_equivalence_classes, unique_wordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</ constructing get_all_transitions >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = gsm_df['question'].tolist()[1]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Current state of the tree of solutions after Iteration  0 :\n",
      "Equivalence Class 0: question_1, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculating the amount of white fiber required, Number of Paths: 2, Path Lengths: [2, 2]\n",
      "Equivalence Class 2: Setting up variables and relationships, Number of Paths: 2, Path Lengths: [2, 2]\n",
      "\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Current state of the tree of solutions after Iteration  1 :\n",
      "Equivalence Class 0: question_1, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculating the amount of white fiber required, Number of Paths: 2, Path Lengths: [2, 2]\n",
      "Equivalence Class 2: Setting up variables and relationships, Number of Paths: 2, Path Lengths: [2, 2]\n",
      "Equivalence Class 3: Summing the total amount of fiber required, Number of Paths: 8, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\n",
      "Current state of the tree of solutions after Iteration  2 :\n",
      "Equivalence Class 0: question_1, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculating the amount of white fiber required, Number of Paths: 2, Path Lengths: [2, 2]\n",
      "Equivalence Class 2: Setting up variables and relationships, Number of Paths: 2, Path Lengths: [2, 2]\n",
      "Equivalence Class 3: Summing the total amount of fiber required, Number of Paths: 8, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SCARY\n",
    "print_logging = False\n",
    "all_equivalence_classes, unique_wordings = get_all_transitions(model, 1, n_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Current state of the tree of solutions after Iteration  0 :\n",
      "Equivalence Class 0: question_0, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculate remaining eggs, Number of Paths: 25, Path Lengths: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Current state of the tree of solutions after Iteration  1 :\n",
      "Equivalence Class 0: question_0, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculate remaining eggs, Number of Paths: 29, Path Lengths: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "Equivalence Class 2: Calculate money earned from selling eggs, Number of Paths: 9, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Equivalence Class 3: Calculate number of eggs sold, Number of Paths: 10, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Equivalence Class 4: Incorrect calculation of eggs sold, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 5: Incorrect calculation of eggs sold due to double counting, Number of Paths: 1, Path Lengths: [3]\n",
      "\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Current state of the tree of solutions after Iteration  2 :\n",
      "Equivalence Class 0: question_0, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculate remaining eggs, Number of Paths: 29, Path Lengths: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "Equivalence Class 2: Calculate money earned from selling eggs, Number of Paths: 25, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 3: Calculate number of eggs sold, Number of Paths: 10, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Equivalence Class 4: Incorrect calculation of eggs sold, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 5: Incorrect calculation of eggs sold due to double counting, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 6: Declaration of the final answer, Number of Paths: 13, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 7: Incorrect calculation of total money earned, Number of Paths: 25, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 8: Incorrect calculation of total income, Number of Paths: 1, Path Lengths: [4]\n",
      "Equivalence Class 9: Incorrect calculation of total money earned including muffins, Number of Paths: 1, Path Lengths: [4]\n",
      "Equivalence Class 10: Incorrect calculation of money earned from selling eggs, Number of Paths: 24, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Stopping sequence detected, stopping generation.\n",
      "Current state of the tree of solutions after Iteration  3 :\n",
      "Equivalence Class 0: question_0, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculate remaining eggs, Number of Paths: 29, Path Lengths: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "Equivalence Class 2: Calculate money earned from selling eggs, Number of Paths: 25, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 3: Calculate number of eggs sold, Number of Paths: 10, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Equivalence Class 4: Incorrect calculation of eggs sold, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 5: Incorrect calculation of eggs sold due to double counting, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 6: Declaration of the final answer, Number of Paths: 18, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n",
      "Equivalence Class 7: Incorrect calculation of total money earned, Number of Paths: 31, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5]\n",
      "Equivalence Class 8: Incorrect calculation of total income, Number of Paths: 2, Path Lengths: [4, 5]\n",
      "Equivalence Class 9: Incorrect calculation of total money earned including muffins, Number of Paths: 1, Path Lengths: [4]\n",
      "Equivalence Class 10: Incorrect calculation of money earned from selling eggs, Number of Paths: 24, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 11: Incorrect calculation of total daily income including muffins, Number of Paths: 2, Path Lengths: [5, 5]\n",
      "Equivalence Class 12: Incorrect calculation of total daily revenue including muffins, Number of Paths: 2, Path Lengths: [5, 5]\n",
      "Equivalence Class 13: Incorrect calculation of total daily earnings including breakfast and muffins, Number of Paths: 4, Path Lengths: [5, 5, 5, 5]\n",
      "Equivalence Class 14: Incorrect calculation of total daily revenue including breakfast and muffins, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 15: Incorrect declaration of the final answer, Number of Paths: 10, Path Lengths: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Equivalence Class 16: Incorrect calculation of total daily income including breakfast and muffins, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 17: Incorrect calculation of total daily income including breakfast and farmers' market earnings, Number of Paths: 1, Path Lengths: [5]\n",
      "\n",
      "Current state of the tree of solutions after Iteration  4 :\n",
      "Equivalence Class 0: question_0, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculate remaining eggs, Number of Paths: 29, Path Lengths: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "Equivalence Class 2: Calculate money earned from selling eggs, Number of Paths: 25, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 3: Calculate number of eggs sold, Number of Paths: 10, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Equivalence Class 4: Incorrect calculation of eggs sold, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 5: Incorrect calculation of eggs sold due to double counting, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 6: Declaration of the final answer, Number of Paths: 19, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6]\n",
      "Equivalence Class 7: Incorrect calculation of total money earned, Number of Paths: 31, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5]\n",
      "Equivalence Class 8: Incorrect calculation of total income, Number of Paths: 2, Path Lengths: [4, 5]\n",
      "Equivalence Class 9: Incorrect calculation of total money earned including muffins, Number of Paths: 1, Path Lengths: [4]\n",
      "Equivalence Class 10: Incorrect calculation of money earned from selling eggs, Number of Paths: 24, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 11: Incorrect calculation of total daily income including muffins, Number of Paths: 2, Path Lengths: [5, 5]\n",
      "Equivalence Class 12: Incorrect calculation of total daily revenue including muffins, Number of Paths: 2, Path Lengths: [5, 5]\n",
      "Equivalence Class 13: Incorrect calculation of total daily earnings including breakfast and muffins, Number of Paths: 4, Path Lengths: [5, 5, 5, 5]\n",
      "Equivalence Class 14: Incorrect calculation of total daily revenue including breakfast and muffins, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 15: Incorrect declaration of the final answer, Number of Paths: 24, Path Lengths: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Equivalence Class 16: Incorrect calculation of total daily income including breakfast and muffins, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 17: Incorrect calculation of total daily income including breakfast and farmers' market earnings, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 18: Incorrect declaration of total daily income including breakfast and muffins, Number of Paths: 4, Path Lengths: [6, 6, 6, 6]\n",
      "Equivalence Class 19: Incorrect declaration of total daily income including breakfast, muffins, and farmers' market earnings, Number of Paths: 4, Path Lengths: [6, 6, 6, 6]\n",
      "Equivalence Class 20: Incorrect declaration of total daily income including all activities, Number of Paths: 2, Path Lengths: [6, 6]\n",
      "\n",
      "Current state of the tree of solutions after Iteration  5 :\n",
      "Equivalence Class 0: question_0, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculate remaining eggs, Number of Paths: 29, Path Lengths: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
      "Equivalence Class 2: Calculate money earned from selling eggs, Number of Paths: 25, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 3: Calculate number of eggs sold, Number of Paths: 10, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Equivalence Class 4: Incorrect calculation of eggs sold, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 5: Incorrect calculation of eggs sold due to double counting, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 6: Declaration of the final answer, Number of Paths: 19, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6]\n",
      "Equivalence Class 7: Incorrect calculation of total money earned, Number of Paths: 31, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5]\n",
      "Equivalence Class 8: Incorrect calculation of total income, Number of Paths: 2, Path Lengths: [4, 5]\n",
      "Equivalence Class 9: Incorrect calculation of total money earned including muffins, Number of Paths: 1, Path Lengths: [4]\n",
      "Equivalence Class 10: Incorrect calculation of money earned from selling eggs, Number of Paths: 24, Path Lengths: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 11: Incorrect calculation of total daily income including muffins, Number of Paths: 2, Path Lengths: [5, 5]\n",
      "Equivalence Class 12: Incorrect calculation of total daily revenue including muffins, Number of Paths: 2, Path Lengths: [5, 5]\n",
      "Equivalence Class 13: Incorrect calculation of total daily earnings including breakfast and muffins, Number of Paths: 4, Path Lengths: [5, 5, 5, 5]\n",
      "Equivalence Class 14: Incorrect calculation of total daily revenue including breakfast and muffins, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 15: Incorrect declaration of the final answer, Number of Paths: 24, Path Lengths: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Equivalence Class 16: Incorrect calculation of total daily income including breakfast and muffins, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 17: Incorrect calculation of total daily income including breakfast and farmers' market earnings, Number of Paths: 1, Path Lengths: [5]\n",
      "Equivalence Class 18: Incorrect declaration of total daily income including breakfast and muffins, Number of Paths: 4, Path Lengths: [6, 6, 6, 6]\n",
      "Equivalence Class 19: Incorrect declaration of total daily income including breakfast, muffins, and farmers' market earnings, Number of Paths: 4, Path Lengths: [6, 6, 6, 6]\n",
      "Equivalence Class 20: Incorrect declaration of total daily income including all activities, Number of Paths: 2, Path Lengths: [6, 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_equivalence_classes, unique_wordings = get_all_transitions(model, 0, n_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved and loaded successfully, and it matches the original.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('pickle_jar', exist_ok=True)\n",
    "\n",
    "# Save all_equivalence_classes and unique_wordings to files in the pickle_jar folder\n",
    "with open('pickle_jar/all_equivalence_classes.pkl', 'wb') as f:\n",
    "    pickle.dump(all_equivalence_classes, f)\n",
    "\n",
    "with open('pickle_jar/unique_wordings.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_wordings, f)\n",
    "\n",
    "# Load the saved files to confirm they are faithful to the original\n",
    "with open('pickle_jar/all_equivalence_classes.pkl', 'rb') as f:\n",
    "    loaded_all_equivalence_classes = pickle.load(f)\n",
    "\n",
    "with open('pickle_jar/unique_wordings.pkl', 'rb') as f:\n",
    "    loaded_unique_wordings = pickle.load(f)\n",
    "\n",
    "# Confirm the loaded data matches the original data\n",
    "assert all_equivalence_classes == loaded_all_equivalence_classes, \"all_equivalence_classes does not match\"\n",
    "assert unique_wordings == loaded_unique_wordings, \"unique_wordings does not match\"\n",
    "\n",
    "print(\"Data saved and loaded successfully, and it matches the original.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_probabalistic_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalence Class Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", \"First, let's calculate the number of eggs Janet has left to sell after eating three for breakfast and baking four for her friends:\\n\\nEggs left to sell = Eggs laid per day - Eggs eaten for breakfast - Eggs baked for friends\\nEggs left to sell = 16 eggs - 3 eggs - 4 eggs\\nEggs left to sell = 9 eggs\\n\\nStep 2:\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for trace in list(all_equivalence_classes['Calculating remaining eggs'].keys()):\n",
    "#     print(len(trace))\n",
    "#     print(trace)\n",
    "# [len(trace) for trace in list(all_equivalence_classes['Calculating remaining eggs'].keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state of the tree of solutions:\n",
      "Equivalence Class 0: question_0, Number of Paths: 1, Path Lengths: [1]\n",
      "Equivalence Class 1: Calculate remaining eggs after eating, Number of Paths: 9, Path Lengths: [2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Equivalence Class 2: Calculate money earned from selling remaining eggs, Number of Paths: 17, Path Lengths: [3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Equivalence Class 3: Calculate number of eggs sold at the market, Number of Paths: 1, Path Lengths: [3]\n",
      "Equivalence Class 4: Calculate total income from all sources, Number of Paths: 1, Path Lengths: [4]\n",
      "Equivalence Class 5: Declaration of the final answer, Number of Paths: 1, Path Lengths: [4]\n",
      "Equivalence Class 6: Declaration of the final total income, Number of Paths: 2, Path Lengths: [5, 5]\n"
     ]
    }
   ],
   "source": [
    "# print(\"Current state of the tree of solutions:\")\n",
    "# eqc_idx = 0\n",
    "# for eq_class, cntr in all_equivalence_classes.items():\n",
    "#     print(f\"Equivalence Class {eqc_idx}: {eq_class}, Number of Paths: {len(cntr)}, Path Lengths: {[len(trace) for trace in list(cntr.keys())]}\")\n",
    "#     eqc_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_wordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deconstructed get_all_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_index = 0\n",
    "# n_samples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = gsm_df['question'].tolist()[question_index]\n",
    "# path_to_tensor = {(question,) : tokenizer.apply_chat_template(prompt_with_step_splits_numbered(question), add_generation_prompt=True, return_tensors=\"pt\")}\n",
    "\n",
    "# question_abstract = f\"question_{question_index}\"\n",
    "# # keep track of the paths that end in each equivalence class\n",
    "# all_equivalence_classes = {question_abstract : Counter([(question,)])}\n",
    "# # track the equivalence classes that we discovered in the last step\n",
    "# new_equivalence_classes = list(all_equivalence_classes.keys())\n",
    "# # track the different wordings we find that make up each equivalence class\n",
    "# unique_wordings = defaultdict(set)\n",
    "# unique_wordings[question_abstract] = {question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if print_logging:\n",
    "#             print(\"CURRENT ALL_EQUIVALENCE_CLASSES: \", all_equivalence_classes)\n",
    "#             print(\"MOST RECENT EQUIVALENCE CLASSES: \", new_equivalence_classes)\n",
    "# # reset the new equivalence classes\n",
    "# prev_equivalence_classes = new_equivalence_classes\n",
    "# new_equivalence_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample paths from each existing equivalence class\n",
    "# # (this should be batched if we're using a transformer)\n",
    "# for class_name in prev_equivalence_classes:\n",
    "#     if print_logging:\n",
    "#         print(\"class name: \", class_name)\n",
    "#         print()\n",
    "    \n",
    "#     # sample some paths that lead to the new equivalence class\n",
    "#     paths = sample_paths(all_equivalence_classes[class_name], n_samples)\n",
    "#     if print_logging:\n",
    "#         print(\"paths: \", paths)\n",
    "#         print()\n",
    "\n",
    "#     for path in paths:\n",
    "#         # sample completions from the model to get the next step following the path\n",
    "#         new_paths, path_to_tensor = sample_completions_from_model(model, tokenizer, path, path_to_tensor, n_samples=n_samples) \n",
    "#         if print_logging:\n",
    "#             print(\"new_paths: \", new_paths)\n",
    "\n",
    "#         # group the completions into equivalence classes\n",
    "#         completion_classes = get_equivalence_classes(new_paths, all_equivalence_classes)\n",
    "#         if print_logging:\n",
    "#             print(\"completions_classes: \", completion_classes)\n",
    "\n",
    "#         # Update our data structures\n",
    "#         # I think it's ok for this kind of thing to be in a for loop,\n",
    "#         # as each operation won't take much time\n",
    "#         for completion_class in completion_classes:\n",
    "#             if completion_class not in all_equivalence_classes:\n",
    "#                 new_equivalence_classes.append(completion_class)\n",
    "#             unique_wordings[completion_class].update(\n",
    "#                 [x[-1] for x in completion_classes[completion_class].keys()]\n",
    "#             )\n",
    "#         # update the running tracker of all discovered euqivalence classes\n",
    "#         all_equivalence_classes = update_equivalence_classes(\n",
    "#             all_equivalence_classes, completion_classes\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if print_logging:\n",
    "#     print(\"CURRENT ALL_EQUIVALENCE_CLASSES: \", all_equivalence_classes)\n",
    "#     print(\"MOST RECENT EQUIVALENCE CLASSES: \", new_equivalence_classes)\n",
    "# # reset the new equivalence classes\n",
    "# prev_equivalence_classes = new_equivalence_classes\n",
    "# new_equivalence_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_name = prev_equivalence_classes[0]\n",
    "# class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = sample_paths(all_equivalence_classes[class_name], n_samples)\n",
    "# if print_logging:\n",
    "#     print(\"paths: \", paths)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_paths, path_to_tensor = sample_completions_from_model(model, tokenizer, path, path_to_tensor, n_samples=n_samples) \n",
    "# if print_logging:\n",
    "#     print(\"new_paths: \", new_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_equivalence_classes(paths, all_equivalence_classes):\n",
    "#     equivalence_classes = defaultdict(Counter)\n",
    "#     current_eq_class_labels = [key for key in all_equivalence_classes.keys() if not key.startswith(\"question_\")]\n",
    "    \n",
    "#     # Loop over the current_eq_class_labels and sample one path for each label    \n",
    "#     current_eq_class_examples = []\n",
    "#     for key in current_eq_class_labels:\n",
    "#         paths = list(all_equivalence_classes[key].keys())\n",
    "#         sampled_path = random.choice(paths)\n",
    "#         current_eq_class_examples.append(sampled_path[-1])\n",
    "\n",
    "#     for path in paths:\n",
    "#         last_state = path[-1]\n",
    "#         if last_state == \"<eos>\":\n",
    "#             continue\n",
    "#         print(\"CUR EQ LABELS: \", current_eq_class_labels)\n",
    "#         print(\"LAST STATE (TO CATEGORIZE): \", last_state)\n",
    "#         last_state_abstract = categorize_step_with_gpt4(path[0], remove_trailing_step_info(last_state), current_eq_class_labels, current_eq_class_examples)\n",
    "#         print(\"gpt4o: \", last_state_abstract, \"LAST STATE: \", last_state)\n",
    "\n",
    "#         if last_state_abstract.startswith(\"NEW : \"):\n",
    "#             last_state_abstract = last_state_abstract[len(\"NEW : \"):]\n",
    "#             current_eq_class_labels.append(last_state_abstract)\n",
    "#         else:\n",
    "#             # Find the closest string match between the response and current_eq_class_labels\n",
    "#             closest_match = get_close_matches(last_state_abstract, current_eq_class_labels, n=1, cutoff=0.6)\n",
    "#             if closest_match:\n",
    "#                 last_state_abstract = closest_match[0]\n",
    "\n",
    "#         equivalence_classes[last_state_abstract].update([path])\n",
    "\n",
    "#     return equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalence_classes = defaultdict(Counter)\n",
    "# current_eq_class_labels = [key for key in all_equivalence_classes.keys() if not key.startswith(\"question_\")]\n",
    "\n",
    "# # Loop over the current_eq_class_labels and sample one path for each label    \n",
    "# current_eq_class_examples = []\n",
    "# for key in current_eq_class_labels:\n",
    "#     paths = list(all_equivalence_classes[key].keys())\n",
    "#     sampled_path = random.choice(paths)\n",
    "#     current_eq_class_examples.append(sampled_path[-1])\n",
    "\n",
    "# current_eq_class_labels, current_eq_class_examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = new_paths[0]\n",
    "# last_state = path[-1]\n",
    "# last_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
