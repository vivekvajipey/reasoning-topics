{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /scr/vvajipey/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b05c830f614bfab9a5494ca3b668cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scr/vvajipey/.cache/huggingface'\n",
    "os.environ['HF_HUB'] = '/scr/vvajipey/.cache/huggingface'\n",
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import get_close_matches\n",
    "\n",
    "gsm_df = pd.read_csv('../distribution/data/gsm8kTest.csv')\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get All Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_questions = gsm_df['question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = gsm_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "all_equivalence_classes = {\"question_0\": Counter([(q0,)])}\n",
    "all_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_equivalence_classes = list(all_equivalence_classes.keys())\n",
    "new_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wordings = defaultdict(set)\n",
    "unique_wordings[\"question_0\"] = {q0}\n",
    "unique_wordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_equivalence_classes = new_equivalence_classes\n",
    "prev_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = prev_equivalence_classes[0]\n",
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equivalence_classes[class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_paths(path_counter, n_samples):\n",
    "    \"\"\"\n",
    "    Sample from a path counter\n",
    "    \"\"\"\n",
    "    return random.choices(\n",
    "        list(path_counter.keys()), weights=list(path_counter.values()), k=n_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "\n",
    "paths = sample_paths(all_equivalence_classes[class_name], n_samples)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paths[0]\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<model.sample>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING COMPLETION FROM MODEL\n",
    "# convert path to tokens, use stopping criteria etc. to generate new path\n",
    "from utils import get_prompt_message\n",
    "\n",
    "# NEED TO JOIN COMPONENTS IF COMPLETING PARTIAL TRACE...\n",
    "question_vector = tokenizer.apply_chat_template(get_prompt_message(path[0], 0), add_generation_prompt=True, return_tensors=\"pt\")\n",
    "input_tensor = question_vector.repeat(n_samples, 1)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_step_splits_justin(question):\n",
    "    # get_prompt_message(overtime_question, 0)\n",
    "    fewshot_question = f\"\"\"ANSWER THE QUESTION STEP-BY-STEP AND CLEARLY MARK BETWEEN STEPS WITH <step split>.\n",
    "\n",
    "Q: Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?\n",
    "\n",
    "A:\n",
    "First, lets calculate how much Tina earns for working a regular 8-hour shift:\n",
    "Regular hours earned = Hourly wage * Hours worked per day * Number of days\n",
    "Regular hours earned = $18.00 * 8 * 5\n",
    "Regular hours earned = $720.00\n",
    "<step split>\n",
    "Next, lets calculate how many overtime hours she works in total:\n",
    "Overtime hours = Total hours worked - Regular hours\n",
    "Overtime hours = 10 hours * 5 days - 8 hours * 5 days\n",
    "Overtime hours = 10 * 5 - 8 * 5 Overtime hours = 50 hours - 40 hours\n",
    "Overtime hours = 10 hours\n",
    "<step split>\n",
    "Now, lets calculate the overtime pay for those 10 hours:\n",
    "Overtime pay per hour = Hourly wage + 1/2 hourly wage\n",
    "Overtime pay per hour = $18.00 + $9.00\n",
    "Overtime pay per hour = $27.00\n",
    "Total overtime pay = Overtime pay per hour * Overtime hours\n",
    "Total overtime pay = $27.00 * 10\n",
    "Total overtime pay = $270.00\n",
    "<step split>\n",
    "Finally, lets calculate her total earnings:\n",
    "Total earnings = Regular hours earned + Total overtime pay\n",
    "Total earnings = $720.00 + $270.00\n",
    "Total earnings = $990.00\n",
    "<step split>\n",
    " So, Tina makes a total of $990.00 for working 10 hours every day for 5 days\n",
    "\n",
    "Q: Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\\n\n",
    "\n",
    "A:\n",
    " First, lets determine how many pages Joy reads in one minute:\n",
    " Pages read per minute = Pages read per 20 minutes / 20 minutes\n",
    " = 8 pages / 20 minutes = 0.4 pages per minute\n",
    "<step split>\n",
    " Next, well find out how long it takes Joy to read 120 pages:\n",
    " Time = Pages to be read / Pages read per minute\n",
    " = 120 pages / 0.4 pages per minute\n",
    " = 300 minutes\n",
    "<step split>\n",
    " Finally, well convert 300 minutes into hours:\n",
    " Hours = Minutes / 60\n",
    " = 300 minutes / 60\n",
    " = 5 hours\n",
    "<step split>\n",
    " So it will take Joy 5 hours to read 120\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": fewshot_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_step_splits_ben(question):\n",
    "    # get_prompt_message(overtime_question, 0)\n",
    "    fewshot_question = f\"\"\"Q: Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?\n",
    "\n",
    "A:\n",
    "Step 1:\n",
    "First, lets calculate how much Tina earns for working a regular 8-hour shift:\n",
    "Regular hours earned = Hourly wage * Hours worked per day * Number of days\n",
    "Regular hours earned = $18.00 * 8 * 5\n",
    "Regular hours earned = $720.00\n",
    "Step 2:\n",
    "Next, lets calculate how many overtime hours she works in total:\n",
    "Overtime hours = Total hours worked - Regular hours\n",
    "Overtime hours = 10 hours * 5 days - 8 hours * 5 days\n",
    "Overtime hours = 10 * 5 - 8 * 5 Overtime hours = 50 hours - 40 hours\n",
    "Overtime hours = 10 hours\n",
    "Step 3:\n",
    "Now, lets calculate the overtime pay for those 10 hours:\n",
    "Overtime pay per hour = Hourly wage + 1/2 hourly wage\n",
    "Overtime pay per hour = $18.00 + $9.00\n",
    "Overtime pay per hour = $27.00\n",
    "Total overtime pay = Overtime pay per hour * Overtime hours\n",
    "Total overtime pay = $27.00 * 10\n",
    "Total overtime pay = $270.00\n",
    "Step 4:\n",
    "Finally, lets calculate her total earnings:\n",
    "Total earnings = Regular hours earned + Total overtime pay\n",
    "Total earnings = $720.00 + $270.00\n",
    "Total earnings = $990.00\n",
    "Step 5:\n",
    "So, Tina makes a total of $990.00 for working 10 hours every day for 5 days\n",
    "\n",
    "Q: Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\\n\n",
    "\n",
    "A:\n",
    "Step 1: \n",
    " First, lets determine how many pages Joy reads in one minute:\n",
    " Pages read per minute = Pages read per 20 minutes / 20 minutes\n",
    " = 8 pages / 20 minutes = 0.4 pages per minute\n",
    "Step 2: \n",
    " Next, well find out how long it takes Joy to read 120 pages:\n",
    " Time = Pages to be read / Pages read per minute\n",
    " = 120 pages / 0.4 pages per minute\n",
    " = 300 minutes\n",
    "Step 3: \n",
    " Finally, well convert 300 minutes into hours:\n",
    " Hours = Minutes / 60\n",
    " = 300 minutes / 60\n",
    " = 5 hours\n",
    "Step 4: \n",
    " So it will take Joy 5 hours to read 120\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "A:\n",
    "Step 1:\n",
    "\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": fewshot_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING COMPLETION FROM MODEL\n",
    "# convert path to tokens, use stopping criteria etc. to generate new path\n",
    "\n",
    "# NEED TO JOIN COMPONENTS IF COMPLETING PARTIAL TRACE...\n",
    "question_vector = tokenizer.apply_chat_template(prompt_with_step_splits_ben(path[0]), add_generation_prompt=True, return_tensors=\"pt\")\n",
    "# input_tensor = question_vector.repeat(n_samples, 1)\n",
    "input_tensor = question_vector.repeat(1, 1)\n",
    "\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_tensor(tensor):\n",
    "    plt.imshow(tensor.cpu().numpy(), cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def convert_to_left_padded(tensor):\n",
    "    trimmed_sequences = []\n",
    "    for seq in tensor:\n",
    "        # Find indices of the first and last non-pad tokens\n",
    "        non_pad_indices = (seq != tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(non_pad_indices) > 0:\n",
    "            start_index = non_pad_indices[0]\n",
    "            end_index = non_pad_indices[-1] + 1\n",
    "            trimmed_sequences.append(seq[start_index:end_index])\n",
    "        else:\n",
    "            # if entire sequence is padding, use an empty sequence\n",
    "            trimmed_sequences.append(torch.tensor([], dtype=torch.long, device=model.device))\n",
    "\n",
    "    # Determine the maximum length after trimming\n",
    "    max_length = max(len(seq) for seq in trimmed_sequences)\n",
    "    padded_tensor = torch.full((tensor.shape[0], max_length), fill_value=tokenizer.pad_token_id, dtype=torch.long, device=model.device)\n",
    "\n",
    "    # put left padding\n",
    "    for i, seq in enumerate(trimmed_sequences):\n",
    "        padded_tensor[i, -len(seq):] = seq\n",
    "\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class BatchSentenceStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_sequences):\n",
    "        # Tokenize each stop sequence and store their token IDs\n",
    "        self.stop_token_ids_list = [tokenizer.encode(seq, add_special_tokens=False)[1:] for seq in stop_sequences]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28750, 28747], [9977, 28705, 28770, 28747], [9977, 28705, 28770, 28747]]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28770, 28747], [9977, 28705, 28781, 28747]]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28781, 28747], [9977, 28705, 28782, 28747]]\n",
    "        print(\"Stop token IDs:\", self.stop_token_ids_list)  # Debugging to see the token IDs\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Check each stop sequence against the end of the input_ids for each sequence in the batch\n",
    "        for stop_token_ids in self.stop_token_ids_list:\n",
    "            if input_ids.shape[1] >= len(stop_token_ids):\n",
    "                # Extract the last tokens of the same length as the stop sequence\n",
    "                last_tokens = input_ids[:, -len(stop_token_ids):]\n",
    "                # Check if they match the stop sequence tokens\n",
    "                is_match = (last_tokens == torch.tensor(stop_token_ids, device=input_ids.device)).all(dim=1)\n",
    "                # If any sequence in the batch matches, return True to stop generation\n",
    "                if is_match.any():\n",
    "                    print(\"Stopping sequence detected, stopping generation.\")\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "stop_sequences = [f\"\\nStep {i}:\" for i in range(10)]\n",
    "step_num_stopping_criteria = StoppingCriteriaList([BatchSentenceStoppingCriteria(tokenizer, stop_sequences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_steps = 10\n",
    "\n",
    "# cur_input = input_tensor\n",
    "\n",
    "# for i in range(num_steps):\n",
    "#         print(\"ITERATION #\", i)\n",
    "\n",
    "#         outputs = model.generate(\n",
    "#             cur_input.to(model.device),\n",
    "#             min_new_tokens=10,\n",
    "#             max_new_tokens=1000,\n",
    "#             return_dict_in_generate=True,\n",
    "#             output_scores=True,\n",
    "#             do_sample=True,\n",
    "#             temperature=0.7,\n",
    "#             top_k=40,\n",
    "#             stopping_criteria=step_num_stopping_criteria\n",
    "#         )\n",
    "#         # for id in outputs.sequences[0][cur_input.shape[1]:]:\n",
    "#             # print(\"id: \", id.item(), \" -- tok: \", tokenizer.decode(id))\n",
    "\n",
    "#         print(tokenizer.batch_decode(outputs.sequences[:, cur_input.shape[1]:]))\n",
    "#         if outputs.sequences[0, -1] == 2:\n",
    "#             print(\"eos!\")\n",
    "#             break \n",
    "#         # cur_input = outputs.sequences\n",
    "#         cur_input = convert_to_left_padded(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize a list to store generated outputs\n",
    "token_outputs = []\n",
    "max_length = 0\n",
    "\n",
    "completions = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    gen_outputs = model.generate(\n",
    "        input_tensor.to(model.device),\n",
    "        min_new_tokens=10,\n",
    "        max_new_tokens=1000,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "        stopping_criteria=step_num_stopping_criteria\n",
    "    ).sequences\n",
    "\n",
    "    # Append the generated outputs to the list\n",
    "    token_outputs.append(gen_outputs)\n",
    "    # Update the maximum length\n",
    "    if gen_outputs.shape[1] > max_length:\n",
    "        max_length = gen_outputs.shape[1]\n",
    "\n",
    "    completions.append(tokenizer.batch_decode(gen_outputs[:, input_tensor.shape[1]:])[0])\n",
    "\n",
    "# Pad each tensor in the list to the maximum length and stack them\n",
    "token_outputs = torch.cat([F.pad(output, (0, max_length - output.shape[1]), value=tokenizer.pad_token_id) for output in token_outputs], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</model.sample>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paths = [path + (completion,) for completion in completions]\n",
    "new_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<get_equivalence_classes> (and update_equivalence_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = new_paths\n",
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict, Counter\n",
    "# equivalence_classes = defaultdict(Counter)\n",
    "# path = paths[0]\n",
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_state = path[-1]\n",
    "# last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import get_close_matches\n",
    "\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def get_bucket_prompt(question, step, buckets=None):\n",
    "    return f\"\"\"I will give you a math problem, and a substep in a solution to the problem. I will also give you a list of natural language label buckets that have been created from previous answers to the same question. Look at the step, identify which bucket it falls under, and return just the name of the bucket. If none of the existing buckets are representative, create a new bucket preceded with the string \"NEW :\". The label name must be descriptive, specific, and concise natural language. Return this new bucket string. \n",
    "\n",
    "Do not generate a new bucket if a step fits into an existing bucket, even if the step is incorrect. \n",
    "Buckets: {buckets}\n",
    "Question: {question} \n",
    "Step: {step}\n",
    "\"\"\"\n",
    "\n",
    "def categorize_step_with_gpt4(question, step, eq_class_labels):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": get_bucket_prompt(question, step, eq_class_labels)}\n",
    "        ],\n",
    "        max_tokens=50,\n",
    "        # temperature=0.7,\n",
    "        temperature=1.0,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# equivalence_classes = defaultdict(Counter)\n",
    "\n",
    "# last_state = path[-1]\n",
    "\n",
    "# current_eq_class_labels = [key for key in all_equivalence_classes.keys() if not key.startswith(\"question_\")]\n",
    "\n",
    "# response = categorize_step_with_gpt4(path[0], last_state, current_eq_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if response.startswith(\"NEW : \"):\n",
    "#     response = response[len(\"NEW : \"):]\n",
    "# else:\n",
    "#     # Find the closest string match between the response and current_eq_class_labels\n",
    "#     closest_match = get_close_matches(response, current_eq_class_labels, n=1, cutoff=0.6)\n",
    "#     if closest_match:\n",
    "#         response = closest_match[0]\n",
    "# equivalence_classes[response].update([path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUTTING TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equivalence_classes = {\"question_0\": Counter([(q0,)])}\n",
    "all_equivalence_classes, list(all_equivalence_classes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # equivalence_classes = defaultdict(Counter)\n",
    "# for path in paths:\n",
    "#     last_state = path[-1]\n",
    "#     # if last_state == \"<eos>\":\n",
    "#     #     continue\n",
    "    \n",
    "#     current_eq_class_labels = [key for key in all_equivalence_classes.keys() if not key.startswith(\"question_\")]\n",
    "#     print(\"CUR EQ LABELS: \", current_eq_class_labels)\n",
    "#     print(\"all_equivalence_classes: \", all_equivalence_classes)\n",
    "#     response = categorize_step_with_gpt4(path[0], last_state, current_eq_class_labels)\n",
    "#     print(\"gpt4o: \", response)\n",
    "#     if response.startswith(\"NEW : \"):\n",
    "#         response = response[len(\"NEW : \"):]\n",
    "#         all_equivalence_classes[response] = Counter([path])\n",
    "#     else:\n",
    "#         # Find the closest string match between the response and current_eq_class_labels\n",
    "#         closest_match = get_close_matches(response, current_eq_class_labels, n=1, cutoff=0.6)\n",
    "#         if closest_match:\n",
    "#             response = closest_match[0]\n",
    "#         all_equivalence_classes[response][path] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(all_equivalence_classes['Calculate remaining eggs from total duck eggs'].keys())[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equivalence_classes(paths):\n",
    "    equivalence_classes = defaultdict(Counter)\n",
    "    current_eq_class_labels = [key for key in all_equivalence_classes.keys() if not key.startswith(\"question_\")]\n",
    "    for path in paths:\n",
    "        last_state = path[-1]\n",
    "        if last_state == \"<eos>\":\n",
    "            continue\n",
    "        print(\"CUR EQ LABELS: \", current_eq_class_labels)\n",
    "        last_state_abstract = categorize_step_with_gpt4(path[0], last_state, current_eq_class_labels)\n",
    "\n",
    "        if last_state_abstract.startswith(\"NEW : \"):\n",
    "            last_state_abstract = last_state_abstract[len(\"NEW : \"):]\n",
    "            current_eq_class_labels.append(last_state_abstract)\n",
    "        else:\n",
    "            # Find the closest string match between the response and current_eq_class_labels\n",
    "            closest_match = get_close_matches(last_state_abstract, current_eq_class_labels, n=1, cutoff=0.6)\n",
    "            if closest_match:\n",
    "                last_state_abstract = closest_match[0]\n",
    "\n",
    "        equivalence_classes[last_state_abstract].update([path])\n",
    "\n",
    "    return equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_classes = get_equivalence_classes(new_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_classes, list(completion_classes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</get_equivalence_classes>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_equivalence_classes(equivalence_classes, new_classes):\n",
    "    \"\"\"\n",
    "    Update the set of equivalence classes with the newly-discovered ones\n",
    "    \"\"\"\n",
    "    for new_class in new_classes:\n",
    "        if new_class in equivalence_classes:\n",
    "            equivalence_classes[new_class] += new_classes[new_class]\n",
    "        else:\n",
    "            equivalence_classes[new_class] = new_classes[new_class]\n",
    "\n",
    "    return equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_classes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for completion_class in completion_classes:\n",
    "    print(completion_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_equivalence_classes = []\n",
    "new_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for completion_class in completion_classes:\n",
    "    if completion_class not in all_equivalence_classes:\n",
    "        new_equivalence_classes.append(completion_class)\n",
    "    unique_wordings[completion_class].update(\n",
    "        [x[-1] for x in completion_classes[completion_class].keys()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_equivalence_classes, unique_wordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the running tracker of all discovered euqivalence classes\n",
    "all_equivalence_classes = update_equivalence_classes(\n",
    "    all_equivalence_classes, completion_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equivalence_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< constructing get_all_transitions >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_paths(path_counter, n_samples):\n",
    "    \"\"\"\n",
    "    Sample from a path counter\n",
    "    \"\"\"\n",
    "    return random.choices(\n",
    "        list(path_counter.keys()), weights=list(path_counter.values()), k=n_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_left_padded(tensor):\n",
    "    trimmed_sequences = []\n",
    "    for seq in tensor:\n",
    "        # Find indices of the first and last non-pad tokens\n",
    "        non_pad_indices = (seq != tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(non_pad_indices) > 0:\n",
    "            start_index = non_pad_indices[0]\n",
    "            end_index = non_pad_indices[-1] + 1\n",
    "            trimmed_sequences.append(seq[start_index:end_index])\n",
    "        else:\n",
    "            # if entire sequence is padding, use an empty sequence\n",
    "            trimmed_sequences.append(torch.tensor([], dtype=torch.long, device=model.device))\n",
    "\n",
    "    # Determine the maximum length after trimming\n",
    "    max_length = max(len(seq) for seq in trimmed_sequences)\n",
    "    padded_tensor = torch.full((tensor.shape[0], max_length), fill_value=tokenizer.pad_token_id, dtype=torch.long, device=model.device)\n",
    "\n",
    "    # put left padding\n",
    "    for i, seq in enumerate(trimmed_sequences):\n",
    "        padded_tensor[i, -len(seq):] = seq\n",
    "\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class BatchSentenceStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_sequences):\n",
    "        # Tokenize each stop sequence and store their token IDs\n",
    "        self.stop_token_ids_list = [tokenizer.encode(seq, add_special_tokens=False)[1:] for seq in stop_sequences]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28750, 28747], [9977, 28705, 28770, 28747], [9977, 28705, 28770, 28747]]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28770, 28747], [9977, 28705, 28781, 28747]]\n",
    "        # self.stop_token_ids_list = [[9977, 28705, 28781, 28747], [9977, 28705, 28782, 28747]]\n",
    "        print(\"Stop token IDs:\", self.stop_token_ids_list)  # Debugging to see the token IDs\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Check each stop sequence against the end of the input_ids for each sequence in the batch\n",
    "        for stop_token_ids in self.stop_token_ids_list:\n",
    "            if input_ids.shape[1] >= len(stop_token_ids):\n",
    "                # Extract the last tokens of the same length as the stop sequence\n",
    "                last_tokens = input_ids[:, -len(stop_token_ids):]\n",
    "                # Check if they match the stop sequence tokens\n",
    "                is_match = (last_tokens == torch.tensor(stop_token_ids, device=input_ids.device)).all(dim=1)\n",
    "                # If any sequence in the batch matches, return True to stop generation\n",
    "                if is_match.any():\n",
    "                    print(\"Stopping sequence detected, stopping generation.\")\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_with_step_splits_numbered(question):\n",
    "    # get_prompt_message(overtime_question, 0)\n",
    "    fewshot_question = f\"\"\"Q: Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?\n",
    "\n",
    "A:\n",
    "Step 1:\n",
    "First, lets calculate how much Tina earns for working a regular 8-hour shift:\n",
    "Regular hours earned = Hourly wage * Hours worked per day * Number of days\n",
    "Regular hours earned = $18.00 * 8 * 5\n",
    "Regular hours earned = $720.00\n",
    "Step 2:\n",
    "Next, lets calculate how many overtime hours she works in total:\n",
    "Overtime hours = Total hours worked - Regular hours\n",
    "Overtime hours = 10 hours * 5 days - 8 hours * 5 days\n",
    "Overtime hours = 10 * 5 - 8 * 5 Overtime hours = 50 hours - 40 hours\n",
    "Overtime hours = 10 hours\n",
    "Step 3:\n",
    "Now, lets calculate the overtime pay for those 10 hours:\n",
    "Overtime pay per hour = Hourly wage + 1/2 hourly wage\n",
    "Overtime pay per hour = $18.00 + $9.00\n",
    "Overtime pay per hour = $27.00\n",
    "Total overtime pay = Overtime pay per hour * Overtime hours\n",
    "Total overtime pay = $27.00 * 10\n",
    "Total overtime pay = $270.00\n",
    "Step 4:\n",
    "Finally, lets calculate her total earnings:\n",
    "Total earnings = Regular hours earned + Total overtime pay\n",
    "Total earnings = $720.00 + $270.00\n",
    "Total earnings = $990.00\n",
    "Step 5:\n",
    "So, Tina makes a total of $990.00 for working 10 hours every day for 5 days\n",
    "\n",
    "Q: Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\\n\n",
    "\n",
    "A:\n",
    "Step 1: \n",
    " First, lets determine how many pages Joy reads in one minute:\n",
    " Pages read per minute = Pages read per 20 minutes / 20 minutes\n",
    " = 8 pages / 20 minutes = 0.4 pages per minute\n",
    "Step 2: \n",
    " Next, well find out how long it takes Joy to read 120 pages:\n",
    " Time = Pages to be read / Pages read per minute\n",
    " = 120 pages / 0.4 pages per minute\n",
    " = 300 minutes\n",
    "Step 3: \n",
    " Finally, well convert 300 minutes into hours:\n",
    " Hours = Minutes / 60\n",
    " = 300 minutes / 60\n",
    " = 5 hours\n",
    "Step 4: \n",
    " So it will take Joy 5 hours to read 120\n",
    "\n",
    "Q: {question}\n",
    "\n",
    "A:\n",
    "Step 1:\n",
    "\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": fewshot_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import get_prompt_message\n",
    "\n",
    "def sample_completions_from_model(model, tokenizer, path, n_samples):\n",
    "    # question_vector = tokenizer.apply_chat_template(get_prompt_message(path[0], 0), add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    question_vector = tokenizer.apply_chat_template(prompt_with_step_splits_numbered(path[0]), add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    input_tensor = question_vector.repeat(n_samples, 1)\n",
    "\n",
    "    stop_sequences = [f\"\\nStep {i}:\" for i in range(10)]\n",
    "    step_num_stopping_criteria = StoppingCriteriaList([BatchSentenceStoppingCriteria(tokenizer, stop_sequences)])\n",
    "\n",
    "    token_outputs = []\n",
    "    \n",
    "    max_length = 0\n",
    "    completions = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        gen_outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            min_new_tokens=10,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "            stopping_criteria=step_num_stopping_criteria\n",
    "        ).sequences\n",
    "\n",
    "        token_outputs.append(gen_outputs)\n",
    "        if gen_outputs.shape[1] > max_length:\n",
    "            max_length = gen_outputs.shape[1]\n",
    "\n",
    "        completions.append(tokenizer.batch_decode(gen_outputs[:, input_tensor.shape[1]:])[0])\n",
    "\n",
    "    # Pad each tensor in the list to the maximum length and stack them\n",
    "    token_outputs = torch.cat([F.pad(output, (0, max_length - output.shape[1]), value=tokenizer.pad_token_id) for output in token_outputs], dim=0)\n",
    "\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_dotenv\u001b[49m(dotenv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../.env\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m openai_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mopenai_api_key)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv(dotenv_path='../.env')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def get_bucket_prompt(question, step, buckets=None):\n",
    "    return f\"\"\"I will give you a math problem, and a substep in a solution to the problem. I will also give you a list of natural language label buckets that have been created from previous answers to the same question. Look at the step, identify which bucket it falls under, and return just the name of the bucket. If none of the existing buckets are representative, create a new bucket preceded with the string \"NEW :\". The label name must be descriptive, specific, and concise natural language. Return this new bucket string. \n",
    "\n",
    "Do not generate a new bucket if a step fits into an existing bucket, even if the step is incorrect. \n",
    "Buckets: {buckets}\n",
    "Question: {question} \n",
    "Step: {step}\n",
    "\"\"\"\n",
    "\n",
    "def categorize_step_with_gpt4(question, step, eq_class_labels):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": get_bucket_prompt(question, step, eq_class_labels)}\n",
    "        ],\n",
    "        max_tokens=50,\n",
    "        # temperature=0.7,\n",
    "        temperature=1.0,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equivalence_classes(paths):\n",
    "    equivalence_classes = defaultdict(Counter)\n",
    "    current_eq_class_labels = [key for key in all_equivalence_classes.keys() if not key.startswith(\"question_\")]\n",
    "    for path in paths:\n",
    "        last_state = path[-1]\n",
    "        if last_state == \"<eos>\":\n",
    "            continue\n",
    "        print(\"CUR EQ LABELS: \", current_eq_class_labels)\n",
    "        last_state_abstract = categorize_step_with_gpt4(path[0], last_state, current_eq_class_labels)\n",
    "\n",
    "        if last_state_abstract.startswith(\"NEW : \"):\n",
    "            last_state_abstract = last_state_abstract[len(\"NEW : \"):]\n",
    "            current_eq_class_labels.append(last_state_abstract)\n",
    "        else:\n",
    "            # Find the closest string match between the response and current_eq_class_labels\n",
    "            closest_match = get_close_matches(last_state_abstract, current_eq_class_labels, n=1, cutoff=0.6)\n",
    "            if closest_match:\n",
    "                last_state_abstract = closest_match[0]\n",
    "\n",
    "        equivalence_classes[last_state_abstract].update([path])\n",
    "\n",
    "    return equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_equivalence_classes(equivalence_classes, new_classes):\n",
    "    \"\"\"\n",
    "    Update the set of equivalence classes with the newly-discovered ones\n",
    "    \"\"\"\n",
    "    for new_class in new_classes:\n",
    "        if new_class in equivalence_classes:\n",
    "            equivalence_classes[new_class] += new_classes[new_class]\n",
    "        else:\n",
    "            equivalence_classes[new_class] = new_classes[new_class]\n",
    "\n",
    "    return equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_transitions(model, question_index, n_samples=2):\n",
    "    \"\"\"\n",
    "    Build a graph of the probability of each state given the previous state.\n",
    "    \"\"\"\n",
    "    question = gsm_df['question'].tolist()[question_index]\n",
    "    question_abstract = f\"question_{question_index}\"\n",
    "    # keep track of the paths that end in each equivalence class\n",
    "    all_equivalence_classes = {question_abstract : Counter([(question,)])}\n",
    "    # track the equivalence classes that we discovered in the last step\n",
    "    new_equivalence_classes = list(all_equivalence_classes.keys())\n",
    "    # track the different wordings we find that make up each equivalence class\n",
    "    unique_wordings = defaultdict(set)\n",
    "    unique_wordings[question_abstract] = {question}\n",
    "\n",
    "    while True:\n",
    "        # reset the new equivalence classes\n",
    "        prev_equivalence_classes = new_equivalence_classes\n",
    "        new_equivalence_classes = []\n",
    "\n",
    "        # sample paths from each existing equivalence class\n",
    "        # (this should be batched if we're using a transformer)\n",
    "        for class_name in prev_equivalence_classes:\n",
    "            # sample some paths that lead to the new equivalence class\n",
    "            paths = sample_paths(all_equivalence_classes[class_name], n_samples)\n",
    "\n",
    "            for path in paths:\n",
    "                # sample completions from the model to get the next step following the path\n",
    "                completions = sample_completions_from_model(model, tokenizer, path, n_samples=n_samples) \n",
    "                new_paths = [path + (completion,) for completion in completions]\n",
    "\n",
    "                # group the completions into equivalence classes\n",
    "                completion_classes = get_equivalence_classes(new_paths)\n",
    "\n",
    "                # Update our data structures\n",
    "                # I think it's ok for this kind of thing to be in a for loop,\n",
    "                # as each operation won't take much time\n",
    "                for completion_class in completion_classes:\n",
    "                    if completion_class not in all_equivalence_classes:\n",
    "                        new_equivalence_classes.append(completion_class)\n",
    "                    unique_wordings[completion_class].update(\n",
    "                        [x[-1] for x in completion_classes[completion_class].keys()]\n",
    "                    )\n",
    "                # update the running tracker of all discovered euqivalence classes\n",
    "                all_equivalence_classes = update_equivalence_classes(\n",
    "                    all_equivalence_classes, completion_classes\n",
    "                )\n",
    "\n",
    "        # break when we stop discovering new equivalence classes\n",
    "        if len(new_equivalence_classes) == 0:\n",
    "            break\n",
    "\n",
    "    return all_equivalence_classes, unique_wordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</ constructing get_all_transitions >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equivalence_classes, unique_wordings = get_all_transitions(model, 0, n_samples=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
