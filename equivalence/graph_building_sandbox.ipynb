{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekvajipey/miniconda3/envs/nightly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.08s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "gpt35_df = pd.read_csv('../conditional/data/112_gsm8k_gpt35_cot_onesent_responses.csv')\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get All Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm_questions = gpt35_df['Question'].tolist()\n",
    "reading_question = gsm_questions[3]\n",
    "reading_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reading_question': Counter({('Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?',): 1})}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "all_equivalence_classes = {\"reading_question\": Counter([(reading_question,)])}\n",
    "all_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading_question']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_equivalence_classes = list(all_equivalence_classes.keys())\n",
    "new_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'reading_question': {'Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?'}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_wordings = defaultdict(set)\n",
    "unique_wordings[\"reading_question\"] = {reading_question}\n",
    "unique_wordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reading_question']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_equivalence_classes = new_equivalence_classes\n",
    "prev_equivalence_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reading_question'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = prev_equivalence_classes[0]\n",
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?',): 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_equivalence_classes[class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_paths(path_counter, n_samples):\n",
    "    \"\"\"\n",
    "    Sample from a path counter\n",
    "    \"\"\"\n",
    "    return random.choices(\n",
    "        list(path_counter.keys()), weights=list(path_counter.values()), k=n_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?',),\n",
       " ('Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?',)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 2\n",
    "\n",
    "paths = sample_paths(all_equivalence_classes[class_name], n_samples)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?',)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = paths[0]\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<model.sample>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:  Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n",
      "A: Let's think step by step.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 76])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAMPLING COMPLETION FROM MODEL\n",
    "# convert path to tokens, use stopping criteria etc. to generate new path\n",
    "from utils import get_prompt_message\n",
    "\n",
    "# NEED TO JOIN COMPONENTS IF COMPLETING PARTIAL TRACE...\n",
    "question_vector = tokenizer.apply_chat_template(get_prompt_message(path[0], 0), add_generation_prompt=True, return_tensors=\"pt\")\n",
    "input_tensor = question_vector.repeat(n_samples, 1)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class BatchSentenceStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_sequences=None):\n",
    "        # Encode each stop sequence and store their token IDs\n",
    "        self.stop_token_ids = [[842, 28705], [13]]\n",
    "        if stop_sequences:\n",
    "            self.stop_token_ids = [tokenizer.encode(seq, add_special_tokens=False) for seq in stop_sequences]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Initialize a tensor to keep track of which sequences should stop\n",
    "        stop_mask = torch.zeros(input_ids.shape[0], dtype=torch.bool, device=input_ids.device)\n",
    "        \n",
    "        # Check each stop sequence against the end of the input_ids for each sequence in the batch\n",
    "        for stop_ids in self.stop_token_ids:\n",
    "            if input_ids.shape[1] >= len(stop_ids):  # Ensure input_ids is long enough\n",
    "                # Check if the last tokens of input_ids match the stop sequence\n",
    "                match = torch.eq(input_ids[:, -len(stop_ids):], torch.tensor(stop_ids, device=input_ids.device)).all(dim=1)\n",
    "                stop_mask |= match  # Update the stop mask\n",
    "                if match.sum() > 0:                    \n",
    "                    print(f\"Mask for {stop_ids}: \", stop_mask)\n",
    "        \n",
    "        return stop_mask\n",
    "\n",
    "sentence_sampling_criteria = StoppingCriteriaList([BatchSentenceStoppingCriteria(tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask for [13]:  tensor([ True, False], device='mps:0')\n",
      "Mask for [13]:  tensor([False,  True], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 118])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "            input_tensor.to(model.device),\n",
    "            # min_new_tokens=min_new_tokens,\n",
    "            max_new_tokens=1000,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "            stopping_criteria=sentence_sampling_criteria\n",
    "        )\n",
    "outputs.sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> [INST] Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\\nA: Let's think step by step. [/INST] 1. Julie has read 12 pages so far.\\n</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"<s> [INST] Q: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\\nA: Let's think step by step. [/INST] 1. Julie started with 120 pages and read 12 pages yesterday, leaving her with 108 pages (120 - 12 = 108).\\n\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  3707  -- tok:  step\n",
      "id:  486  -- tok:  by\n",
      "id:  3707  -- tok:  step\n",
      "id:  28723  -- tok:  .\n",
      "id:  733  -- tok:  [\n",
      "id:  28748  -- tok:  /\n",
      "id:  16289  -- tok:  INST\n",
      "id:  28793  -- tok:  ]\n",
      "id:  28705  -- tok:  \n",
      "id:  28740  -- tok:  1\n",
      "id:  28723  -- tok:  .\n",
      "id:  22706  -- tok:  Julie\n",
      "id:  659  -- tok:  has\n",
      "id:  1220  -- tok:  read\n",
      "id:  28705  -- tok:  \n",
      "id:  28740  -- tok:  1\n",
      "id:  28750  -- tok:  2\n",
      "id:  6718  -- tok:  pages\n",
      "id:  579  -- tok:  so\n",
      "id:  2082  -- tok:  far\n"
     ]
    }
   ],
   "source": [
    "for id in outputs.sequences[0][-50:-30]:\n",
    "    print(\"id: \", id.item(), \" -- tok: \", tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAbCAYAAADhwYyIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAADB0lEQVR4nO3dy0vUURTA8TMz2UyPjblIhpqy0gIrjNIekkgU6mwipEUtI1ete/wH/QdubB8RtRw1IdRV9lIxZCjKZyKRpUQtZB7tfr97rjgPLWbwfj+rezgzv7lz7/wuh98RDGSz2awAAABnBUs9AQAAUFoUAwAAOI5iAAAAx1EMAADgOIoBAAAcRzEAAIDjKAYAAHAcxQAAAI6jGAAAwHHbCn3hleB1FQcjEW+c7D6uclPtj1QcP9bijRPJYZW7u3hKxRNNIRVnGuu9cWDkg8r1z7/LN+2CxVuu+cGv3yqXGH2h4ss3b3nj7ycjKjf2oFvF7TVn/SCdVrlQ9V4VpxYWdX73Ln8O1rrl0trVpeJw73t93SMH/esOPlt/viISDIdVvHTV34/Kx2/0a435ioj8aa7zxpH+UZWbvd+k4sk7/rp1HL2octnVVRWnLtSrODQ07o+tOQSqKvV7Z+ZlPSs3GlUcWfb362dthcqN39P73FHbrD+3wr+1zHUQEdkxnFSx+f3s9bb3vS3a4I37F8Ykl3hrp76Wtdcme80Dsag3znycUrlgXY2+7sAT/zqHzqlc75dXKq7pu61i86xYcxac364/11ibYu4HEX1PfGvQe7n/4YiKizlXzOvu/PRD5ez1ttcmGK32xqnpOZULHT6g4vTnmU3PT0RksKdHxe2xM3pOVXu8cWZ5ReX6pvQ6FcOcx5rzyPquiaHnKo6fuOTnJl6qnH1emTYzX3td+mbfFvzetn2nVbzh39PraZVLL+nfl33WmfeEeU6IiAxknub9bJ4MAADgOIoBAAAcFyj0HxXZbQKXbaUWiWqPiJRFiyTX4698/leLxGyPiJRHi8Rsj4iUR4vEbI+I0CIx5WqRFMM+NyY7YypOz331xsU83rblvZf+URvTbtuY94NI7laAbaOtgVK1BWy0CQAAQMlQDAAA4DiKAQAAHFfw3wwAAICtiScDAAA4jmIAAADHUQwAAOA4igEAABxHMQAAgOMoBgAAcBzFAAAAjqMYAADAcRQDAAA47i+EEyM55R151QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_left_padded(tensor):\n",
    "    trimmed_sequences = []\n",
    "    for seq in tensor:\n",
    "        # Find indices of the first and last non-pad tokens\n",
    "        non_pad_indices = (seq != tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(non_pad_indices) > 0:\n",
    "            start_index = non_pad_indices[0]\n",
    "            end_index = non_pad_indices[-1] + 1\n",
    "            trimmed_sequences.append(seq[start_index:end_index])\n",
    "        else:\n",
    "            # if entire sequence is padding, use an empty sequence\n",
    "            trimmed_sequences.append(torch.tensor([], dtype=torch.long, device=model.device))\n",
    "\n",
    "    # Determine the maximum length after trimming\n",
    "    max_length = max(len(seq) for seq in trimmed_sequences)\n",
    "    padded_tensor = torch.full((tensor.shape[0], max_length), fill_value=tokenizer.pad_token_id, dtype=torch.long, device=model.device)\n",
    "\n",
    "    # put left padding\n",
    "    for i, seq in enumerate(trimmed_sequences):\n",
    "        padded_tensor[i, -len(seq):] = seq\n",
    "\n",
    "    return padded_tensor\n",
    "\n",
    "padded_output = convert_to_left_padded(outputs.sequences)\n",
    "\n",
    "# from utils import plot_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_tensor(tensor):\n",
    "    plt.imshow(tensor.cpu().numpy(), cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_tensor(padded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</model.sample>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
